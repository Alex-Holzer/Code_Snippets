{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "\n",
    "def save_dataframe_to_csv(\n",
    "    df: DataFrame,\n",
    "    folder_path: str,\n",
    "    file_name: str,\n",
    "    partition_by: Optional[Union[str, List[str]]] = None,\n",
    "    overwrite: bool = False,\n",
    "    header: bool = True,\n",
    "    delimiter: str = \",\",\n",
    "    encoding: str = \"utf-8\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save a PySpark DataFrame as a single CSV file in the specified path in Databricks.\n",
    "\n",
    "    This function saves a PySpark DataFrame as a CSV file, with options for\n",
    "    partitioning, overwriting, and basic CSV customization.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The PySpark DataFrame to be saved.\n",
    "        folder_path (str): The directory path where the CSV file will be saved.\n",
    "        file_name (str): The name of the CSV file (without the .csv extension).\n",
    "        partition_by (Optional[Union[str, List[str]]]): Column(s) to partition the output by.\n",
    "            Note: Using this will result in multiple files. Default is None.\n",
    "        overwrite (bool): Whether to overwrite the file if it already exists. Default is False.\n",
    "        header (bool): Whether to include column names as the first row. Default is True.\n",
    "        delimiter (str): The delimiter to use in the CSV file. Default is \",\".\n",
    "        encoding (str): The character encoding to use. Default is \"utf-8\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input validation fails.\n",
    "        Exception: If there's an error during the save operation.\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "        >>> save_dataframe_to_csv(df, \"/dbfs/path/to/folder\", \"users\")\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"‚ùå Input 'df' must be a PySpark DataFrame\")\n",
    "    if not isinstance(folder_path, str) or not folder_path.strip():\n",
    "        raise ValueError(\"‚ùå 'folder_path' must be a non-empty string\")\n",
    "    if not isinstance(file_name, str) or not file_name.strip():\n",
    "        raise ValueError(\"‚ùå 'file_name' must be a non-empty string\")\n",
    "    if partition_by is not None and not isinstance(partition_by, (str, list)):\n",
    "        raise ValueError(\"‚ùå 'partition_by' must be a string, list of strings, or None\")\n",
    "\n",
    "    try:\n",
    "        # Construct the full file path\n",
    "        full_path = f\"{folder_path}/{file_name}\"\n",
    "\n",
    "        # Prepare write options\n",
    "        write_options = {\n",
    "            \"header\": str(header).lower(),\n",
    "            \"delimiter\": delimiter,\n",
    "            \"encoding\": encoding,\n",
    "        }\n",
    "\n",
    "        # Prepare the write operation\n",
    "        write_op = (\n",
    "            df.coalesce(1)\n",
    "            .write.options(**write_options)\n",
    "            .mode(\"overwrite\" if overwrite else \"error\")\n",
    "        )\n",
    "\n",
    "        # Apply partitioning if specified\n",
    "        if partition_by:\n",
    "            write_op = write_op.partitionBy(partition_by)\n",
    "            print(\"‚ö†Ô∏è Note: Using partition_by will result in multiple output files.\")\n",
    "\n",
    "        # Save the DataFrame as CSV\n",
    "        write_op.csv(full_path)\n",
    "\n",
    "        print(f\"‚úÖ DataFrame successfully saved as CSV: {full_path}\")\n",
    "\n",
    "        # Get and print some statistics about the saved data\n",
    "        saved_df = spark.read.csv(full_path, header=header, inferSchema=True)\n",
    "        row_count = saved_df.count()\n",
    "        file_count = saved_df.rdd.getNumPartitions()\n",
    "        print(f\"üìä Saved {row_count} rows in {file_count} file(s)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"‚ùå Error saving DataFrame to CSV: {str(e)}\"\n",
    "        print(error_message)\n",
    "        raise Exception(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "from typing import List, Optional, Union\n",
    "import re\n",
    "\n",
    "\n",
    "def get_dataframe(\n",
    "    folder_path: str,\n",
    "    schema: Optional[Union[str, StructType]] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    header: bool = True,\n",
    "    delimiter: str = \",\",\n",
    "    ignore_files: List[str] = [],\n",
    "    recursive: bool = False,\n",
    "    infer_schema: bool = False,\n",
    "    cache: bool = False,\n",
    "    repartition: Optional[int] = None,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract all CSV files from a folder and combine them into a single DataFrame in Databricks.\n",
    "\n",
    "    This function reads all CSV files from the specified folder, combining them\n",
    "    using a single read operation for better performance. It provides various options\n",
    "    for customizing the read process and optimizing the resulting DataFrame.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The directory path containing the CSV file(s).\n",
    "        schema (Optional[Union[str, StructType]]): The schema for the DataFrame. Can be a string or StructType.\n",
    "            If None, the schema will be inferred. Default is None.\n",
    "        encoding (str): The character encoding of the CSV file(s). Default is \"utf-8\".\n",
    "        header (bool): Whether the CSV file(s) have a header row. Default is True.\n",
    "        delimiter (str): The delimiter used in the CSV file(s). Default is \",\".\n",
    "        ignore_files (List[str]): List of filenames to ignore in the folder. Default is empty list.\n",
    "        recursive (bool): Whether to search for CSV files in subfolders. Default is False.\n",
    "        infer_schema (bool): Whether to infer the schema from a subset of the data. Default is False.\n",
    "        cache (bool): Whether to cache the resulting DataFrame. Default is False.\n",
    "        repartition (Optional[int]): Number of partitions for the final DataFrame. If None, no repartitioning is done.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame containing the combined data from all CSV files.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input validation fails or no CSV files are found.\n",
    "        Exception: If there's an error during the read operation.\n",
    "\n",
    "    Example:\n",
    "        >>> df = get_dataframe(\"/dbfs/path/to/csv/folder\", header=True, recursive=True, cache=True)\n",
    "        >>> df.show()\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(folder_path, str) or not folder_path.strip():\n",
    "        raise ValueError(\"‚ùå 'folder_path' must be a non-empty string\")\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.ls(folder_path)\n",
    "    except Exception:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå The specified folder does not exist or is not accessible: {folder_path}\"\n",
    "        )\n",
    "\n",
    "    # Validate schema if it's a string\n",
    "    if isinstance(schema, str):\n",
    "        try:\n",
    "            schema = StructType.fromDDL(schema)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"‚ùå Invalid schema string: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        # Prepare file path for reading\n",
    "        file_path = f\"{folder_path}{'/**' if recursive else ''}/*.csv\"\n",
    "\n",
    "        # Filter out ignored files\n",
    "        if ignore_files:\n",
    "            ignore_pattern = \"|\".join(re.escape(f) for f in ignore_files)\n",
    "            files = [\n",
    "                f.path\n",
    "                for f in dbutils.fs.ls(folder_path)\n",
    "                if f.path.endswith(\".csv\") and not re.search(ignore_pattern, f.name)\n",
    "            ]\n",
    "            if not files:\n",
    "                raise ValueError(\n",
    "                    f\"‚ùå No CSV files found in the specified folder after applying ignore list: {folder_path}\"\n",
    "                )\n",
    "            file_path = files\n",
    "\n",
    "        # Read options\n",
    "        read_options = {\n",
    "            \"encoding\": encoding,\n",
    "            \"header\": str(header).lower(),\n",
    "            \"delimiter\": delimiter,\n",
    "            \"ignoreLeadingWhiteSpace\": \"true\",\n",
    "            \"ignoreTrailingWhiteSpace\": \"true\",\n",
    "        }\n",
    "\n",
    "        if schema:\n",
    "            read_options[\"schema\"] = schema\n",
    "        elif infer_schema:\n",
    "            read_options[\"inferSchema\"] = \"true\"\n",
    "\n",
    "        # Read all CSV files\n",
    "        df = spark.read.options(**read_options).csv(file_path)\n",
    "\n",
    "        # Add metadata column\n",
    "        df = df.withColumn(\"source_file\", F.input_file_name())\n",
    "\n",
    "        # Repartition if specified\n",
    "        if repartition:\n",
    "            df = df.repartition(repartition)\n",
    "\n",
    "        # Cache if specified\n",
    "        if cache:\n",
    "            df = df.cache()\n",
    "\n",
    "        print(f\"‚úÖ Successfully read CSV file(s) from: {folder_path}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"‚ùå Error reading CSV file(s): {str(e)}\"\n",
    "        print(error_message)\n",
    "        raise Exception(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "def save_dataframe_to_delta(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    overwrite_schema: bool = False,\n",
    "    merge_schema: bool = False,\n",
    "    optimize_write: bool = True,\n",
    "    partition_by: list = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save a PySpark DataFrame as a Delta table in Databricks.\n",
    "\n",
    "    This function saves a PySpark DataFrame as a Delta table using saveAsTable or overwrite mode.\n",
    "    It provides options for overwriting existing tables, merging schemas, and optimizing writes.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The PySpark DataFrame to be saved.\n",
    "        table_name (str): The name of the Delta table to be created or overwritten.\n",
    "        overwrite_schema (bool): Whether to overwrite the schema if the table already exists.\n",
    "                                 Default is False.\n",
    "        merge_schema (bool): Whether to merge the new schema with the existing one.\n",
    "                             Only applicable when overwrite_schema is False. Default is False.\n",
    "        optimize_write (bool): Whether to optimize the write operation. Default is True.\n",
    "        partition_by (list): List of columns to partition the table by. Default is None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input validation fails.\n",
    "        Exception: If there's an error during the save operation.\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "        >>> save_dataframe_to_delta(df, \"users_table\", merge_schema=True, partition_by=[\"id\"])\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"‚ùå Input 'df' must be a PySpark DataFrame\")\n",
    "    if not isinstance(table_name, str) or not table_name.strip():\n",
    "        raise ValueError(\"‚ùå 'table_name' must be a non-empty string\")\n",
    "    if partition_by and not isinstance(partition_by, list):\n",
    "        raise ValueError(\"‚ùå 'partition_by' must be a list of column names or None\")\n",
    "\n",
    "    try:\n",
    "        # Check if the table already exists\n",
    "        table_exists = spark.catalog._jcatalog.tableExists(table_name)\n",
    "\n",
    "        # Prepare the write operation\n",
    "        write_op = df.write.format(\"delta\")\n",
    "\n",
    "        if optimize_write:\n",
    "            write_op = write_op.option(\"optimizeWrite\", \"true\")\n",
    "\n",
    "        if partition_by:\n",
    "            write_op = write_op.partitionBy(partition_by)\n",
    "\n",
    "        if table_exists:\n",
    "            if overwrite_schema:\n",
    "                write_op = write_op.mode(\"overwrite\")\n",
    "                print(\n",
    "                    f\"‚ÑπÔ∏è Overwriting existing table '{table_name}' including its schema.\"\n",
    "                )\n",
    "            elif merge_schema:\n",
    "                write_op = write_op.option(\"mergeSchema\", \"true\").mode(\"overwrite\")\n",
    "                print(f\"‚ÑπÔ∏è Merging new schema with existing table '{table_name}'.\")\n",
    "            else:\n",
    "                write_op = write_op.mode(\"overwrite\")\n",
    "                print(\n",
    "                    f\"‚ÑπÔ∏è Overwriting data in existing table '{table_name}' without changing schema.\"\n",
    "                )\n",
    "        else:\n",
    "            write_op = write_op.mode(\"overwrite\")\n",
    "            print(f\"‚ÑπÔ∏è Creating new table '{table_name}'.\")\n",
    "\n",
    "        # Save the DataFrame as a Delta table\n",
    "        write_op.saveAsTable(table_name)\n",
    "\n",
    "        print(f\"‚úÖ DataFrame successfully saved as Delta table: {table_name}\")\n",
    "\n",
    "        # Get and print some statistics about the saved data\n",
    "        saved_df = spark.table(table_name)\n",
    "        row_count = saved_df.count()\n",
    "        column_count = len(saved_df.columns)\n",
    "        print(\n",
    "            f\"üìä Saved {row_count} rows with {column_count} columns in Delta table '{table_name}'\"\n",
    "        )\n",
    "\n",
    "        # Print Delta table details\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        print(f\"‚ÑπÔ∏è Delta table information:\")\n",
    "        print(\n",
    "            f\"   - Location: {delta_table.detail().select('location').collect()[0][0]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   - Version: {delta_table.history(1).select('version').collect()[0][0]}\"\n",
    "        )\n",
    "\n",
    "    except AnalysisException as ae:\n",
    "        error_message = f\"‚ùå Error saving DataFrame to Delta table: {str(ae)}\"\n",
    "        print(error_message)\n",
    "        if \"already exists\" in str(ae):\n",
    "            print(\n",
    "                \"‚ÑπÔ∏è Consider using overwrite_schema=True if you want to replace the existing table schema.\"\n",
    "            )\n",
    "        raise Exception(error_message)\n",
    "    except Exception as e:\n",
    "        error_message = f\"‚ùå Error saving DataFrame to Delta table: {str(e)}\"\n",
    "        print(error_message)\n",
    "        raise Exception(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import expr\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def clean_single_name(name: str, to_upper: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Clean a single column name.\n",
    "\n",
    "    Args:\n",
    "        name (str): The column name to clean\n",
    "        to_upper (bool): If True, converts name to uppercase. Default is False (lowercase).\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned column name\n",
    "    \"\"\"\n",
    "    name = name.upper() if to_upper else name.lower()\n",
    "    name = re.sub(r\"[\\s\\-\\.\\[\\]\\(\\)\\{\\}\\,\\;\\:]+\", \"_\", name)\n",
    "    name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "    return \"col_\" + name if not name[0].isalpha() else name\n",
    "\n",
    "\n",
    "def get_unique_names(names: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ensure uniqueness of column names.\n",
    "\n",
    "    Args:\n",
    "        names (List[str]): List of column names\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of unique column names\n",
    "    \"\"\"\n",
    "    name_counts: Dict[str, int] = {}\n",
    "    unique_names: List[str] = []\n",
    "    for name in names:\n",
    "        if name in name_counts:\n",
    "            name_counts[name] += 1\n",
    "            unique_names.append(f\"{name}_{name_counts[name]}\")\n",
    "        else:\n",
    "            name_counts[name] = 0\n",
    "            unique_names.append(name)\n",
    "    return unique_names\n",
    "\n",
    "\n",
    "def generate_rename_expressions(\n",
    "    old_names: List[str], new_names: List[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate expressions for renaming columns.\n",
    "\n",
    "    Args:\n",
    "        old_names (List[str]): Original column names\n",
    "        new_names (List[str]): New column names\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of expressions for renaming columns\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f\"`{old}` AS `{new}`\" for old, new in zip(old_names, new_names) if old != new\n",
    "    ]\n",
    "\n",
    "\n",
    "def log_column_changes(old_names: List[str], new_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Log the changes made to column names.\n",
    "\n",
    "    Args:\n",
    "        old_names (List[str]): Original column names\n",
    "        new_names (List[str]): New column names\n",
    "    \"\"\"\n",
    "    changes = [(old, new) for old, new in zip(old_names, new_names) if old != new]\n",
    "    if changes:\n",
    "        print(\"üîÑ Column renaming summary:\")\n",
    "        for old, new in changes:\n",
    "            print(f\"  '{old}' -> '{new}'\")\n",
    "    else:\n",
    "        print(\"‚úÖ No column renaming was necessary.\")\n",
    "\n",
    "\n",
    "def clean_column_names(df: DataFrame, to_upper: bool = False) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Efficiently clean and standardize column names of a PySpark DataFrame.\n",
    "\n",
    "    This function orchestrates the cleaning process of DataFrame column names,\n",
    "    ensuring they adhere to naming conventions and are unique.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame\n",
    "        to_upper (bool): If True, converts names to uppercase. Default is False (lowercase).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with cleaned column names\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a PySpark DataFrame\n",
    "        RuntimeError: If column cleaning or renaming fails\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.createDataFrame([(1, 'a'), (2, 'b')], ['ID', 'Column With Spaces!'])\n",
    "        >>> cleaned_df = clean_column_names(df)\n",
    "        >>> cleaned_df.printSchema()\n",
    "        root\n",
    "         |-- id: long (nullable = true)\n",
    "         |-- column_with_spaces: string (nullable = true)\n",
    "\n",
    "    Note:\n",
    "        This function is optimized for large DataFrames and uses PySpark's internal\n",
    "        functions where possible to maximize performance.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"‚ùå Input must be a PySpark DataFrame\")\n",
    "\n",
    "    try:\n",
    "        # Clean and uniquify column names\n",
    "        new_names = get_unique_names(\n",
    "            [clean_single_name(col, to_upper) for col in df.columns]\n",
    "        )\n",
    "\n",
    "        # Generate renaming expressions\n",
    "        rename_exprs = generate_rename_expressions(df.columns, new_names)\n",
    "\n",
    "        # Apply renaming in a single transformation\n",
    "        if rename_exprs:\n",
    "            df = df.selectExpr(\n",
    "                *rename_exprs,\n",
    "                *[\n",
    "                    f\"`{col}`\"\n",
    "                    for col in df.columns\n",
    "                    if col not in dict(zip(df.columns, new_names))\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        # Log the changes\n",
    "        log_column_changes(df.columns, new_names)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå Error during column name cleaning: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count, when, col\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def assert_dataframe_equality(df1: DataFrame, df2: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Assert that two PySpark DataFrames have equal content, optimized for speed.\n",
    "\n",
    "    This function efficiently compares two DataFrames and asserts that they have the same content.\n",
    "    It assumes that both DataFrames have the same column names.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): The first DataFrame to compare\n",
    "        df2 (DataFrame): The second DataFrame to compare\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the DataFrames are not equal, with catchy details about the differences\n",
    "\n",
    "    Example:\n",
    "        >>> df1 = spark.createDataFrame([(1, 'a'), (2, 'b')], ['id', 'value'])\n",
    "        >>> df2 = spark.createDataFrame([(1, 'a'), (2, 'b')], ['id', 'value'])\n",
    "        >>> assert_dataframe_equality(df1, df2)  # This will pass\n",
    "        >>>\n",
    "        >>> df3 = spark.createDataFrame([(1, 'a'), (3, 'c')], ['id', 'value'])\n",
    "        >>> assert_dataframe_equality(df1, df3)  # This will raise an AssertionError\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if both inputs are DataFrames\n",
    "    if not isinstance(df1, DataFrame) or not isinstance(df2, DataFrame):\n",
    "        raise AssertionError(\n",
    "            \"üö´ Oops! Both inputs must be PySpark DataFrames. Let's stick to the script!\"\n",
    "        )\n",
    "\n",
    "    # Check if the column names are the same\n",
    "    if df1.columns != df2.columns:\n",
    "        raise AssertionError(\n",
    "            f\"üî§ Column name mismatch! We've got:\\n{df1.columns}\\nvs\\n{df2.columns}\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Efficiently compare DataFrames\n",
    "        df_combined = df1.selectExpr(\"*\", \"1 as df1_marker\").unionAll(\n",
    "            df2.selectExpr(\"*\", \"2 as df1_marker\")\n",
    "        )\n",
    "\n",
    "        diff_stats = (\n",
    "            df_combined.groupBy(*df1.columns)\n",
    "            .agg(\n",
    "                count(when(col(\"df1_marker\") == 1, True)).alias(\"count_df1\"),\n",
    "                count(when(col(\"df1_marker\") == 2, True)).alias(\"count_df2\"),\n",
    "            )\n",
    "            .where(\"count_df1 != count_df2\")\n",
    "        )\n",
    "\n",
    "        diff_count = diff_stats.count()\n",
    "\n",
    "        if diff_count > 0:\n",
    "            sample_diff = diff_stats.limit(5).collect()\n",
    "\n",
    "            error_message = f\"üìä DataFrames are like apples and oranges! Found {diff_count} mismatched rows.\\n\"\n",
    "            error_message += \"üîç Here's a sneak peek at the differences:\\n\"\n",
    "            for row in sample_diff:\n",
    "                error_message += f\"   {row.asDict()}\\n\"\n",
    "            error_message += \"üí° Tip: Check your data sources or transformations!\"\n",
    "\n",
    "            raise AssertionError(error_message)\n",
    "\n",
    "        # If we've made it this far, the DataFrames are equal\n",
    "        print(\"üéâ Jackpot! The DataFrames are identical twins.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if isinstance(e, AssertionError):\n",
    "            raise e\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"‚ùå An unexpected error occurred while comparing DataFrames: {str(e)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DataFrameEqualityTest\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"score\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create two identical DataFrames\n",
    "data1 = [(1, \"Alice\", 85.5), (2, \"Bob\", 92.0), (3, \"Charlie\", None), (4, \"David\", 78.5)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema)\n",
    "df2 = spark.createDataFrame(data1, schema)\n",
    "\n",
    "# Create a slightly different DataFrame\n",
    "data3 = [\n",
    "    (1, \"Alice\", 85.5),\n",
    "    (2, \"Bob\", 92.0),\n",
    "    (3, \"Charlie\", None),\n",
    "    (4, \"David\", 79.0),  # Changed score from 78.5 to 79.0\n",
    "]\n",
    "\n",
    "df3 = spark.createDataFrame(data3, schema)\n",
    "\n",
    "# Test the assert_dataframe_equality function\n",
    "print(\"Test 1: Comparing identical DataFrames\")\n",
    "try:\n",
    "    assert_dataframe_equality(df1, df2)\n",
    "except AssertionError as e:\n",
    "    print(f\"Unexpected Error: {str(e)}\")\n",
    "\n",
    "print(\"\\nTest 2: Comparing DataFrames with a difference\")\n",
    "try:\n",
    "    assert_dataframe_equality(df1, df3)\n",
    "except AssertionError as e:\n",
    "    print(f\"Expected Error: {str(e)}\")\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
