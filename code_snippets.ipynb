{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, Literal\n",
    "from functools import reduce\n",
    "\n",
    "# PySpark Core Imports\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DataType,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# PySpark MLlib Imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, OneHotEncoder, StringIndexer\n",
    "\n",
    "# Additional Third-Party Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "LOGGING_LEVEL = logging.INFO\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(level=LOGGING_LEVEL)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Commonly Used Functions and Aliases\n",
    "col = F.col\n",
    "lit = F.lit\n",
    "count = F.count\n",
    "mean = F.mean\n",
    "stddev = F.stddev\n",
    "min = F.min\n",
    "max = F.max\n",
    "when = F.when\n",
    "regexp_replace = F.regexp_replace\n",
    "trim = F.trim\n",
    "lower = F.lower\n",
    "row_number = F.row_number\n",
    "broadcast = F.broadcast\n",
    "\n",
    "# Custom Type Aliases\n",
    "NumericCol = Union[int, float]\n",
    "StringCol = str\n",
    "ColumnList = List[Column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_column_types(\n",
    "    df: DataFrame, type_mapping: Dict[str, Union[str, DataType]]\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Change the data types of specified columns in a PySpark DataFrame.\n",
    "\n",
    "    This function efficiently modifies the data types of multiple columns in a single pass,\n",
    "    utilizing PySpark's internal functions for optimal performance on large datasets.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "        type_mapping (Dict[str, Union[str, DataType]]): A dictionary mapping column names\n",
    "            to their desired data types. The data types can be specified as strings\n",
    "            (e.g., 'int', 'float', 'timestamp') or PySpark DataType objects.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with updated column data types.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid column name is provided.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"1\", \"2.0\", \"true\", \"2023-01-01\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"A\", \"B\", \"C\", \"D\"])\n",
    "        >>> type_mapping = {\"A\": \"int\", \"B\": \"float\", \"C\": \"boolean\", \"D\": \"timestamp\"}\n",
    "        >>> result_df = change_column_types(df, type_mapping)\n",
    "        >>> result_df.printSchema()\n",
    "        root\n",
    "         |-- A: integer (nullable = true)\n",
    "         |-- B: float (nullable = true)\n",
    "         |-- C: boolean (nullable = true)\n",
    "         |-- D: timestamp (nullable = true)\n",
    "    \"\"\"\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input must be a PySpark DataFrame\")\n",
    "\n",
    "    if not isinstance(type_mapping, dict):\n",
    "        raise ValueError(\"Type mapping must be a dictionary\")\n",
    "\n",
    "    # Validate column names\n",
    "    invalid_columns = set(type_mapping.keys()) - set(df.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(\n",
    "            f\"Columns not found in the DataFrame: {', '.join(invalid_columns)}\"\n",
    "        )\n",
    "\n",
    "    # Apply type changes in a single pass\n",
    "    return df.select(\n",
    "        *[\n",
    "            col(c).cast(type_mapping.get(c, df.schema[c].dataType)).alias(c)\n",
    "            for c in df.columns\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df: DataFrame, columns_map: Dict[str, str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Rename columns of a PySpark DataFrame based on a provided mapping.\n",
    "\n",
    "    This function efficiently renames multiple columns in a single operation,\n",
    "    optimizing performance for large DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "        columns_map (Dict[str, str]): A dictionary mapping old column names to new column names.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with renamed columns.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a DataFrame or if the columns_map is not a dictionary.\n",
    "        KeyError: If any of the old column names in columns_map are not present in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"John\", \"Doe\", 30)]\n",
    "        >>> df = spark.createDataFrame(data, [\"first_name\", \"last_name\", \"age\"])\n",
    "        >>> columns_map = {\"first_name\": \"name\", \"last_name\": \"surname\"}\n",
    "        >>> result_df = rename_columns(df, columns_map)\n",
    "        >>> result_df.show()\n",
    "        +----+-------+---+\n",
    "        |name|surname|age|\n",
    "        +----+-------+---+\n",
    "        |John|    Doe| 30|\n",
    "        +----+-------+---+\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input must be a PySpark DataFrame\")\n",
    "    if not isinstance(columns_map, dict):\n",
    "        raise ValueError(\"columns_map must be a dictionary\")\n",
    "\n",
    "    # Check if all old column names exist in the DataFrame\n",
    "    missing_columns = set(columns_map.keys()) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise KeyError(\n",
    "            f\"Columns not found in the DataFrame: {', '.join(missing_columns)}\"\n",
    "        )\n",
    "\n",
    "    # Prepare the list of new column names\n",
    "    new_columns = [columns_map.get(c, c) for c in df.columns]\n",
    "\n",
    "    # Rename columns in a single operation\n",
    "    return df.toDF(*new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+\n",
      "|   name|age|stte|\n",
      "+-------+---+----+\n",
      "|  Alice| 34|  NY|\n",
      "|    Bob| 45|  CA|\n",
      "|Charlie| 29|  TX|\n",
      "+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_column_names(df: DataFrame, max_length: int = 64) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Thoroughly cleans and standardizes column names of a PySpark DataFrame:\n",
    "    - Converts all characters to uppercase.\n",
    "    - Trims leading and trailing spaces.\n",
    "    - Replaces spaces and invalid characters with underscores.\n",
    "    - Ensures the column name does not start with a digit by prefixing with 'COL_'.\n",
    "    - Replaces multiple consecutive underscores with a single underscore.\n",
    "    - Truncates column names longer than max_length characters.\n",
    "    - Ensures uniqueness by appending a number to duplicate names.\n",
    "    - Removes any leading or trailing underscores.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "        max_length (int): Maximum allowed length for column names. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with cleaned and standardized column names.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a PySpark DataFrame or if max_length is not positive.\n",
    "        RuntimeError: If cleaning results in empty column names or non-unique names.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"Alice\", 34, \"NY\"), (\"Bob\", 45, \"CA\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"Name \", \"1age\", \"St@te!\"])\n",
    "        >>> df_clean = clean_column_names(df)\n",
    "        >>> df_clean.printSchema()\n",
    "        root\n",
    "         |-- NAME: string (nullable = true)\n",
    "         |-- COL_1AGE: long (nullable = true)\n",
    "         |-- STATE: string (nullable = true)\n",
    "    \"\"\"\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input must be a PySpark DataFrame\")\n",
    "    if max_length <= 0:\n",
    "        raise ValueError(\"max_length must be a positive integer\")\n",
    "\n",
    "    def clean_name(name: str) -> str:\n",
    "        # Convert to uppercase and trim\n",
    "        name = name.upper().strip()\n",
    "        # Replace invalid characters with underscores\n",
    "        name = re.sub(r\"[^\\w\\s]\", \"_\", name)\n",
    "        # Replace spaces with underscores\n",
    "        name = name.replace(\" \", \"_\")\n",
    "        # Replace multiple consecutive underscores with a single underscore\n",
    "        name = re.sub(r\"_+\", \"_\", name)\n",
    "        # Ensure the column does not start with a digit\n",
    "        if name[0].isdigit():\n",
    "            name = f\"COL_{name}\"\n",
    "        # Remove leading and trailing underscores\n",
    "        name = name.strip(\"_\")\n",
    "        # Truncate to max_length characters if longer\n",
    "        name = name[:max_length]\n",
    "        return name\n",
    "\n",
    "    # Clean all column names\n",
    "    new_column_names = [clean_name(col) for col in df.columns]\n",
    "\n",
    "    # Check for empty column names after cleaning\n",
    "    if \"\" in new_column_names:\n",
    "        raise RuntimeError(\"Cleaning resulted in empty column name(s)\")\n",
    "\n",
    "    # Ensure uniqueness\n",
    "    name_count: dict = {}\n",
    "    unique_names: List[str] = []\n",
    "    for name in new_column_names:\n",
    "        if name in name_count:\n",
    "            name_count[name] += 1\n",
    "            unique_name = f\"{name}_{name_count[name]}\"\n",
    "            # Truncate again if necessary after adding the suffix\n",
    "            unique_name = unique_name[:max_length]\n",
    "            unique_names.append(unique_name)\n",
    "        else:\n",
    "            name_count[name] = 0\n",
    "            unique_names.append(name)\n",
    "\n",
    "    # Check if we still have unique names after truncation\n",
    "    if len(set(unique_names)) != len(df.columns):\n",
    "        raise RuntimeError(\n",
    "            \"Unable to generate unique column names within the specified length\"\n",
    "        )\n",
    "\n",
    "    # Create a list of (old_name, new_name) tuples\n",
    "    column_mapping: List[Tuple[str, str]] = list(zip(df.columns, unique_names))\n",
    "\n",
    "    # Rename columns using selectExpr for better performance with many columns\n",
    "    expr = [f\"`{old}` as `{new}`\" for old, new in column_mapping]\n",
    "    return df.selectExpr(*expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_names(\n",
    "    df: DataFrame, case: Literal[\"upper\", \"lower\", \"title\"] = \"lower\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the column names of a PySpark DataFrame to the specified case.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "        case (Literal[\"upper\", \"lower\", \"title\"]): The case to convert the column names to.\n",
    "            'upper': Converts all column names to uppercase.\n",
    "            'lower': Converts all column names to lowercase.\n",
    "            'title': Converts the first letter of each word in column names to uppercase.\n",
    "            Defaults to \"lower\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with converted column names.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a PySpark DataFrame or if the case is invalid.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"Alice\", 34, \"NY\"), (\"Bob\", 45, \"CA\"), (\"Charlie\", 29, \"TX\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"Name\", \"Age\", \"State\"])\n",
    "        >>> df_upper = convert_column_names(df, 'upper')\n",
    "        >>> df_upper.show()\n",
    "        +-------+---+-----+\n",
    "        |   NAME|AGE|STATE|\n",
    "        +-------+---+-----+\n",
    "        |  Alice| 34|   NY|\n",
    "        |    Bob| 45|   CA|\n",
    "        |Charlie| 29|   TX|\n",
    "        +-------+---+-----+\n",
    "        >>> df_title = convert_column_names(df, 'title')\n",
    "        >>> df_title.show()\n",
    "        +-------+---+-----+\n",
    "        |   Name|Age|State|\n",
    "        +-------+---+-----+\n",
    "        |  Alice| 34|   NY|\n",
    "        |    Bob| 45|   CA|\n",
    "        |Charlie| 29|   TX|\n",
    "        +-------+---+-----+\n",
    "    \"\"\"\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input must be a PySpark DataFrame\")\n",
    "\n",
    "    if case not in [\"upper\", \"lower\", \"title\"]:\n",
    "        raise ValueError(\n",
    "            \"The 'case' parameter must be one of 'upper', 'lower', or 'title'.\"\n",
    "        )\n",
    "\n",
    "    case_functions = {\"upper\": str.upper, \"lower\": str.lower, \"title\": str.title}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_string_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Trims leading and trailing whitespace from all string columns in a PySpark DataFrame.\n",
    "\n",
    "    This function identifies all columns of StringType in the input DataFrame and applies\n",
    "    the trim operation to remove leading and trailing whitespace. Non-string columns are\n",
    "    left unchanged.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with all string columns trimmed.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a PySpark DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\" John \", \"  Doe  \", 30), (\"Alice  \", \" Smith \", 25)]\n",
    "        >>> df = spark.createDataFrame(data, [\"first_name\", \"last_name\", \"age\"])\n",
    "        >>> trimmed_df = trim_string_columns(df)\n",
    "        >>> trimmed_df.show()\n",
    "        +----------+---------+---+\n",
    "        |first_name|last_name|age|\n",
    "        +----------+---------+---+\n",
    "        |      John|      Doe| 30|\n",
    "        |     Alice|    Smith| 25|\n",
    "        +----------+---------+---+\n",
    "    \"\"\"\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input must be a PySpark DataFrame\")\n",
    "\n",
    "    # Create a list of column expressions\n",
    "    trim_exprs = [\n",
    "        (\n",
    "            F.trim(F.col(c)).alias(c)\n",
    "            if isinstance(df.schema[c].dataType, StringType)\n",
    "            else F.col(c)\n",
    "        )\n",
    "        for c in df.columns\n",
    "    ]\n",
    "\n",
    "    # Apply the trim expressions to the DataFrame\n",
    "    return df.select(*trim_exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "| Age| Name|               Group|\n",
      "+----+-----+--------------------+\n",
      "|  15| John|   Name contains 'o'|\n",
      "|  25| Jane|Age is 25, 35, or 55|\n",
      "|  35|  Doe|            35 - Doe|\n",
      "|  55|Alice|          55 - Alice|\n",
      "|  75|  Bob|            75 - Bob|\n",
      "|null|  Eve|            Has Name|\n",
      "|  30| null|               Adult|\n",
      "+----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_mapped_column(\n",
    "    df: DataFrame,\n",
    "    target_column: str,\n",
    "    mapping_list: List[Tuple[Column, Union[str, Callable[[DataFrame], Column]]]],\n",
    "    default_value: str = \"Unknown\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column to the DataFrame based on mappings defined in a list of tuples.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        target_column (str): Name of the new column to create.\n",
    "        mapping_list (List[Tuple[Column, Union[str, Callable[[DataFrame], Column]]]]): List with conditions as elements and mapped values or functions as values.\n",
    "        default_value (str): Default value for rows not matching any condition.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the new mapped column.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession, functions as F\n",
    "        >>> spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "        >>> data = [\n",
    "        >>>     (15, \"John\"),\n",
    "        >>>     (25, \"Jane\"),\n",
    "        >>>     (35, \"Doe\"),\n",
    "        >>>     (55, \"Alice\"),\n",
    "        >>>     (75, \"Bob\"),\n",
    "        >>>     (None, \"Eve\"),\n",
    "        >>>     (30, None),\n",
    "        >>> ]\n",
    "        >>> columns = [\"Age\", \"Name\"]\n",
    "        >>> df = spark.createDataFrame(data, columns)\n",
    "        >>> age_group_mapping = [\n",
    "        >>>     (F.col(\"Age\") < 18, \"Minor\"),\n",
    "        >>>     (F.col(\"Age\").isNull(), \"No Age\"),\n",
    "        >>>     ((F.col(\"Age\") >= 30) & (F.col(\"Age\") < 50), \"Adult\"),\n",
    "        >>>     (F.col(\"Name\").startswith(\"A\"), \"Name with A\"),\n",
    "        >>>     (F.col(\"Name\").rlike(\"^[Jj]\"), \"Starts with J or j\"),\n",
    "        >>>     (F.col(\"Name\").contains(\"a\"), \"Contains 'a'\"),\n",
    "        >>>     ((F.col(\"Age\") == 25) | (F.col(\"Age\") == 35), \"Specific Age 25 or 35\"),\n",
    "        >>>     (~F.col(\"Age\").between(18, 65), \"Not between 18 and 65\"),\n",
    "        >>>     (F.col(\"Name\").isNotNull(), \"Has Name\"),\n",
    "        >>>     (F.col(\"Age\").isin(25, 35, 55), \"Age is 25, 35, or 55\"),\n",
    "        >>>     (F.col(\"Name\").like(\"%o%\"), \"Name contains 'o'\"),\n",
    "        >>>     (F.col(\"Age\") > 10, lambda df: F.concat_ws(\" - \", df[\"Age\"], df[\"Name\"])),\n",
    "        >>> ]\n",
    "        >>> df_with_age_group = add_mapped_column(df, \"Group\", age_group_mapping)\n",
    "        >>> df_with_age_group.show()\n",
    "    \"\"\"\n",
    "    # Start with the default value\n",
    "    when_expr: Column = F.lit(default_value)\n",
    "\n",
    "    # Iterate over the mapping list and build the condition expression\n",
    "    for condition, value in mapping_list:\n",
    "        if callable(value):\n",
    "            when_expr = F.when(condition, value(df)).otherwise(when_expr)\n",
    "        else:\n",
    "            when_expr = F.when(condition, value).otherwise(when_expr)\n",
    "\n",
    "    # Add the new column to the DataFrame\n",
    "    df_with_new_column = df.withColumn(target_column, when_expr)\n",
    "\n",
    "    return df_with_new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ Column id has duplicate values.\n",
      "ðŸ’¡ Examples: 1\n",
      "âœ… Uniqueness check passed for: id, value\n",
      "âœ… Uniqueness check passed for: id, value, category\n"
     ]
    }
   ],
   "source": [
    "def assert_unique_combination(df: DataFrame, columns: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Assert that specified columns or their combination have unique values in the DataFrame.\n",
    "\n",
    "    This function checks for uniqueness in the specified columns. If a single column\n",
    "    is provided, it checks that column for uniqueness. If multiple columns are provided,\n",
    "    it checks the combination of these columns for uniqueness.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame to check.\n",
    "        columns (List[str]): List of column names to check for uniqueness.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If duplicate values are found in the specified column(s).\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.createDataFrame([(1, 'A'), (2, 'B'), (1, 'C')], ['id', 'value'])\n",
    "        >>> assert_unique_combination(df, ['id'])  # Raises ValueError\n",
    "        >>> assert_unique_combination(df, ['id', 'value'])  # No error\n",
    "    \"\"\"\n",
    "    if not columns:\n",
    "        raise ValueError(\"At least one column must be specified.\")\n",
    "\n",
    "    if len(columns) == 1:\n",
    "        check_column = columns[0]\n",
    "    else:\n",
    "        check_column = \"unique_combo_\" + \"_\".join(columns)\n",
    "        concat_expr = F.concat_ws(\"||\", *columns)\n",
    "        df = df.withColumn(check_column, concat_expr)\n",
    "\n",
    "    duplicates = df.groupBy(check_column).count().filter(F.col(\"count\") > 1)\n",
    "\n",
    "    if duplicates.count() > 0:\n",
    "        dup_values = duplicates.select(check_column).limit(3).collect()\n",
    "        dup_list = [row[check_column] for row in dup_values]\n",
    "\n",
    "        if len(columns) == 1:\n",
    "            error_message = f\"ðŸš¨ Column {columns[0]} has duplicate values.\"\n",
    "        else:\n",
    "            error_message = f\"ðŸš¨ Combination of {', '.join(columns)} is not unique.\"\n",
    "\n",
    "        error_message += f\"\\nðŸ’¡ Examples: {', '.join(map(str, dup_list))}\"\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    if len(columns) > 1:\n",
    "        df = df.drop(check_column)\n",
    "\n",
    "    print(f\"âœ… Uniqueness check passed for: {', '.join(columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_values(\n",
    "    df: DataFrame,\n",
    "    columns: Optional[Union[str, List[str]]] = None,\n",
    "    missing_patterns: Optional[Dict[str, List[str]]] = None,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Replace values indicating missing data with None in specified columns of a PySpark DataFrame.\n",
    "\n",
    "    This function identifies and replaces various forms of missing values, including:\n",
    "    - Cells containing only special characters\n",
    "    - Empty strings\n",
    "    - Variations of the word \"null\" (including when surrounded by other characters)\n",
    "    - Custom patterns provided by the user\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        columns (Optional[Union[str, List[str]]]): Column(s) to process. If None, all columns are processed.\n",
    "        missing_patterns (Optional[Dict[str, List[str]]]): Custom patterns to identify missing values.\n",
    "            Keys are column names, values are lists of regex patterns.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with missing values replaced by None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input DataFrame is empty or if specified columns don't exist.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.appName(\"MissingValueExample\").getOrCreate()\n",
    "        >>> data = [(\"John\", \" \"), (\"Jane\", \"(null)\"), (\"Bob\", \"@#$\"), (\"Alice\", \"N/A\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"name\", \"value\"])\n",
    "        >>> custom_patterns = {\"value\": [r\"N/A\"]}\n",
    "        >>> result_df = replace_missing_values(df, \"value\", custom_patterns)\n",
    "        >>> result_df.show()\n",
    "        +-----+-----+\n",
    "        | name|value|\n",
    "        +-----+-----+\n",
    "        | John| NULL|\n",
    "        | Jane| NULL|\n",
    "        |  Bob| NULL|\n",
    "        |Alice| NULL|\n",
    "        +-----+-----+\n",
    "    \"\"\"\n",
    "    if df.rdd.isEmpty():\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    elif isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    non_existent_cols = set(columns) - set(df.columns)\n",
    "    if non_existent_cols:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {non_existent_cols}\")\n",
    "\n",
    "    missing_patterns = missing_patterns or {}\n",
    "\n",
    "    logger.info(f\"Processing {len(columns)} columns\")\n",
    "\n",
    "    def get_missing_value_expression(column: str) -> Column:\n",
    "        base_patterns = [\n",
    "            r\"^[\\s\\W]*$\",  # Only special characters or whitespace\n",
    "            r\"^\\s*$\",  # Empty string\n",
    "            r\".*null.*\",  # \"null\" surrounded by any characters\n",
    "        ]\n",
    "        custom_patterns = missing_patterns.get(column, [])\n",
    "        all_patterns = base_patterns + custom_patterns\n",
    "\n",
    "        return when(\n",
    "            trim(lower(col(column))).rlike(\n",
    "                \"|\".join(f\"({pattern})\" for pattern in all_patterns)\n",
    "            )\n",
    "            | col(column).isNull(),\n",
    "            lit(None),\n",
    "        ).otherwise(col(column))\n",
    "\n",
    "    # Preserve original column types\n",
    "    original_types = {\n",
    "        field.name: field.dataType\n",
    "        for field in df.schema.fields\n",
    "        if field.name in columns\n",
    "    }\n",
    "\n",
    "    # Process all columns in a single select operation\n",
    "    expressions = [\n",
    "        get_missing_value_expression(c).alias(c) if c in columns else col(c)\n",
    "        for c in df.columns\n",
    "    ]\n",
    "\n",
    "    result_df = df.select(*expressions)\n",
    "\n",
    "    # Restore original column types\n",
    "    for column, data_type in original_types.items():\n",
    "        if not isinstance(data_type, StringType):\n",
    "            result_df = result_df.withColumn(column, col(column).cast(data_type))\n",
    "\n",
    "    logger.info(\"Missing value replacement completed\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_data(\n",
    "    base_path: str,\n",
    "    folder_name: Optional[str] = None,\n",
    "    file_pattern: Optional[str] = None,\n",
    "    encoding: str = \"UTF-8\",\n",
    "    sep: str = \";\",\n",
    "    header: bool = True,\n",
    "    include_subdirectories: bool = True,\n",
    "    cache_result: bool = False,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve and combine CSV data from a specified path and optionally from a specific folder.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path where the CSV data is located.\n",
    "        folder_name (Optional[str]): Specific folder name within the base path. If None, base_path is used.\n",
    "        file_pattern (Optional[str]): String pattern to match in file names. If None, all CSV files are considered.\n",
    "        encoding (str): Encoding of the CSV files. Defaults to \"UTF-8\".\n",
    "        sep (str): Separator used in the CSV files. Defaults to \";\".\n",
    "        header (bool): Whether the CSV files have a header row. Defaults to True.\n",
    "        include_subdirectories (bool): Whether to include subdirectories in the search. Defaults to True.\n",
    "        cache_result (bool): Whether to cache the resulting DataFrame. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame of all relevant CSV files.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified path or folder does not exist.\n",
    "        ValueError: If no CSV files are found in the specified location.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Construct the full path\n",
    "    full_path = f\"{base_path}/{folder_name}\" if folder_name else base_path\n",
    "\n",
    "    # List all CSV files\n",
    "    if include_subdirectories:\n",
    "        files = dbutils.fs.ls(full_path)\n",
    "        csv_files = [\n",
    "            f.path\n",
    "            for f in files\n",
    "            if f.path.endswith(\".csv\")\n",
    "            and (file_pattern is None or file_pattern in f.name)\n",
    "        ]\n",
    "    else:\n",
    "        files = dbutils.fs.ls(full_path)\n",
    "        csv_files = [\n",
    "            f.path\n",
    "            for f in files\n",
    "            if f.path.endswith(\".csv\")\n",
    "            and (file_pattern is None or file_pattern in f.name)\n",
    "        ]\n",
    "\n",
    "    if not csv_files:\n",
    "        raise ValueError(\n",
    "            f\"ðŸ” No CSV files found matching the pattern: {file_pattern} in {full_path} ðŸ”\"\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ“Š Found {len(csv_files)} CSV files matching the criteria\")\n",
    "\n",
    "    # Infer schema from all files\n",
    "    inferred_schema = infer_schema(csv_files[0], sep, header)\n",
    "\n",
    "    # Read and combine CSV files\n",
    "    def read_csv(file_path: str) -> DataFrame:\n",
    "        return spark.read.csv(\n",
    "            file_path,\n",
    "            schema=inferred_schema,\n",
    "            header=header,\n",
    "            sep=sep,\n",
    "            encoding=encoding,\n",
    "            ignoreLeadingWhiteSpace=True,\n",
    "            ignoreTrailingWhiteSpace=True,\n",
    "        ).withColumn(\"source_file\", lit(file_path.split(\"/\")[-1]))\n",
    "\n",
    "    dfs = spark.sparkContext.parallelize(csv_files).map(read_csv).collect()\n",
    "\n",
    "    if not dfs:\n",
    "        raise ValueError(f\"ðŸ“­ No valid CSV data could be read from {full_path}\")\n",
    "\n",
    "    # Combine all DataFrames using unionByName with checkpointing\n",
    "    result_df = reduce(\n",
    "        lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs\n",
    "    )\n",
    "    result_df = result_df.checkpoint()\n",
    "\n",
    "    if cache_result:\n",
    "        result_df = result_df.cache()\n",
    "\n",
    "    # Calculate and log statistics\n",
    "    total_rows = result_df.count()\n",
    "    column_stats = calculate_column_stats(result_df)\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    print(\n",
    "        f\"ðŸŽ‰ Successfully combined {len(dfs)} CSV files into a DataFrame with {total_rows} rows in {duration:.2f} seconds! ðŸŽ‰\"\n",
    "    )\n",
    "    print(\"Column Statistics:\")\n",
    "    for col, stats in column_stats.items():\n",
    "        print(f\"  {col}: {stats}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def infer_schema(file_path: str, sep: str, header: bool) -> StructType:\n",
    "    \"\"\"Infer the schema from the first file\"\"\"\n",
    "    return spark.read.csv(file_path, sep=sep, header=header, inferSchema=True).schema\n",
    "\n",
    "\n",
    "def calculate_column_stats(df: DataFrame) -> dict:\n",
    "    \"\"\"Calculate basic statistics for each column\"\"\"\n",
    "    return {\n",
    "        col: df.agg(\n",
    "            count(col).alias(\"count\"),\n",
    "            mean(col).alias(\"mean\"),\n",
    "            stddev(col).alias(\"stddev\"),\n",
    "            min(col).alias(\"min\"),\n",
    "            max(col).alias(\"max\"),\n",
    "        )\n",
    "        .collect()[0]\n",
    "        .asDict()\n",
    "        for col in df.columns\n",
    "        if df.schema[col].dataType.typeName() in [\"double\", \"float\", \"int\", \"long\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_join(\n",
    "    left_df: DataFrame,\n",
    "    right_df: DataFrame,\n",
    "    on: Union[str, List[str]],\n",
    "    how: str = \"inner\",\n",
    "    suffix_left: str = \"_left\",\n",
    "    suffix_right: str = \"_right\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join two DataFrames with advanced features like automatic column renaming for duplicates.\n",
    "\n",
    "    This function performs a join operation on two DataFrames and automatically renames\n",
    "    columns with the same name in both DataFrames to avoid confusion.\n",
    "\n",
    "    Args:\n",
    "        left_df (DataFrame): The left DataFrame for the join operation.\n",
    "        right_df (DataFrame): The right DataFrame for the join operation.\n",
    "        on (Union[str, List[str]]): Column(s) to join on. Can be a string for single column\n",
    "                                    or a list of strings for multiple columns.\n",
    "        how (str, optional): Type of join to perform. Defaults to \"inner\".\n",
    "                             Options: \"inner\", \"outer\", \"left\", \"right\", \"leftsemi\", \"leftanti\".\n",
    "        suffix_left (str, optional): Suffix to add to duplicate column names from left DataFrame.\n",
    "                                     Defaults to \"_left\".\n",
    "        suffix_right (str, optional): Suffix to add to duplicate column names from right DataFrame.\n",
    "                                      Defaults to \"_right\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame resulting from the join operation with renamed columns.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid join type is specified.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.appName(\"AdvancedJoinExample\").getOrCreate()\n",
    "        >>>\n",
    "        >>> # Create sample DataFrames\n",
    "        >>> df1 = spark.createDataFrame([(1, \"A\", 100), (2, \"B\", 200), (3, \"C\", 300)], [\"id\", \"name\", \"value\"])\n",
    "        >>> df2 = spark.createDataFrame([(1, \"X\", 1000), (2, \"Y\", 2000), (4, \"Z\", 4000)], [\"id\", \"category\", \"value\"])\n",
    "        >>>\n",
    "        >>> # Perform advanced join\n",
    "        >>> result = advanced_join(df1, df2, on=\"id\", how=\"left\")\n",
    "        >>> result.show()\n",
    "        +---+----+----------+--------+-----------+\n",
    "        | id|name|value_left|category|value_right|\n",
    "        +---+----+----------+--------+-----------+\n",
    "        |  1|   A|       100|       X|       1000|\n",
    "        |  2|   B|       200|       Y|       2000|\n",
    "        |  3|   C|       300|    null|       null|\n",
    "        +---+----+----------+--------+-----------+\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if how not in [\"inner\", \"outer\", \"left\", \"right\", \"leftsemi\", \"leftanti\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid join type: {how}. Supported types are: inner, outer, left, right, leftsemi, leftanti\"\n",
    "        )\n",
    "\n",
    "    # Get the list of common column names (excluding join columns)\n",
    "    if isinstance(on, str):\n",
    "        on = [on]\n",
    "    left_columns = set(left_df.columns) - set(on)\n",
    "    right_columns = set(right_df.columns) - set(on)\n",
    "    common_columns = list(left_columns.intersection(right_columns))\n",
    "\n",
    "    # Rename common columns in both DataFrames\n",
    "    left_renamed = left_df.select(\n",
    "        *[\n",
    "            col(c).alias(f\"{c}{suffix_left}\") if c in common_columns else col(c)\n",
    "            for c in left_df.columns\n",
    "        ]\n",
    "    )\n",
    "    right_renamed = right_df.select(\n",
    "        *[\n",
    "            col(c).alias(f\"{c}{suffix_right}\") if c in common_columns else col(c)\n",
    "            for c in right_df.columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Perform the join operation\n",
    "    joined_df = left_renamed.join(right_renamed, on=on, how=how)\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def convert_to_timestamp(\n",
    "    df: DataFrame,\n",
    "    columns: Union[str, List[str]],\n",
    "    format: str = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert string column(s) to timestamp format in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        columns (Union[str, List[str]]): Column name(s) to convert.\n",
    "        format (str, optional): Timestamp format string. Defaults to \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with converted timestamp column(s).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input column(s) are not present in the DataFrame.\n",
    "\n",
    "    Examples:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"2023-07-27T13:18:12.039+0000\",), (\"2023-07-28T13:20:30.039+0000\",)]\n",
    "        >>> df = spark.createDataFrame(data, [\"ZEITPUNKT\"])\n",
    "        >>> result_df = convert_to_timestamp(df, \"ZEITPUNKT\")\n",
    "        >>> result_df.printSchema()\n",
    "        root\n",
    "         |-- ZEITPUNKT: timestamp (nullable = true)\n",
    "\n",
    "        >>> result_df.show(truncate=False)\n",
    "        +----------------------------+\n",
    "        |ZEITPUNKT                   |\n",
    "        +----------------------------+\n",
    "        |2023-07-27 13:18:12.039     |\n",
    "        |2023-07-28 13:20:30.039     |\n",
    "        +----------------------------+\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    # Validate input columns\n",
    "    invalid_columns = set(columns) - set(df.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns are not present in the DataFrame: {invalid_columns}\"\n",
    "        )\n",
    "\n",
    "    # Convert string columns to timestamp\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col, to_timestamp(df[col], format))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_current_data(\n",
    "    df: DataFrame,\n",
    "    partition_columns: Union[str, List[str]],\n",
    "    timestamp_column: str,\n",
    "    ascending: bool = False,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the most current dataset from a DataFrame containing historical data.\n",
    "\n",
    "    This function partitions the data based on specified column(s) and selects the\n",
    "    most recent record for each partition using a timestamp column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame containing historical data.\n",
    "        partition_columns (Union[str, List[str]]): Column(s) to partition the data.\n",
    "            Can be a string for a single column or a list of strings for multiple columns.\n",
    "        timestamp_column (str): Name of the column containing timestamp information.\n",
    "        ascending (bool, optional): If True, selects the earliest record.\n",
    "            If False (default), selects the latest record.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame containing only the most current records.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified columns are not present in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> from pyspark.sql.functions import to_timestamp\n",
    "        >>>\n",
    "        >>> spark = SparkSession.builder.appName(\"MostCurrentDataExample\").getOrCreate()\n",
    "        >>>\n",
    "        >>> # Create a sample DataFrame with historical data\n",
    "        >>> data = [\n",
    "        ...     (1, \"A\", \"2023-01-01 10:00:00\", 100),\n",
    "        ...     (1, \"A\", \"2023-01-02 11:00:00\", 150),\n",
    "        ...     (2, \"B\", \"2023-01-01 09:00:00\", 200),\n",
    "        ...     (2, \"B\", \"2023-01-03 12:00:00\", 250),\n",
    "        ...     (3, \"C\", \"2023-01-02 08:00:00\", 300),\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"id\", \"category\", \"timestamp\", \"value\"])\n",
    "        >>> df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "        >>>\n",
    "        >>> # Get the most current data\n",
    "        >>> result = get_most_current_data(df, [\"id\", \"category\"], \"timestamp\")\n",
    "        >>> result.show()\n",
    "        +---+--------+-------------------+-----+\n",
    "        | id|category|          timestamp|value|\n",
    "        +---+--------+-------------------+-----+\n",
    "        |  1|       A|2023-01-02 11:00:00|  150|\n",
    "        |  2|       B|2023-01-03 12:00:00|  250|\n",
    "        |  3|       C|2023-01-02 08:00:00|  300|\n",
    "        +---+--------+-------------------+-----+\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    all_columns = df.columns\n",
    "    if isinstance(partition_columns, str):\n",
    "        partition_columns = [partition_columns]\n",
    "\n",
    "    invalid_columns = set(partition_columns + [timestamp_column]) - set(all_columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns are not present in the DataFrame: {invalid_columns}\"\n",
    "        )\n",
    "\n",
    "    # Create a window specification\n",
    "    window_spec = Window.partitionBy(partition_columns).orderBy(\n",
    "        col(timestamp_column).desc()\n",
    "    )\n",
    "\n",
    "    if ascending:\n",
    "        window_spec = Window.partitionBy(partition_columns).orderBy(\n",
    "            col(timestamp_column).asc()\n",
    "        )\n",
    "\n",
    "    # Add row number column\n",
    "    df_with_row_number = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "    # Filter to keep only the most current record for each partition\n",
    "    most_current_df = df_with_row_number.filter(col(\"row_number\") == 1).drop(\n",
    "        \"row_number\"\n",
    "    )\n",
    "\n",
    "    return most_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+-----+----+-------------+\n",
      "|category|total_value|avg_value|count|rank|running_total|\n",
      "+--------+-----------+---------+-----+----+-------------+\n",
      "|       A|        250|    125.0|    2|   1|          250|\n",
      "|       B|        450|    225.0|    2|   1|          450|\n",
      "|       C|        650|    325.0|    2|   1|          650|\n",
      "+--------+-----------+---------+-----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def apply_groupby_aggregations(\n",
    "    df: DataFrame,\n",
    "    group_cols: List[str],\n",
    "    agg_expressions: Dict[str, Union[str, F.Column]],\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply groupBy and aggregations on a PySpark DataFrame.\n",
    "\n",
    "    This function groups the input DataFrame by the specified columns and applies\n",
    "    the given aggregation expressions. It supports both string-based and Column-based\n",
    "    aggregation expressions.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        group_cols (List[str]): List of column names to group by.\n",
    "        agg_expressions (Dict[str, Union[str, F.Column]]): Dictionary of aggregation expressions.\n",
    "            Keys are the resulting column names, and values are either string expressions\n",
    "            or PySpark Column objects representing the aggregation.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with the grouping and aggregations applied.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is not a PySpark DataFrame, if group_cols is empty,\n",
    "                    or if agg_expressions is empty.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"A\", 1, 100), (\"B\", 2, 200), (\"A\", 3, 300), (\"B\", 4, 400)]\n",
    "        >>> df = spark.createDataFrame(data, [\"group\", \"value1\", \"value2\"])\n",
    "        >>> group_cols = [\"group\"]\n",
    "        >>> agg_expressions = {\n",
    "        ...     \"sum_value1\": \"sum(value1)\",\n",
    "        ...     \"avg_value2\": F.avg(\"value2\"),\n",
    "        ...     \"count\": F.count(\"*\")\n",
    "        ... }\n",
    "        >>> result_df = apply_groupby_aggregations(df, group_cols, agg_expressions)\n",
    "        >>> result_df.show()\n",
    "        +-----+---------+---------+-----+\n",
    "        |group|sum_value1|avg_value2|count|\n",
    "        +-----+---------+---------+-----+\n",
    "        |    B|        6|    300.0|    2|\n",
    "        |    A|        4|    200.0|    2|\n",
    "        +-----+---------+---------+-----+\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a PySpark DataFrame\")\n",
    "    if not group_cols:\n",
    "        raise ValueError(\"'group_cols' must not be empty\")\n",
    "    if not agg_expressions:\n",
    "        raise ValueError(\"'agg_expressions' must not be empty\")\n",
    "\n",
    "    # Ensure all group columns exist in the DataFrame\n",
    "    missing_cols = set(group_cols) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(\n",
    "            f\"Group columns not found in DataFrame: {', '.join(missing_cols)}\"\n",
    "        )\n",
    "\n",
    "    # Process aggregation expressions\n",
    "    agg_cols = []\n",
    "    for col_name, expr in agg_expressions.items():\n",
    "        if isinstance(expr, str):\n",
    "            agg_cols.append(F.expr(expr).alias(col_name))\n",
    "        elif isinstance(expr, F.Column):\n",
    "            agg_cols.append(expr.alias(col_name))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid aggregation expression for '{col_name}'. \"\n",
    "                \"Must be a string or Column object.\"\n",
    "            )\n",
    "\n",
    "    # Apply groupBy and aggregations\n",
    "    return df.groupBy(*group_cols).agg(*agg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_window_calculations(\n",
    "    df: DataFrame,\n",
    "    partition_cols: List[str],\n",
    "    order_cols: List[str],\n",
    "    window_expressions: Dict[str, Union[str, F.Column, Callable]],\n",
    "    ascending: Union[bool, List[bool]] = True,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add window function calculations to a PySpark DataFrame.\n",
    "\n",
    "    This function applies window calculations to the input DataFrame based on the specified\n",
    "    partition and order columns. It supports various types of window expressions and allows\n",
    "    for flexible ordering.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        partition_cols (List[str]): List of column names to partition by.\n",
    "        order_cols (List[str]): List of column names to order by.\n",
    "        window_expressions (Dict[str, Union[str, F.Column, Callable]]): Dictionary of window expressions.\n",
    "            Keys are the resulting column names, and values can be:\n",
    "            - String: A SQL-like expression (e.g., \"rank()\")\n",
    "            - Column: A PySpark Column object (e.g., F.rank())\n",
    "            - Callable: A function that takes a Window spec and returns a Column\n",
    "        ascending (Union[bool, List[bool]]): Specifies the ordering direction.\n",
    "            If a single boolean, it applies to all order columns.\n",
    "            If a list, it should match the length of order_cols. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with additional window calculations.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If inputs are invalid or if columns are not found in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"A\", 1, 100), (\"A\", 2, 150), (\"B\", 1, 200), (\"B\", 2, 250)]\n",
    "        >>> df = spark.createDataFrame(data, [\"group\", \"subgroup\", \"value\"])\n",
    "        >>> window_expressions = {\n",
    "        ...     \"row_number\": F.row_number(),\n",
    "        ...     \"rank\": \"rank()\",\n",
    "        ...     \"running_total\": F.sum(\"value\").over,\n",
    "        ...     \"lag_value\": F.lag(\"value\", 1).over\n",
    "        ... }\n",
    "        >>> result_df = add_window_calculations(\n",
    "        ...     df,\n",
    "        ...     partition_cols=[\"group\"],\n",
    "        ...     order_cols=[\"subgroup\", \"value\"],\n",
    "        ...     window_expressions=window_expressions,\n",
    "        ...     ascending=[True, False]\n",
    "        ... )\n",
    "        >>> result_df.show()\n",
    "        +-----+--------+-----+----------+----+-------------+---------+\n",
    "        |group|subgroup|value|row_number|rank|running_total|lag_value|\n",
    "        +-----+--------+-----+----------+----+-------------+---------+\n",
    "        |    A|       2|  150|         1|   1|          150|     null|\n",
    "        |    A|       1|  100|         2|   2|          250|      150|\n",
    "        |    B|       2|  250|         1|   1|          250|     null|\n",
    "        |    B|       1|  200|         2|   2|          450|      250|\n",
    "        +-----+--------+-----+----------+----+-------------+---------+\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a PySpark DataFrame\")\n",
    "    if not partition_cols:\n",
    "        raise ValueError(\"'partition_cols' must not be empty\")\n",
    "    if not order_cols:\n",
    "        raise ValueError(\"'order_cols' must not be empty\")\n",
    "    if not window_expressions:\n",
    "        raise ValueError(\"'window_expressions' must not be empty\")\n",
    "\n",
    "    # Validate columns exist in DataFrame\n",
    "    all_cols = set(partition_cols + order_cols)\n",
    "    missing_cols = all_cols - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {', '.join(missing_cols)}\")\n",
    "\n",
    "    # Handle ascending parameter\n",
    "    if isinstance(ascending, bool):\n",
    "        ascending = [ascending] * len(order_cols)\n",
    "    elif isinstance(ascending, list):\n",
    "        if len(ascending) != len(order_cols):\n",
    "            raise ValueError(\"Length of 'ascending' list must match 'order_cols'\")\n",
    "    else:\n",
    "        raise ValueError(\"'ascending' must be a boolean or a list of booleans\")\n",
    "\n",
    "    # Create window specification\n",
    "    window_spec = Window.partitionBy(*partition_cols).orderBy(\n",
    "        *[col if asc else col.desc() for col, asc in zip(order_cols, ascending)]\n",
    "    )\n",
    "\n",
    "    # Apply window expressions\n",
    "    for column, expr in window_expressions.items():\n",
    "        if isinstance(expr, str):\n",
    "            df = df.withColumn(column, F.expr(expr).over(window_spec))\n",
    "        elif isinstance(expr, F.Column):\n",
    "            df = df.withColumn(column, expr.over(window_spec))\n",
    "        elif callable(expr):\n",
    "            df = df.withColumn(column, expr(window_spec))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid window expression for '{column}'. \"\n",
    "                \"Must be a string, Column object, or callable.\"\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df: DataFrame, categorical_columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on specified categorical columns.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    categorical_columns (list): List of categorical column names to encode\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with one-hot encoded columns\n",
    "    \"\"\"\n",
    "    stages = []\n",
    "    for cat_col in categorical_columns:\n",
    "        string_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_index\")\n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{cat_col}_index\"], outputCols=[f\"{cat_col}_encoded\"]\n",
    "        )\n",
    "        stages += [string_indexer, encoder]\n",
    "\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    model = pipeline.fit(df)\n",
    "    encoded_df = model.transform(df)\n",
    "\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: DataFrame, subset: list = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    subset (list): List of columns to consider for duplicates (default: None, uses all columns)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    if subset:\n",
    "        return df.dropDuplicates(subset=subset)\n",
    "    else:\n",
    "        return df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sql_query(\n",
    "    df: DataFrame, query: str, view_name: str = \"temp_view\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a custom SQL query to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    query (str): SQL query to apply\n",
    "    view_name (str): Name for the temporary view (default: \"temp_view\")\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Result of the SQL query\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(view_name)\n",
    "    result = df.sparkSession.sql(query)\n",
    "    df.sparkSession.catalog.dropTempView(view_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_numeric_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for all numeric columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Summary statistics including count, mean, stddev, min, and max for each numeric column\n",
    "    \"\"\"\n",
    "    numeric_columns = [f.name for f in df.schema.fields if f.dataType.simpleString() in ('double', 'float', 'int', 'long')]\n",
    "    \n",
    "    summary = df.select([\n",
    "        F.count(F.col(c)).alias(f\"{c}_count\"),\n",
    "        F.mean(F.col(c)).alias(f\"{c}_mean\"),\n",
    "        F.stddev(F.col(c)).alias(f\"{c}_stddev\"),\n",
    "        F.min(F.col(c)).alias(f\"{c}_min\"),\n",
    "        F.max(F.col(c)).alias(f\"{c}_max\")\n",
    "    for c in numeric_columns])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def identify_outliers(df: DataFrame, column: str, lower_quantile: float = 0.25, upper_quantile: float = 0.75, iqr_multiplier: float = 1.5) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Identify outliers in a specified column using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    column (str): Name of the column to check for outliers\n",
    "    lower_quantile (float): Lower quantile for IQR calculation (default: 0.25)\n",
    "    upper_quantile (float): Upper quantile for IQR calculation (default: 0.75)\n",
    "    iqr_multiplier (float): Multiplier for IQR to determine outlier boundaries (default: 1.5)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Original DataFrame with an additional boolean column indicating outliers\n",
    "    \"\"\"\n",
    "    quantiles = df.approxQuantile(column, [lower_quantile, upper_quantile], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "    \n",
    "    return df.withColumn(f\"{column}_is_outlier\", \n",
    "                         ~F.col(column).between(lower_bound, upper_bound))\n",
    "\n",
    "def pivot_and_unpivot(df: DataFrame, id_cols: List[str], pivot_col: str, value_col: str) -> Dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform both pivot and unpivot operations on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    id_cols (List[str]): List of columns to use as identifiers\n",
    "    pivot_col (str): Column to use for pivoting\n",
    "    value_col (str): Column containing the values to be pivoted\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, DataFrame]: Dictionary containing both pivoted and unpivoted DataFrames\n",
    "    \"\"\"\n",
    "    # Pivot operation\n",
    "    pivot_df = df.groupBy(id_cols).pivot(pivot_col).agg(F.first(value_col))\n",
    "    \n",
    "    # Unpivot operation\n",
    "    value_columns = [c for c in pivot_df.columns if c not in id_cols]\n",
    "    unpivot_expr = [F.expr(f\"stack({len(value_columns)}, {', '.join([f''''{c}', {c}''' for c in value_columns])}) as ({pivot_col}, {value_col})\")]\n",
    "    unpivot_df = pivot_df.select(*id_cols, *unpivot_expr)\n",
    "    \n",
    "    return {\"pivoted\": pivot_df, \"unpivoted\": unpivot_df}\n",
    "\n",
    "\n",
    "def compare_dataframes(df1: DataFrame, df2: DataFrame, join_columns: List[str]) -> Dict[str, Union[DataFrame, int]]:\n",
    "    \"\"\"\n",
    "    Compare two DataFrames and identify differences.\n",
    "\n",
    "    Args:\n",
    "    df1 (DataFrame): First DataFrame\n",
    "    df2 (DataFrame): Second DataFrame\n",
    "    join_columns (List[str]): Columns to use for joining the DataFrames\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Union[DataFrame, int]]: Dictionary containing DataFrames with differences and counts\n",
    "    \"\"\"\n",
    "    # Ensure both DataFrames have the same columns\n",
    "    columns = sorted(set(df1.columns + df2.columns))\n",
    "    df1 = df1.select(columns)\n",
    "    df2 = df2.select(columns)\n",
    "    \n",
    "    # Perform full outer join\n",
    "    joined = df1.join(df2, join_columns, \"full_outer\")\n",
    "    \n",
    "    # Identify rows present in df1 but not in df2\n",
    "    in_df1_not_df2 = joined.filter(' AND '.join([f\"(df2.`{col}` IS NULL OR df1.`{col}` != df2.`{col}`)\" for col in columns if col not in join_columns]))\n",
    "    \n",
    "    # Identify rows present in df2 but not in df1\n",
    "    in_df2_not_df1 = joined.filter(' AND '.join([f\"(df1.`{col}` IS NULL OR df1.`{col}` != df2.`{col}`)\" for col in columns if col not in join_columns]))\n",
    "    \n",
    "    return {\n",
    "        \"in_df1_not_df2\": in_df1_not_df1,\n",
    "        \"in_df2_not_df1\": in_df2_not_df1,\n",
    "        \"total_differences\": in_df1_not_df2.count() + in_df2_not_df1.count()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(df: DataFrame, date_column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add various date-related features to a DataFrame based on a specified date column.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    date_column (str): Name of the column containing date information\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with additional date-related columns\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.withColumn(\"year\", F.year(F.col(date_column)))\n",
    "        .withColumn(\"month\", F.month(F.col(date_column)))\n",
    "        .withColumn(\"day\", F.dayofmonth(F.col(date_column)))\n",
    "        .withColumn(\"day_of_week\", F.dayofweek(F.col(date_column)))\n",
    "        .withColumn(\"day_of_year\", F.dayofyear(F.col(date_column)))\n",
    "        .withColumn(\"week_of_year\", F.weekofyear(F.col(date_column)))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(date_column)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def profile_data_quality(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Profile the data quality of a DataFrame, including null counts, distinct counts, and data types.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Data quality profile including column name, data type, null count, distinct count, and sample values\n",
    "    \"\"\"\n",
    "    def sample_values(col_name):\n",
    "        return F.concat_ws(\", \", F.collect_set(F.col(col_name)).over(Window.partitionBy(F.lit(1)).rowsBetween(-1000, 1000)))\n",
    "\n",
    "    profile = df.select([\n",
    "        F.lit(c).alias(\"column_name\"),\n",
    "        F.lit(str(df.schema[c].dataType)).alias(\"data_type\"),\n",
    "        F.count(F.when(F.col(c).isNull(), c)).alias(\"null_count\"),\n",
    "        F.count(F.when(F.col(c).isNotNull(), c)).alias(\"non_null_count\"),\n",
    "        F.countDistinct(c).alias(\"distinct_count\"),\n",
    "        sample_values(c).alias(\"sample_values\")\n",
    "    for c in df.columns])\n",
    "\n",
    "    return profile.select(\"column_name\", \"data_type\", \"null_count\", \"non_null_count\", \"distinct_count\", \n",
    "                          F.expr(\"substring(sample_values, 1, 100)\").alias(\"sample_values\"))\n",
    "\n",
    "\n",
    "def binning(df: DataFrame, column: str, num_bins: int, strategy: str = \"equal_range\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform binning on a numeric column.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    column (str): Column to bin\n",
    "    num_bins (int): Number of bins\n",
    "    strategy (str): Binning strategy ('equal_range' or 'equal_frequency')\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with binned column added\n",
    "    \"\"\"\n",
    "    if strategy == \"equal_range\":\n",
    "        return df.withColumn(f\"{column}_binned\", F.ntile(num_bins).over(Window.orderBy(column)))\n",
    "    elif strategy == \"equal_frequency\":\n",
    "        quantiles = df.approxQuantile(column, [i/num_bins for i in range(1, num_bins)], 0.01)\n",
    "        return df.withColumn(f\"{column}_binned\", F.bucketizer(F.col(column), [-float(\"inf\")] + quantiles + [float(\"inf\")]))\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be either 'equal_range' or 'equal_frequency'\")\n",
    "\n",
    "def detect_data_drift(df1: DataFrame, df2: DataFrame, columns: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detect data drift between two DataFrames for specified columns.\n",
    "\n",
    "    Args:\n",
    "    df1 (DataFrame): First DataFrame (e.g., training data)\n",
    "    df2 (DataFrame): Second DataFrame (e.g., new data)\n",
    "    columns (List[str]): List of columns to check for drift\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Data drift statistics including column name, drift metric, and p-value\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import kurtosis, skewness, mean, stddev\n",
    "    from scipy import stats\n",
    "\n",
    "    drift_results = []\n",
    "\n",
    "    for column in columns:\n",
    "        stats1 = df1.select(mean(column).alias(\"mean\"), \n",
    "                            stddev(column).alias(\"std\"),\n",
    "                            kurtosis(column).alias(\"kurtosis\"),\n",
    "                            skewness(column).alias(\"skewness\")).collect()[0]\n",
    "        \n",
    "        stats2 = df2.select(mean(column).alias(\"mean\"), \n",
    "                            stddev(column).alias(\"std\"),\n",
    "                            kurtosis(column).alias(\"kurtosis\"),\n",
    "                            skewness(column).alias(\"skewness\")).collect()[0]\n",
    "\n",
    "        # Perform Kolmogorov-Smirnov test\n",
    "        ks_statistic, p_value = stats.ks_2samp(df1.select(column).rdd.flatMap(lambda x: x).collect(),\n",
    "                                               df2.select(column).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "        drift_results.append((column, ks_statistic, p_value, \n",
    "                              stats1.mean, stats2.mean, \n",
    "                              stats1.std, stats2.std,\n",
    "                              stats1.kurtosis, stats2.kurtosis,\n",
    "                              stats1.skewness, stats2.skewness))\n",
    "\n",
    "    drift_df = spark.createDataFrame(drift_results, \n",
    "                                     [\"column\", \"ks_statistic\", \"p_value\", \n",
    "                                      \"mean1\", \"mean2\", \"std1\", \"std2\", \n",
    "                                      \"kurtosis1\", \"kurtosis2\", \"skewness1\", \"skewness2\"])\n",
    "    \n",
    "    return drift_df\n",
    "\n",
    "def generate_summary_report(df: DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary report of the DataFrame in Markdown format.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    str: Markdown-formatted summary report\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    num_columns = len(df.columns)\n",
    "    \n",
    "    numeric_columns = [c for c, dtype in df.dtypes if dtype in ('int', 'double', 'float')]\n",
    "    categorical_columns = [c for c, dtype in df.dtypes if dtype not in ('int', 'double', 'float')]\n",
    "    \n",
    "    summary_stats = df.summary()\n",
    "    \n",
    "    report = f\"# DataFrame Summary Report\\n\\n\"\n",
    "    report += f\"## Basic Information\\n\"\n",
    "    report += f\"- Number of rows: {num_rows}\\n\"\n",
    "    report += f\"- Number of columns: {num_columns}\\n\"\n",
    "    report += f\"- Numeric columns: {', '.join(numeric_columns)}\\n\"\n",
    "    report += f\"- Categorical columns: {', '.join(categorical_columns)}\\n\\n\"\n",
    "    \n",
    "    report += f\"## Numeric Column Statistics\\n\"\n",
    "    for col in numeric_columns:\n",
    "        stats = summary_stats.filter(F.col(\"summary\").isin(\"min\", \"max\", \"mean\", \"stddev\")).select(col).collect()\n",
    "        report += f\"### {col}\\n\"\n",
    "        report += f\"- Min: {stats[0][0]}\\n\"\n",
    "        report += f\"- Max: {stats[1][0]}\\n\"\n",
    "        report += f\"- Mean: {stats[2][0]}\\n\"\n",
    "        report += f\"- StdDev: {stats[3][0]}\\n\\n\"\n",
    "    \n",
    "    report += f\"## Categorical Column Statistics\\n\"\n",
    "    for col in categorical_columns:\n",
    "        top_values = df.groupBy(col).count().orderBy(F.desc(\"count\")).limit(5)\n",
    "        report += f\"### {col}\\n\"\n",
    "        report += f\"Top 5 values:\\n\"\n",
    "        for row in top_values.collect():\n",
    "            report += f\"- {row[0]}: {row[1]}\\n\"\n",
    "        report += \"\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, lit, count, abs, concat_ws\n",
    "from pyspark.sql.types import StructType, DataType\n",
    "\n",
    "\n",
    "def identify_schema_differences(df1: DataFrame, df2: DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Identify differences in schema between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): First DataFrame.\n",
    "        df2 (DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing schema differences.\n",
    "    \"\"\"\n",
    "\n",
    "    def schema_to_dict(schema: StructType) -> Dict[str, str]:\n",
    "        return {field.name: field.dataType for field in schema.fields}\n",
    "\n",
    "    schema1, schema2 = schema_to_dict(df1.schema), schema_to_dict(df2.schema)\n",
    "\n",
    "    return {\n",
    "        \"columns_only_in_df1\": list(set(schema1.keys()) - set(schema2.keys())),\n",
    "        \"columns_only_in_df2\": list(set(schema2.keys()) - set(schema1.keys())),\n",
    "        \"type_mismatches\": [\n",
    "            (col, schema1[col].simpleString(), schema2[col].simpleString())\n",
    "            for col in set(schema1.keys()) & set(schema2.keys())\n",
    "            if schema1[col] != schema2[col]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_row_counts(df1: DataFrame, df2: DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Compare the number of rows in two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): First DataFrame.\n",
    "        df2 (DataFrame): Second DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: A dictionary containing row count information.\n",
    "    \"\"\"\n",
    "    count1, count2 = df1.count(), df2.count()\n",
    "    return {\n",
    "        \"rows_in_df1\": count1,\n",
    "        \"rows_in_df2\": count2,\n",
    "        \"row_difference\": count2 - count1,\n",
    "    }\n",
    "\n",
    "\n",
    "def find_key_column_differences(\n",
    "    df1: DataFrame, df2: DataFrame, key_columns: List[str]\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Find differences in key columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): First DataFrame.\n",
    "        df2 (DataFrame): Second DataFrame.\n",
    "        key_columns (List[str]): Columns that uniquely identify rows.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: A dictionary containing key column difference information.\n",
    "    \"\"\"\n",
    "    df1_keys = df1.select(key_columns).distinct()\n",
    "    df2_keys = df2.select(key_columns).distinct()\n",
    "\n",
    "    return {\n",
    "        \"keys_only_in_df1\": df1_keys.subtract(df2_keys).count(),\n",
    "        \"keys_only_in_df2\": df2_keys.subtract(df1_keys).count(),\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_timestamp_differences(\n",
    "    df1: DataFrame, df2: DataFrame, key_columns: List[str], timestamp_column: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze differences in timestamp column between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): First DataFrame.\n",
    "        df2 (DataFrame): Second DataFrame.\n",
    "        key_columns (List[str]): Columns that uniquely identify rows.\n",
    "        timestamp_column (str): Name of the timestamp column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing timestamp difference analysis.\n",
    "    \"\"\"\n",
    "    join_condition = [df1[col] == df2[col] for col in key_columns]\n",
    "\n",
    "    return (\n",
    "        df1.alias(\"df1\")\n",
    "        .join(df2.alias(\"df2\"), join_condition, \"full_outer\")\n",
    "        .select(\n",
    "            *[col(f\"df1.{c}\").alias(f\"{c}_df1\") for c in key_columns],\n",
    "            col(f\"df1.{timestamp_column}\").alias(f\"{timestamp_column}_df1\"),\n",
    "            col(f\"df2.{timestamp_column}\").alias(f\"{timestamp_column}_df2\"),\n",
    "            when(\n",
    "                col(f\"df1.{timestamp_column}\") > col(f\"df2.{timestamp_column}\"),\n",
    "                \"df1 more recent\",\n",
    "            )\n",
    "            .when(\n",
    "                col(f\"df1.{timestamp_column}\") < col(f\"df2.{timestamp_column}\"),\n",
    "                \"df2 more recent\",\n",
    "            )\n",
    "            .otherwise(\"Same timestamp\")\n",
    "            .alias(\"timestamp_comparison\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def compare_column_values(\n",
    "    df1: DataFrame,\n",
    "    df2: DataFrame,\n",
    "    key_columns: List[str],\n",
    "    exclude_columns: List[str] = None,\n",
    "    numeric_tolerance: float = 1e-6,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Compare values in corresponding columns between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): First DataFrame.\n",
    "        df2 (DataFrame): Second DataFrame.\n",
    "        key_columns (List[str]): Columns that uniquely identify rows.\n",
    "        exclude_columns (List[str], optional): Columns to exclude from comparison.\n",
    "        numeric_tolerance (float, optional): Tolerance for numeric comparisons.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing value differences.\n",
    "    \"\"\"\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    join_condition = [df1[col] == df2[col] for col in key_columns]\n",
    "    comparison_columns = [\n",
    "        col for col in df1.columns if col not in key_columns + exclude_columns\n",
    "    ]\n",
    "\n",
    "    diff_df = df1.alias(\"df1\").join(df2.alias(\"df2\"), join_condition, \"full_outer\")\n",
    "\n",
    "    for col_name in comparison_columns:\n",
    "        data_type = df1.schema[col_name].dataType\n",
    "\n",
    "        if isinstance(\n",
    "            data_type, (IntegerType, LongType, FloatType, DoubleType, DecimalType)\n",
    "        ):\n",
    "            diff_df = diff_df.withColumn(\n",
    "                f\"{col_name}_diff\",\n",
    "                when(\n",
    "                    abs(col(f\"df1.{col_name}\") - col(f\"df2.{col_name}\"))\n",
    "                    > numeric_tolerance,\n",
    "                    concat_ws(\" -> \", col(f\"df1.{col_name}\"), col(f\"df2.{col_name}\")),\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            diff_df = diff_df.withColumn(\n",
    "                f\"{col_name}_diff\",\n",
    "                when(\n",
    "                    col(f\"df1.{col_name}\") != col(f\"df2.{col_name}\"),\n",
    "                    concat_ws(\" -> \", col(f\"df1.{col_name}\"), col(f\"df2.{col_name}\")),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    select_columns = key_columns + [f\"{col}_diff\" for col in comparison_columns]\n",
    "    return diff_df.select(*select_columns).where(\n",
    "        \" OR \".join([f\"`{col}_diff` IS NOT NULL\" for col in comparison_columns])\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_dataframe_differences(\n",
    "    df1: DataFrame,\n",
    "    df2: DataFrame,\n",
    "    key_columns: List[str],\n",
    "    timestamp_column: str = None,\n",
    "    exclude_columns: List[str] = None,\n",
    "    numeric_tolerance: float = 1e-6,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of differences between two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): First DataFrame.\n",
    "        df2 (DataFrame): Second DataFrame.\n",
    "        key_columns (List[str]): Columns that uniquely identify rows.\n",
    "        timestamp_column (str, optional): Name of the timestamp column.\n",
    "        exclude_columns (List[str], optional): Columns to exclude from comparison.\n",
    "        numeric_tolerance (float, optional): Tolerance for numeric comparisons.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing comprehensive difference analysis.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If key columns are not present in both DataFrames.\n",
    "    \"\"\"\n",
    "    if not set(key_columns).issubset(df1.columns) or not set(key_columns).issubset(\n",
    "        df2.columns\n",
    "    ):\n",
    "        raise ValueError(\"Key columns must be present in both DataFrames.\")\n",
    "\n",
    "    analysis = {\n",
    "        \"schema_differences\": identify_schema_differences(df1, df2),\n",
    "        \"row_count_comparison\": compare_row_counts(df1, df2),\n",
    "        \"key_column_differences\": find_key_column_differences(df1, df2, key_columns),\n",
    "        \"value_differences\": compare_column_values(\n",
    "            df1, df2, key_columns, exclude_columns, numeric_tolerance\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if timestamp_column:\n",
    "        if timestamp_column not in df1.columns or timestamp_column not in df2.columns:\n",
    "            raise ValueError(\n",
    "                f\"Timestamp column '{timestamp_column}' not found in one or both DataFrames.\"\n",
    "            )\n",
    "        analysis[\"timestamp_analysis\"] = analyze_timestamp_differences(\n",
    "            df1, df2, key_columns, timestamp_column\n",
    "        )\n",
    "\n",
    "    return analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
