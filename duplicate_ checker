
```python
from pyspark.sql import SparkSession
from pyspark.sql import Window
from pyspark.sql import functions as F

def find_duplicates(df, subset=None):
    """
    Find duplicate rows in a PySpark DataFrame.
    
    Args:
    df (pyspark.sql.DataFrame): The input DataFrame.
    subset (list, optional): List of columns to consider for identifying duplicates.
                             If None, all columns are considered. Default is None.
    
    Returns:
    pyspark.sql.DataFrame: A DataFrame containing only the duplicate rows.
    """
    # If subset is not provided, use all columns
    if subset is None:
        subset = df.columns
    
    # Define a window specification that partitions by the specified columns
    window_spec = Window.partitionBy(subset)
    
    # Add a count column using the window function
    df_with_count = df.withColumn("count", F.count("*").over(window_spec))
    
    # Filter the rows where the count is greater than 1
    duplicates = df_with_count.filter(F.col("count") > 1).drop("count")
    
    return duplicates


