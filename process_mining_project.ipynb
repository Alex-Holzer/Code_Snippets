{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def convert_to_timestamp(\n",
    "    df: DataFrame,\n",
    "    columns: Union[str, List[str]],\n",
    "    format: str = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert string column(s) to timestamp format in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        columns (Union[str, List[str]]): Column name(s) to convert.\n",
    "        format (str, optional): Timestamp format string. Defaults to \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with converted timestamp column(s).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input column(s) are not present in the DataFrame.\n",
    "\n",
    "    Examples:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"2023-07-27T13:18:12.039+0000\",), (\"2023-07-28T13:20:30.039+0000\",)]\n",
    "        >>> df = spark.createDataFrame(data, [\"ZEITPUNKT\"])\n",
    "        >>> result_df = convert_to_timestamp(df, \"ZEITPUNKT\")\n",
    "        >>> result_df.printSchema()\n",
    "        root\n",
    "         |-- ZEITPUNKT: timestamp (nullable = true)\n",
    "\n",
    "        >>> result_df.show(truncate=False)\n",
    "        +----------------------------+\n",
    "        |ZEITPUNKT                   |\n",
    "        +----------------------------+\n",
    "        |2023-07-27 13:18:12.039     |\n",
    "        |2023-07-28 13:20:30.039     |\n",
    "        +----------------------------+\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    # Validate input columns\n",
    "    invalid_columns = set(columns) - set(df.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns are not present in the DataFrame: {invalid_columns}\"\n",
    "        )\n",
    "\n",
    "    # Convert string columns to timestamp\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col, to_timestamp(df[col], format))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def standardize_process_mining_column_names(\n",
    "    df: DataFrame,\n",
    "    case_column: str,\n",
    "    activity_column: str,\n",
    "    timestamp_column: str,\n",
    "    standardized_case_name: str = \"_CASE_KEY\",\n",
    "    standardized_activity_name: str = \"ACTIVITY\",\n",
    "    standardized_timestamp_name: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize column names for process mining by renaming case, activity, and timestamp columns.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        case_column (str): Name of the column containing case IDs.\n",
    "        activity_column (str): Name of the column containing activities.\n",
    "        timestamp_column (str): Name of the column containing timestamps.\n",
    "        standardized_case_name (str, optional): Standardized name for the case column. Defaults to \"_CASE_KEY\".\n",
    "        standardized_activity_name (str, optional): Standardized name for the activity column. Defaults to \"ACTIVITY\".\n",
    "        standardized_timestamp_name (str, optional): Standardized name for the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with standardized column names for process mining.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns are not present in the input DataFrame.\n",
    "        ValueError: If any of the standardized names are already present in the DataFrame but don't match the columns to be renamed.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"A1\", \"Start\", \"2023-01-01\"), (\"A2\", \"End\", \"2023-01-02\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"ID\", \"Action\", \"Date\"])\n",
    "        >>> standardized_df = standardize_process_mining_column_names(df, \"ID\", \"Action\", \"Date\")\n",
    "        >>> standardized_df.show()\n",
    "        +-----------+--------+----------+\n",
    "        |_CASE_KEY  |ACTIVITY|EVENTTIME |\n",
    "        +-----------+--------+----------+\n",
    "        |A1         |Start   |2023-01-01|\n",
    "        |A2         |End     |2023-01-02|\n",
    "        +-----------+--------+----------+\n",
    "    \"\"\"\n",
    "    # Check if all specified columns are present in the DataFrame\n",
    "    required_columns = {case_column, activity_column, timestamp_column}\n",
    "    missing_columns = required_columns - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns are missing from the DataFrame: {missing_columns}\"\n",
    "        )\n",
    "\n",
    "    # Create a mapping of original column names to standardized names\n",
    "    column_mapping = {\n",
    "        case_column: standardized_case_name,\n",
    "        activity_column: standardized_activity_name,\n",
    "        timestamp_column: standardized_timestamp_name,\n",
    "    }\n",
    "\n",
    "    # Check if any of the standardized names already exist in the DataFrame\n",
    "    existing_standard_names = set(column_mapping.values()) & set(df.columns)\n",
    "    conflicting_names = existing_standard_names - set(column_mapping.keys())\n",
    "    if conflicting_names:\n",
    "        raise ValueError(\n",
    "            f\"The following standardized names already exist in the DataFrame and don't match the columns to be renamed: {conflicting_names}\"\n",
    "        )\n",
    "\n",
    "    # Rename the columns\n",
    "    for original, standardized in column_mapping.items():\n",
    "        df = df.withColumnRenamed(original, standardized)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import List, Callable, Dict, Any\n",
    "\n",
    "\n",
    "def process_mining_preprocessing_pipeline(\n",
    "    df: DataFrame,\n",
    "    pipeline_steps: List[Callable[[DataFrame, Dict[str, Any]], DataFrame]],\n",
    "    config: Dict[str, Any],\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a flexible series of preprocessing steps for process mining on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        pipeline_steps (List[Callable]): List of functions to apply to the DataFrame.\n",
    "        config (Dict[str, Any]): Configuration dictionary for the pipeline steps.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame ready for process mining.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the preprocessing or validation steps fail.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...         (\"A2\", \"End\", \"2023-01-02 11:00:00\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"ID\", \"Action\", \"Date\"])\n",
    "        >>> config = {\n",
    "        ...     \"case_column\": \"ID\",\n",
    "        ...     \"activity_column\": \"Action\",\n",
    "        ...     \"timestamp_column\": \"Date\",\n",
    "        ...     \"additional_string_columns\": []\n",
    "        ... }\n",
    "        >>> pipeline_steps = [\n",
    "        ...     trim_all_strings,\n",
    "        ...     convert_empty_string_to_null,\n",
    "        ...     standardize_process_mining_names,\n",
    "        ...     convert_to_timestamp,\n",
    "        ...     check_process_mining_conditions\n",
    "        ... ]\n",
    "        >>> processed_df = process_mining_preprocessing_pipeline(df, pipeline_steps, config)\n",
    "        >>> processed_df.show()\n",
    "    \"\"\"\n",
    "    for step in pipeline_steps:\n",
    "        df = df.transform(lambda df: step(df, config))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"A2\", \"End  \", \"2023-01-02 11:00:00\"),\n",
    "        (\"A3\", \"\", \"2023-01-03 12:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"ID\", \"Action\", \"Date\"])\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"case_column\": \"ID\",\n",
    "        \"activity_column\": \"Action\",\n",
    "        \"timestamp_column\": \"Date\",\n",
    "        \"additional_string_columns\": [],\n",
    "        \"show_examples\": True,  # for check_process_mining_conditions\n",
    "    }\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_steps = [\n",
    "        trim_all_strings,\n",
    "        convert_empty_string_to_null,\n",
    "        standardize_process_mining_names,\n",
    "        convert_to_timestamp,\n",
    "        check_process_mining_conditions,\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        processed_df = process_mining_preprocessing_pipeline(df, pipeline_steps, config)\n",
    "        print(\"Preprocessing successful!\")\n",
    "        processed_df.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Preprocessing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, when, year, expr, sum as spark_sum\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "def check_process_mining_conditions(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"TIMESTAMP\",\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Efficiently check if all necessary conditions for the process mining process are met.\n",
    "\n",
    "    This function performs all checks in a single pass over the data, minimizing Spark actions\n",
    "    and optimizing for large datasets.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"TIMESTAMP\".\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: A catchy success message if no errors are found, None otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the conditions are not met, with a summary of all errors found.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> from pyspark.sql.functions import to_timestamp\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...         (\"A1\", \"Process\", \"2023-01-01 11:00:00\"),\n",
    "        ...         (\"A1\", \"End\", \"2023-01-01 12:00:00\"),\n",
    "        ...         (\"A2\", \"Start\", \"2023-01-02 09:00:00\"),\n",
    "        ...         (\"A2\", \"End\", \"2023-01-02 10:00:00\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"CASE_KEY\", \"ACTIVITY\", \"TIMESTAMP\"])\n",
    "        >>> df = df.withColumn(\"TIMESTAMP\", to_timestamp(\"TIMESTAMP\"))\n",
    "        >>> result = check_process_mining_conditions(df)\n",
    "        >>> print(result)\n",
    "        ðŸŽ‰ Process mining data perfection achieved! Your data is ready to uncover insights.\n",
    "    \"\"\"\n",
    "    # Check column types\n",
    "    for column, expected_type in [\n",
    "        (case_column, StringType),\n",
    "        (activity_column, StringType),\n",
    "        (timestamp_column, TimestampType),\n",
    "    ]:\n",
    "        if not isinstance(df.schema[column].dataType, expected_type):\n",
    "            raise ValueError(\n",
    "                f\"ðŸ“Š Column '{column}' should be a {expected_type.__name__}, but it's a {df.schema[column].dataType}.\"\n",
    "            )\n",
    "\n",
    "    # Prepare all checks in a single DataFrame operation\n",
    "    checks_df = df.select(\n",
    "        when(col(case_column).isNull(), 1).otherwise(0).alias(\"null_cases\"),\n",
    "        when(col(activity_column).isNull(), 1).otherwise(0).alias(\"null_activities\"),\n",
    "        when(col(timestamp_column).isNull(), 1).otherwise(0).alias(\"null_timestamps\"),\n",
    "        when(year(col(timestamp_column)) < 1970, 1)\n",
    "        .otherwise(0)\n",
    "        .alias(\"early_timestamps\"),\n",
    "        when(year(col(timestamp_column)) > 2100, 1)\n",
    "        .otherwise(0)\n",
    "        .alias(\"future_timestamps\"),\n",
    "        expr(\n",
    "            f\"count(*) over (partition by {case_column}, {activity_column}, {timestamp_column})\"\n",
    "        ).alias(\"event_count\"),\n",
    "        expr(f\"count(*) over (partition by {case_column})\").alias(\"case_event_count\"),\n",
    "    )\n",
    "\n",
    "    # Collect all error counts in a single action\n",
    "    error_counts = checks_df.agg(\n",
    "        spark_sum(\"null_cases\").alias(\"null_cases\"),\n",
    "        spark_sum(\"null_activities\").alias(\"null_activities\"),\n",
    "        spark_sum(\"null_timestamps\").alias(\"null_timestamps\"),\n",
    "        spark_sum(\"early_timestamps\").alias(\"early_timestamps\"),\n",
    "        spark_sum(\"future_timestamps\").alias(\"future_timestamps\"),\n",
    "        spark_sum(when(col(\"event_count\") > 1, 1).otherwise(0)).alias(\n",
    "            \"duplicate_events\"\n",
    "        ),\n",
    "        spark_sum(when(col(\"case_event_count\") == 0, 1).otherwise(0)).alias(\n",
    "            \"cases_without_activities\"\n",
    "        ),\n",
    "    ).collect()[0]\n",
    "\n",
    "    # Convert to dictionary for easier handling\n",
    "    error_dict: Dict[str, int] = error_counts.asDict()\n",
    "\n",
    "    # Prepare error messages\n",
    "    error_messages = []\n",
    "    if error_dict[\"null_cases\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸš« Found {error_dict['null_cases']} null values in '{case_column}' column.\"\n",
    "        )\n",
    "    if error_dict[\"null_activities\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸš« Found {error_dict['null_activities']} null values in '{activity_column}' column.\"\n",
    "        )\n",
    "    if error_dict[\"null_timestamps\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸš« Found {error_dict['null_timestamps']} null values in '{timestamp_column}' column.\"\n",
    "        )\n",
    "    if error_dict[\"early_timestamps\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"â³ Found {error_dict['early_timestamps']} timestamps before 1970.\"\n",
    "        )\n",
    "    if error_dict[\"future_timestamps\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸ”® Found {error_dict['future_timestamps']} timestamps after 2100.\"\n",
    "        )\n",
    "    if error_dict[\"duplicate_events\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸ‘¯ Found {error_dict['duplicate_events']} duplicate events. Each combination of {case_column}, {activity_column}, and {timestamp_column} should be unique!\"\n",
    "        )\n",
    "    if error_dict[\"cases_without_activities\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸ” Found {error_dict['cases_without_activities']} cases with no activities.\"\n",
    "        )\n",
    "\n",
    "    if error_messages:\n",
    "        raise ValueError(\"\\n\".join(error_messages))\n",
    "\n",
    "    return \"ðŸŽ‰ Process mining data perfection achieved! Your data is ready to uncover insights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "\n",
    "\n",
    "def find_duplicate_case_activity_eventtime(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    eventtime_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies and returns all duplicate case-activity-eventtime combinations in a process mining DataFrame.\n",
    "\n",
    "    Duplicate combinations are defined as rows with the same values for _CASE_KEY, ACTIVITY, and EVENTTIME.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame containing process mining data.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        eventtime_column (str, optional): Name of the eventtime column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing all rows that are part of duplicate combinations,\n",
    "                   along with a count of how many times each combination appears.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified columns are not present in the DataFrame or are of incorrect type.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> from pyspark.sql.functions import to_timestamp\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"A1\", \"Start\", \"2023-01-01 10:00:00\"),  # Duplicate\n",
    "        ...     (\"A1\", \"Process\", \"2023-01-01 11:00:00\"),\n",
    "        ...     (\"A2\", \"Start\", \"2023-01-02 09:00:00\"),\n",
    "        ...     (\"A2\", \"Start\", \"2023-01-02 09:00:00\"),  # Duplicate\n",
    "        ...     (\"A2\", \"End\", \"2023-01-02 10:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", to_timestamp(\"EVENTTIME\"))\n",
    "        >>> duplicates = find_duplicate_case_activity_eventtime(df)\n",
    "        >>> duplicates.show()\n",
    "        +---------+--------+-------------------+-----+\n",
    "        |_CASE_KEY|ACTIVITY|          EVENTTIME|count|\n",
    "        +---------+--------+-------------------+-----+\n",
    "        |       A1|   Start|2023-01-01 10:00:00|    2|\n",
    "        |       A2|   Start|2023-01-02 09:00:00|    2|\n",
    "        +---------+--------+-------------------+-----+\n",
    "    \"\"\"\n",
    "    # Validate column presence and types\n",
    "    for column, expected_type in [\n",
    "        (case_column, StringType),\n",
    "        (activity_column, StringType),\n",
    "        (eventtime_column, TimestampType),\n",
    "    ]:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
    "        if not isinstance(df.schema[column].dataType, expected_type):\n",
    "            raise ValueError(\n",
    "                f\"Column '{column}' should be of type {expected_type.__name__}, but is {df.schema[column].dataType}\"\n",
    "            )\n",
    "\n",
    "    # Identify duplicates\n",
    "    window_spec = Window.partitionBy(case_column, activity_column, eventtime_column)\n",
    "\n",
    "    duplicates_df = (\n",
    "        df.withColumn(\"count\", count(\"*\").over(window_spec))\n",
    "        .filter(col(\"count\") > 1)\n",
    "        .select(case_column, activity_column, eventtime_column, \"count\")\n",
    "        .distinct()\n",
    "        .orderBy(case_column, activity_column, eventtime_column)\n",
    "    )\n",
    "\n",
    "    return duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, lit, round\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def calculate_duration_from_start_to_target(\n",
    "    df: DataFrame,\n",
    "    target_activities: Union[str, List[str]],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the time difference between the start activity and the first occurrence of any specified target activity for each case.\n",
    "    Only returns cases where a target activity is found.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        target_activities (Union[str, List[str]]): The target activity or list of target activities to calculate duration to.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the following columns:\n",
    "            - case_column: The case identifier\n",
    "            - start_activity: The name of the start activity\n",
    "            - start_timestamp: The timestamp of the start activity\n",
    "            - target_activity: The name of the first occurring target activity\n",
    "            - target_timestamp: The timestamp of the first occurring target activity\n",
    "            - duration_minutes: Duration in minutes (rounded to 2 decimal places)\n",
    "            - duration_hours: Duration in hours (rounded to 2 decimal places)\n",
    "            - duration_days: Duration in days (rounded to 2 decimal places)\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        ...     (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        ...     (\"case2\", \"Start\", \"2023-01-01 09:00:00\"),\n",
    "        ...     (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        ...     (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        ...     (\"case3\", \"Other\", \"2023-01-02 08:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "        >>> result = calculate_duration_from_start_to_target(df, [\"Middle\", \"End\"])\n",
    "        >>> result.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Convert target_activities to a list if it's a single string\n",
    "    if isinstance(target_activities, str):\n",
    "        target_activities = [target_activities]\n",
    "\n",
    "    # Window specification for operations within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Find the start activity and its timestamp for each case\n",
    "    start_activity_df = (\n",
    "        df.withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"start_activity\"),\n",
    "            col(timestamp_column).alias(\"start_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Find the first occurrence of any target activity and its timestamp for each case\n",
    "    target_activity_df = (\n",
    "        df.filter(col(activity_column).isin(target_activities))\n",
    "        .withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"target_activity\"),\n",
    "            col(timestamp_column).alias(\"target_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join the start activity and target activity dataframes\n",
    "    result_df = start_activity_df.join(target_activity_df, case_column, \"inner\")\n",
    "\n",
    "    # Calculate duration in seconds, then convert to minutes, hours, and days\n",
    "    result_df = result_df.withColumn(\n",
    "        \"duration_seconds\",\n",
    "        F.unix_timestamp(\"target_timestamp\") - F.unix_timestamp(\"start_timestamp\"),\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        result_df.withColumn(\"duration_minutes\", round(col(\"duration_seconds\") / 60, 2))\n",
    "        .withColumn(\"duration_hours\", round(col(\"duration_seconds\") / 3600, 2))\n",
    "        .withColumn(\"duration_days\", round(col(\"duration_seconds\") / 86400, 2))\n",
    "        .drop(\"duration_seconds\")\n",
    "    )\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = [\n",
    "        case_column,\n",
    "        \"start_activity\",\n",
    "        \"start_timestamp\",\n",
    "        \"target_activity\",\n",
    "        \"target_timestamp\",\n",
    "        \"duration_minutes\",\n",
    "        \"duration_hours\",\n",
    "        \"duration_days\",\n",
    "    ]\n",
    "\n",
    "    return result_df.select(final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, round\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def calculate_duration_between_activities(\n",
    "    df: DataFrame,\n",
    "    start_activities: Union[str, List[str]],\n",
    "    target_activities: Union[str, List[str]],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the time difference between the first occurrence of any specified start activity\n",
    "    and the first occurrence of any specified target activity for each case.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        start_activities (Union[str, List[str]]): The start activity or list of start activities.\n",
    "        target_activities (Union[str, List[str]]): The target activity or list of target activities.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the following columns:\n",
    "            - case_column: The case identifier\n",
    "            - start_activity: The name of the first occurring start activity\n",
    "            - start_timestamp: The timestamp of the first occurring start activity\n",
    "            - target_activity: The name of the first occurring target activity\n",
    "            - target_timestamp: The timestamp of the first occurring target activity\n",
    "            - duration_minutes: Duration in minutes (rounded to 2 decimal places)\n",
    "            - duration_hours: Duration in hours (rounded to 2 decimal places)\n",
    "            - duration_days: Duration in days (rounded to 2 decimal places)\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        ...     (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        ...     (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        ...     (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        ...     (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        ...     (\"case3\", \"Other\", \"2023-01-02 08:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "        >>> result = calculate_duration_between_activities(df, [\"Start\", \"Begin\"], [\"Middle\", \"End\"])\n",
    "        >>> result.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Convert start_activities and target_activities to lists if they're single strings\n",
    "    if isinstance(start_activities, str):\n",
    "        start_activities = [start_activities]\n",
    "    if isinstance(target_activities, str):\n",
    "        target_activities = [target_activities]\n",
    "\n",
    "    # Window specification for operations within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Find the first occurrence of any start activity and its timestamp for each case\n",
    "    start_activity_df = (\n",
    "        df.filter(col(activity_column).isin(start_activities))\n",
    "        .withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"start_activity\"),\n",
    "            col(timestamp_column).alias(\"start_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Find the first occurrence of any target activity and its timestamp for each case\n",
    "    target_activity_df = (\n",
    "        df.filter(col(activity_column).isin(target_activities))\n",
    "        .withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"target_activity\"),\n",
    "            col(timestamp_column).alias(\"target_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join the start activity and target activity dataframes\n",
    "    result_df = start_activity_df.join(target_activity_df, case_column, \"inner\")\n",
    "\n",
    "    # Calculate duration in seconds, then convert to minutes, hours, and days\n",
    "    result_df = result_df.withColumn(\n",
    "        \"duration_seconds\",\n",
    "        F.unix_timestamp(\"target_timestamp\") - F.unix_timestamp(\"start_timestamp\"),\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        result_df.withColumn(\"duration_minutes\", round(col(\"duration_seconds\") / 60, 2))\n",
    "        .withColumn(\"duration_hours\", round(col(\"duration_seconds\") / 3600, 2))\n",
    "        .withColumn(\"duration_days\", round(col(\"duration_seconds\") / 86400, 2))\n",
    "        .drop(\"duration_seconds\")\n",
    "    )\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = [\n",
    "        case_column,\n",
    "        \"start_activity\",\n",
    "        \"start_timestamp\",\n",
    "        \"target_activity\",\n",
    "        \"target_timestamp\",\n",
    "        \"duration_minutes\",\n",
    "        \"duration_hours\",\n",
    "        \"duration_days\",\n",
    "    ]\n",
    "\n",
    "    return result_df.select(final_columns)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Other\", \"2023-01-02 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    # Calculate duration from the first occurrence of either \"Start\" or \"Begin\" to the first occurrence of either \"Middle\" or \"End\"\n",
    "    result = calculate_duration_between_activities(\n",
    "        df, [\"Start\", \"Begin\"], [\"Middle\", \"End\"]\n",
    "    )\n",
    "    result.show(truncate=False)\n",
    "\n",
    "    # Calculate duration from \"Start\" to \"End\" only\n",
    "    result_start_to_end = calculate_duration_between_activities(df, \"Start\", \"End\")\n",
    "    result_start_to_end.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, round, min as spark_min, max as spark_max\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def calculate_throughput_time(\n",
    "    df: DataFrame,\n",
    "    start_activities: Union[str, List[str]],\n",
    "    end_activities: Union[str, List[str]],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the throughput time between specified sets of start and end activities for each case.\n",
    "    The throughput time includes all intermediate activities between the first occurrence of any start activity\n",
    "    and the last occurrence of any end activity.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        start_activities (Union[str, List[str]]): The start activity or list of start activities.\n",
    "        end_activities (Union[str, List[str]]): The end activity or list of end activities.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the following columns:\n",
    "            - case_column: The case identifier\n",
    "            - first_start_activity: The name of the first occurring start activity\n",
    "            - first_start_timestamp: The timestamp of the first occurring start activity\n",
    "            - last_end_activity: The name of the last occurring end activity\n",
    "            - last_end_timestamp: The timestamp of the last occurring end activity\n",
    "            - throughput_time_minutes: Throughput time in minutes (rounded to 2 decimal places)\n",
    "            - throughput_time_hours: Throughput time in hours (rounded to 2 decimal places)\n",
    "            - throughput_time_days: Throughput time in days (rounded to 2 decimal places)\n",
    "            - activity_count: The number of activities between start and end (inclusive)\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"case1\", \"Middle1\", \"2023-01-02 11:00:00\"),\n",
    "        ...     (\"case1\", \"Middle2\", \"2023-01-03 09:00:00\"),\n",
    "        ...     (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        ...     (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        ...     (\"case2\", \"Process\", \"2023-01-01 14:00:00\"),\n",
    "        ...     (\"case2\", \"Finish\", \"2023-01-01 17:00:00\"),\n",
    "        ...     (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        ...     (\"case3\", \"Other\", \"2023-01-02 08:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "        >>> result = calculate_throughput_time(df, [\"Start\", \"Begin\"], [\"End\", \"Finish\"])\n",
    "        >>> result.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Convert start_activities and end_activities to lists if they're single strings\n",
    "    if isinstance(start_activities, str):\n",
    "        start_activities = [start_activities]\n",
    "    if isinstance(end_activities, str):\n",
    "        end_activities = [end_activities]\n",
    "\n",
    "    # Window specification for operations within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Find the first occurrence of any start activity and its timestamp for each case\n",
    "    start_df = df.filter(col(activity_column).isin(start_activities))\n",
    "    first_start = (\n",
    "        start_df.withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            case_column,\n",
    "            col(timestamp_column).alias(\"first_start_timestamp\"),\n",
    "            col(activity_column).alias(\"first_start_activity\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Find the last occurrence of any end activity and its timestamp for each case\n",
    "    end_df = df.filter(col(activity_column).isin(end_activities))\n",
    "    last_end = (\n",
    "        end_df.withColumn(\n",
    "            \"row_number\",\n",
    "            F.row_number().over(case_window.orderBy(F.desc(timestamp_column))),\n",
    "        )\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            case_column,\n",
    "            col(timestamp_column).alias(\"last_end_timestamp\"),\n",
    "            col(activity_column).alias(\"last_end_activity\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join the start and end dataframes\n",
    "    result_df = first_start.join(last_end, case_column, \"inner\")\n",
    "\n",
    "    # Calculate throughput time in seconds, then convert to minutes, hours, and days\n",
    "    result_df = result_df.withColumn(\n",
    "        \"throughput_time_seconds\",\n",
    "        F.unix_timestamp(\"last_end_timestamp\")\n",
    "        - F.unix_timestamp(\"first_start_timestamp\"),\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        result_df.withColumn(\n",
    "            \"throughput_time_minutes\", round(col(\"throughput_time_seconds\") / 60, 2)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"throughput_time_hours\", round(col(\"throughput_time_seconds\") / 3600, 2)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"throughput_time_days\", round(col(\"throughput_time_seconds\") / 86400, 2)\n",
    "        )\n",
    "        .drop(\"throughput_time_seconds\")\n",
    "    )\n",
    "\n",
    "    # Calculate the number of activities between start and end (inclusive)\n",
    "    activity_count = (\n",
    "        df.join(result_df, case_column)\n",
    "        .filter(\n",
    "            (col(timestamp_column) >= col(\"first_start_timestamp\"))\n",
    "            & (col(timestamp_column) <= col(\"last_end_timestamp\"))\n",
    "        )\n",
    "        .groupBy(case_column)\n",
    "        .agg(F.count(\"*\").alias(\"activity_count\"))\n",
    "    )\n",
    "\n",
    "    # Join the activity count to the result dataframe\n",
    "    result_df = result_df.join(activity_count, case_column, \"left_outer\")\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = [\n",
    "        case_column,\n",
    "        \"first_start_activity\",\n",
    "        \"first_start_timestamp\",\n",
    "        \"last_end_activity\",\n",
    "        \"last_end_timestamp\",\n",
    "        \"throughput_time_minutes\",\n",
    "        \"throughput_time_hours\",\n",
    "        \"throughput_time_days\",\n",
    "        \"activity_count\",\n",
    "    ]\n",
    "\n",
    "    return result_df.select(final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lower,\n",
    "    first,\n",
    "    last,\n",
    "    collect_set,\n",
    "    array_contains,\n",
    "    lit,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "\n",
    "def filter_process_cases(\n",
    "    df: DataFrame,\n",
    "    case_flows_through: Optional[Union[str, List[str]]] = None,\n",
    "    case_does_not_flow_through: Optional[Union[str, List[str]]] = None,\n",
    "    case_starts_with: Optional[Union[str, List[str]]] = None,\n",
    "    case_ends_with: Optional[Union[str, List[str]]] = None,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Highly optimized function to filter process cases based on specified flow conditions for large datasets (case-insensitive).\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        case_flows_through (Optional[Union[str, List[str]]]): Activity or list of activities that cases must flow through.\n",
    "        case_does_not_flow_through (Optional[Union[str, List[str]]]): Activity or list of activities that cases must not flow through.\n",
    "        case_starts_with (Optional[Union[str, List[str]]]): Activity or list of activities that cases must start with.\n",
    "        case_ends_with (Optional[Union[str, List[str]]]): Activity or list of activities that cases must end with.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet all specified conditions.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_process_cases(\n",
    "        ...     df,\n",
    "        ...     case_flows_through=[\"Middle\", \"Review\"],\n",
    "        ...     case_does_not_flow_through=\"Reject\",\n",
    "        ...     case_starts_with=\"Start\",\n",
    "        ...     case_ends_with=[\"End\", \"Complete\"]\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to convert input to lowercase list\n",
    "    def to_lower_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        return [s.lower() for s in (x if isinstance(x, list) else [x])]\n",
    "\n",
    "    # Convert inputs to lowercase lists\n",
    "    flows_through = to_lower_list(case_flows_through)\n",
    "    not_flows_through = to_lower_list(case_does_not_flow_through)\n",
    "    starts_with = to_lower_list(case_starts_with)\n",
    "    ends_with = to_lower_list(case_ends_with)\n",
    "\n",
    "    # If no filtering conditions are provided, return the original DataFrame\n",
    "    if not any([flows_through, not_flows_through, starts_with, ends_with]):\n",
    "        return df\n",
    "\n",
    "    # Create a window spec for each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Precompute case summary\n",
    "    case_summary = df.groupBy(case_column).agg(\n",
    "        collect_set(lower(col(activity_column))).alias(\"activities\"),\n",
    "        first(lower(col(activity_column))).alias(\"first_activity\"),\n",
    "        last(lower(col(activity_column))).alias(\"last_activity\"),\n",
    "    )\n",
    "\n",
    "    # Apply filters\n",
    "    if flows_through:\n",
    "        case_summary = case_summary.filter(\n",
    "            all(\n",
    "                array_contains(col(\"activities\"), lit(activity))\n",
    "                for activity in flows_through\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if not_flows_through:\n",
    "        case_summary = case_summary.filter(\n",
    "            ~array_contains(col(\"activities\"), lit(activity))\n",
    "            for activity in not_flows_through\n",
    "        )\n",
    "\n",
    "    if starts_with:\n",
    "        case_summary = case_summary.filter(col(\"first_activity\").isin(starts_with))\n",
    "\n",
    "    if ends_with:\n",
    "        case_summary = case_summary.filter(col(\"last_activity\").isin(ends_with))\n",
    "\n",
    "    # Get the list of cases that meet all conditions\n",
    "    valid_cases = case_summary.select(case_column)\n",
    "\n",
    "    # Filter the original DataFrame\n",
    "    return df.join(valid_cases, case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data (you would replace this with your large dataset)\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"start\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Other\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case2\", \"END\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Begin\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"middle\", \"2023-01-02 08:00:00\"),\n",
    "        (\"case3\", \"Finish\", \"2023-01-03 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    filtered_df = filter_process_cases(\n",
    "        df,\n",
    "        case_flows_through=\"Middle\",\n",
    "        case_does_not_flow_through=\"Other\",\n",
    "        case_starts_with=[\"Start\", \"Begin\"],\n",
    "        case_ends_with=[\"End\", \"Finish\"],\n",
    "    )\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nFiltered DataFrame:\")\n",
    "    filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lower, lead, when, lit, collect_list\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def filter_process_flow(\n",
    "    df: DataFrame,\n",
    "    first_activity: str,\n",
    "    second_activity: str,\n",
    "    relationship: Literal[\n",
    "        \"directly_followed\",\n",
    "        \"followed_anytime_by\",\n",
    "        \"not_directly_followed\",\n",
    "        \"never_followed_by\",\n",
    "    ],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter the process flow based on the relationship between two specified activities.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        first_activity (str): The first activity in the relationship.\n",
    "        second_activity (str): The second activity in the relationship.\n",
    "        relationship (Literal[\"directly_followed\", \"followed_anytime_by\", \"not_directly_followed\", \"never_followed_by\"]):\n",
    "            The type of relationship between the two activities.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the specified relationship.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid relationship is provided.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_process_flow(\n",
    "        ...     df,\n",
    "        ...     first_activity=\"Start\",\n",
    "        ...     second_activity=\"Review\",\n",
    "        ...     relationship=\"directly_followed\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate the relationship parameter\n",
    "    valid_relationships = [\n",
    "        \"directly_followed\",\n",
    "        \"followed_anytime_by\",\n",
    "        \"not_directly_followed\",\n",
    "        \"never_followed_by\",\n",
    "    ]\n",
    "    if relationship not in valid_relationships:\n",
    "        raise ValueError(\n",
    "            f\"Invalid relationship. Must be one of: {', '.join(valid_relationships)}\"\n",
    "        )\n",
    "\n",
    "    # Create a window spec for each case, ordered by timestamp\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Convert activities to lowercase for case-insensitive matching\n",
    "    df = df.withColumn(activity_column, lower(col(activity_column)))\n",
    "    first_activity = first_activity.lower()\n",
    "    second_activity = second_activity.lower()\n",
    "\n",
    "    if relationship == \"directly_followed\":\n",
    "        # Add a column with the next activity\n",
    "        df_with_next = df.withColumn(\n",
    "            \"next_activity\", lead(activity_column).over(case_window)\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity is directly followed by second_activity\n",
    "        filtered_cases = (\n",
    "            df_with_next.filter(\n",
    "                (col(activity_column) == first_activity)\n",
    "                & (col(\"next_activity\") == second_activity)\n",
    "            )\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "    elif relationship == \"followed_anytime_by\":\n",
    "        # Collect all activities for each case\n",
    "        case_activities = df.groupBy(case_column).agg(\n",
    "            collect_list(activity_column).alias(\"activities\")\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity appears before second_activity\n",
    "        filtered_cases = case_activities.filter(\n",
    "            (array_contains(col(\"activities\"), first_activity))\n",
    "            & (array_contains(col(\"activities\"), second_activity))\n",
    "            & (\n",
    "                col(\"activities\").getItem(\n",
    "                    array_position(col(\"activities\"), first_activity)\n",
    "                )\n",
    "                < col(\"activities\").getItem(\n",
    "                    array_position(col(\"activities\"), second_activity)\n",
    "                )\n",
    "            )\n",
    "        ).select(case_column)\n",
    "\n",
    "    elif relationship == \"not_directly_followed\":\n",
    "        # Add a column with the next activity\n",
    "        df_with_next = df.withColumn(\n",
    "            \"next_activity\", lead(activity_column).over(case_window)\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity is not directly followed by second_activity\n",
    "        filtered_cases = (\n",
    "            df_with_next.filter(\n",
    "                (col(activity_column) == first_activity)\n",
    "                & (col(\"next_activity\") != second_activity)\n",
    "            )\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "    else:  # never_followed_by\n",
    "        # Collect all activities for each case\n",
    "        case_activities = df.groupBy(case_column).agg(\n",
    "            collect_list(activity_column).alias(\"activities\")\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity appears but is never followed by second_activity\n",
    "        filtered_cases = case_activities.filter(\n",
    "            (array_contains(col(\"activities\"), first_activity))\n",
    "            & (\n",
    "                ~array_contains(col(\"activities\"), second_activity)\n",
    "                | (\n",
    "                    col(\"activities\").getItem(\n",
    "                        array_position(col(\"activities\"), first_activity)\n",
    "                    )\n",
    "                    > col(\"activities\").getItem(\n",
    "                        array_position(col(\"activities\"), second_activity)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ).select(case_column)\n",
    "\n",
    "    # Join the filtered cases back to the original DataFrame\n",
    "    return df.join(filtered_cases, case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Review\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-03 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    relationships = [\n",
    "        \"directly_followed\",\n",
    "        \"followed_anytime_by\",\n",
    "        \"not_directly_followed\",\n",
    "        \"never_followed_by\",\n",
    "    ]\n",
    "\n",
    "    for rel in relationships:\n",
    "        filtered_df = filter_process_flow(df, \"Start\", \"Review\", rel)\n",
    "        print(f\"\\nFiltered DataFrame ({rel}):\")\n",
    "        filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lower, count, when\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def filter_rework_cases(\n",
    "    df: DataFrame,\n",
    "    activity: str,\n",
    "    occurs_between: Optional[Tuple[int, int]] = None,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter cases based on the number of occurrences of a specific activity within a given range.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        activity (str): The activity to check for rework.\n",
    "        occurs_between (Optional[Tuple[int, int]]): The range of occurrences to filter for.\n",
    "                                                    Defaults to None, which is treated as (0, float('inf')).\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the specified rework criteria.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_rework_cases(\n",
    "        ...     df,\n",
    "        ...     activity=\"Review\",\n",
    "        ...     occurs_between=(2, 4)\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Set default range if not provided\n",
    "    if occurs_between is None:\n",
    "        occurs_between = (0, float(\"inf\"))\n",
    "\n",
    "    min_occurrences, max_occurrences = occurs_between\n",
    "\n",
    "    # Validate input\n",
    "    if min_occurrences < 0 or max_occurrences < min_occurrences:\n",
    "        raise ValueError(\"Invalid occurrence range. Ensure min >= 0 and max >= min.\")\n",
    "\n",
    "    # Convert activity to lowercase for case-insensitive matching\n",
    "    activity = activity.lower()\n",
    "\n",
    "    # Count occurrences of the activity for each case\n",
    "    activity_counts = df.groupBy(case_column).agg(\n",
    "        count(when(lower(col(activity_column)) == activity, True)).alias(\n",
    "            \"activity_count\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filter based on the occurrence range\n",
    "    if max_occurrences == float(\"inf\"):\n",
    "        filtered_cases = activity_counts.filter(\n",
    "            col(\"activity_count\") >= min_occurrences\n",
    "        )\n",
    "    else:\n",
    "        filtered_cases = activity_counts.filter(\n",
    "            (col(\"activity_count\") >= min_occurrences)\n",
    "            & (col(\"activity_count\") <= max_occurrences)\n",
    "        )\n",
    "\n",
    "    # Handle the special case of (0,1)\n",
    "    if min_occurrences == 0 and max_occurrences == 1:\n",
    "        filtered_cases = activity_counts.filter(col(\"activity_count\") <= 1)\n",
    "\n",
    "    # Join the filtered cases back to the original DataFrame\n",
    "    return df.join(filtered_cases.select(case_column), case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Review\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"Review\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Review\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-02 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-03 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-04 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Test different occurrence ranges\n",
    "    ranges = [(None), (0, 1), (1, 2), (2, float(\"inf\"))]\n",
    "\n",
    "    for range_ in ranges:\n",
    "        filtered_df = filter_rework_cases(df, \"Review\", range_)\n",
    "        range_str = f\"({range_[0]}, {range_[1]})\" if range_ else \"Default (0, inf)\"\n",
    "        print(f\"\\nFiltered DataFrame (Review occurs between {range_str}):\")\n",
    "        filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, first, last, when, lit, sum, isnull\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def filter_attribute_cases(\n",
    "    df: DataFrame,\n",
    "    attribute_column: str,\n",
    "    attribute_value: Union[str, int, float],\n",
    "    once_occurred: bool = False,\n",
    "    first_occurrence: bool = False,\n",
    "    last_occurrence: bool = False,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter cases based on the occurrence of a specific attribute value in the process,\n",
    "    ignoring null values when considering first or last occurrences.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        attribute_column (str): Name of the column containing the attribute to filter on.\n",
    "        attribute_value (Union[str, int, float]): The value of the attribute to filter for.\n",
    "        once_occurred (bool): If True, select cases where the attribute occurred at least once. Defaults to False.\n",
    "        first_occurrence (bool): If True, select cases where the attribute occurred first (ignoring nulls). Defaults to False.\n",
    "        last_occurrence (bool): If True, select cases where the attribute occurred last (ignoring nulls). Defaults to False.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the specified attribute criteria.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no occurrence type is specified or if the attribute column doesn't exist.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_attribute_cases(\n",
    "        ...     df,\n",
    "        ...     attribute_column=\"PRIORITY\",\n",
    "        ...     attribute_value=\"High\",\n",
    "        ...     first_occurrence=True\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not any([once_occurred, first_occurrence, last_occurrence]):\n",
    "        raise ValueError(\n",
    "            \"At least one occurrence type (once_occurred, first_occurrence, or last_occurrence) must be True.\"\n",
    "        )\n",
    "\n",
    "    if attribute_column not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Column '{attribute_column}' does not exist in the DataFrame.\"\n",
    "        )\n",
    "\n",
    "    # Create a window spec for each case, ordered by timestamp\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Create a flag column for rows where the attribute matches the specified value\n",
    "    df_with_flag = df.withColumn(\n",
    "        \"attribute_flag\",\n",
    "        when(\n",
    "            (col(attribute_column) == attribute_value)\n",
    "            & (~isnull(col(attribute_column))),\n",
    "            1,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    # Perform filtering based on occurrence type\n",
    "    filtered_cases = None\n",
    "\n",
    "    if once_occurred:\n",
    "        # Select cases where the attribute occurred at least once\n",
    "        filtered_cases = (\n",
    "            df_with_flag.groupBy(case_column)\n",
    "            .agg((sum(\"attribute_flag\") > 0).alias(\"occurred\"))\n",
    "            .filter(col(\"occurred\"))\n",
    "            .select(case_column)\n",
    "        )\n",
    "\n",
    "    if first_occurrence:\n",
    "        # Find the first non-null occurrence of the attribute for each case\n",
    "        first_occurrences = (\n",
    "            df_with_flag.withColumn(\n",
    "                \"first_non_null\",\n",
    "                first(\"attribute_flag\", ignorenulls=True).over(case_window),\n",
    "            )\n",
    "            .filter(col(\"first_non_null\") == 1)\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        filtered_cases = (\n",
    "            first_occurrences\n",
    "            if filtered_cases is None\n",
    "            else filtered_cases.union(first_occurrences)\n",
    "        )\n",
    "\n",
    "    if last_occurrence:\n",
    "        # Find the last non-null occurrence of the attribute for each case\n",
    "        last_occurrences = (\n",
    "            df_with_flag.withColumn(\n",
    "                \"last_non_null\",\n",
    "                last(\"attribute_flag\", ignorenulls=True).over(case_window),\n",
    "            )\n",
    "            .filter(col(\"last_non_null\") == 1)\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        filtered_cases = (\n",
    "            last_occurrences\n",
    "            if filtered_cases is None\n",
    "            else filtered_cases.union(last_occurrences)\n",
    "        )\n",
    "\n",
    "    # Remove duplicates in case multiple conditions were applied\n",
    "    filtered_cases = filtered_cases.distinct()\n",
    "\n",
    "    # Join the filtered cases back to the original DataFrame\n",
    "    return df.join(filtered_cases, case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data with null values\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", None),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"Low\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", None),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", None),\n",
    "        (\"case3\", \"End\", \"2023-01-03 08:00:00\", \"High\"),\n",
    "        (\"case4\", \"Start\", \"2023-01-01 08:00:00\", None),\n",
    "        (\"case4\", \"Middle\", \"2023-01-02 08:00:00\", \"Medium\"),\n",
    "        (\"case4\", \"End\", \"2023-01-03 08:00:00\", None),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Test different occurrence types\n",
    "    occurrence_types = [\n",
    "        {\"once_occurred\": True},\n",
    "        {\"first_occurrence\": True},\n",
    "        {\"last_occurrence\": True},\n",
    "        {\"once_occurred\": True, \"first_occurrence\": True, \"last_occurrence\": True},\n",
    "    ]\n",
    "\n",
    "    for occurrence in occurrence_types:\n",
    "        filtered_df = filter_attribute_cases(\n",
    "            df, attribute_column=\"PRIORITY\", attribute_value=\"High\", **occurrence\n",
    "        )\n",
    "        print(f\"\\nFiltered DataFrame (PRIORITY = 'High', {occurrence}):\")\n",
    "        filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, first, last, when\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def standardize_attribute_values(\n",
    "    df: DataFrame,\n",
    "    attribute_columns: Union[str, List[str]],\n",
    "    use_first_occurrence: bool = True,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize attribute values for each case based on the first or last non-null occurrence.\n",
    "    Can handle either a single attribute or multiple attributes.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        attribute_columns (Union[str, List[str]]): Name of the column(s) containing the attribute(s) to standardize.\n",
    "        use_first_occurrence (bool): If True, use the first non-null occurrence; if False, use the last. Defaults to True.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with standardized attribute values for each case.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the attribute columns don't exist in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> # For a single attribute\n",
    "        >>> standardized_df = standardize_attribute_values(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=\"PRIORITY\",\n",
    "        ...     use_first_occurrence=True\n",
    "        ... )\n",
    "        >>> # For multiple attributes\n",
    "        >>> standardized_df = standardize_attribute_values(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"PRIORITY\", \"CATEGORY\", \"DEPARTMENT\"],\n",
    "        ...     use_first_occurrence=False\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Convert single attribute to list for uniform processing\n",
    "    if isinstance(attribute_columns, str):\n",
    "        attribute_columns = [attribute_columns]\n",
    "\n",
    "    # Validate input\n",
    "    missing_columns = [col for col in attribute_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns do not exist in the DataFrame: {', '.join(missing_columns)}\"\n",
    "        )\n",
    "\n",
    "    # Create a window spec for each case, ordered by timestamp\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Function to get standardized value\n",
    "    def get_standardized_value(column):\n",
    "        if use_first_occurrence:\n",
    "            return first(col(column), ignorenulls=True).over(case_window)\n",
    "        else:\n",
    "            return last(col(column), ignorenulls=True).over(case_window)\n",
    "\n",
    "    # Create standardized columns\n",
    "    for attribute_column in attribute_columns:\n",
    "        standardized_value = get_standardized_value(attribute_column)\n",
    "        df = df.withColumn(f\"standardized_{attribute_column}\", standardized_value)\n",
    "\n",
    "    # Replace original attribute columns with standardized values\n",
    "    for attribute_column in attribute_columns:\n",
    "        df = df.withColumn(\n",
    "            attribute_column,\n",
    "            when(\n",
    "                col(f\"standardized_{attribute_column}\").isNotNull(),\n",
    "                col(f\"standardized_{attribute_column}\"),\n",
    "            ).otherwise(col(attribute_column)),\n",
    "        ).drop(f\"standardized_{attribute_column}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data with null values and multiple attributes\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", None, \"Dept1\", None),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\", \"Cat1\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"Low\", \"Dept1\", \"Cat2\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\", \"Dept3\", \"Cat1\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\", \"Dept3\", \"Cat2\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\", \"Dept2\", \"Cat2\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", None, None, \"Cat3\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", None, \"Dept1\", None),\n",
    "        (\"case3\", \"End\", \"2023-01-03 08:00:00\", \"High\", \"Dept2\", \"Cat1\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(\n",
    "        data,\n",
    "        [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\", \"DEPARTMENT\", \"CATEGORY\"],\n",
    "    )\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Standardize a single attribute based on first occurrence\n",
    "    df_single = standardize_attribute_values(df, \"PRIORITY\", use_first_occurrence=True)\n",
    "    print(\"\\nStandardized DataFrame (Single Attribute, First Occurrence):\")\n",
    "    df_single.show(truncate=False)\n",
    "\n",
    "    # Standardize multiple attributes based on last occurrence\n",
    "    df_multiple = standardize_attribute_values(\n",
    "        df, [\"PRIORITY\", \"DEPARTMENT\", \"CATEGORY\"], use_first_occurrence=False\n",
    "    )\n",
    "    print(\"\\nStandardized DataFrame (Multiple Attributes, Last Occurrence):\")\n",
    "    df_multiple.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    concat_ws,\n",
    "    count,\n",
    "    desc,\n",
    "    percent_rank,\n",
    "    collect_list,\n",
    "    struct,\n",
    "    lit,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def process_variant_analysis(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    attribute_columns: Optional[List[str]] = None,\n",
    "    top_n_variants: int = 10,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform process variant analysis, including unique variants count, frequency, and \"happy path\" identification.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "        attribute_columns (Optional[List[str]]): List of attribute columns to distinguish variants. Defaults to None.\n",
    "        top_n_variants (int): Number of top variants to include in the detailed output. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with process variant analysis results.\n",
    "\n",
    "    Example:\n",
    "        >>> variant_analysis = process_variant_analysis(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"],\n",
    "        ...     top_n_variants=5\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if attribute_columns:\n",
    "        missing_columns = [col for col in attribute_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"The following columns do not exist in the DataFrame: {', '.join(missing_columns)}\"\n",
    "            )\n",
    "\n",
    "    # Create a window spec for ordering activities within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Collect activities (and attributes if specified) for each case\n",
    "    if attribute_columns:\n",
    "        collect_columns = [activity_column] + attribute_columns\n",
    "        df_variants = df.withColumn(\n",
    "            \"variant\",\n",
    "            concat_ws(\n",
    "                \"->\",\n",
    "                collect_list(concat_ws(\":\", *[col(c) for c in collect_columns])).over(\n",
    "                    case_window\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_variants = df.withColumn(\n",
    "            \"variant\", concat_ws(\"->\", collect_list(activity_column).over(case_window))\n",
    "        )\n",
    "\n",
    "    # Get unique variants per case\n",
    "    df_unique_variants = (\n",
    "        df_variants.groupBy(case_column)\n",
    "        .agg(collect_list(\"variant\").alias(\"variant_list\"))\n",
    "        .select(case_column, col(\"variant_list\").getItem(0).alias(\"variant\"))\n",
    "    )\n",
    "\n",
    "    # Calculate variant frequencies\n",
    "    variant_freq = (\n",
    "        df_unique_variants.groupBy(\"variant\")\n",
    "        .agg(count(\"*\").alias(\"frequency\"))\n",
    "        .orderBy(desc(\"frequency\"))\n",
    "    )\n",
    "\n",
    "    # Calculate total number of cases and identify the \"happy path\"\n",
    "    total_cases = df_unique_variants.count()\n",
    "    happy_path = variant_freq.first()[\"variant\"]\n",
    "    happy_path_frequency = variant_freq.first()[\"frequency\"]\n",
    "    happy_path_percentage = (happy_path_frequency / total_cases) * 100\n",
    "\n",
    "    # Calculate percentage for each variant\n",
    "    variant_freq = variant_freq.withColumn(\n",
    "        \"percentage\", (col(\"frequency\") / total_cases) * 100\n",
    "    )\n",
    "\n",
    "    # Add rank to variants\n",
    "    variant_freq = variant_freq.withColumn(\n",
    "        \"rank\", percent_rank().over(Window.orderBy(desc(\"frequency\")))\n",
    "    )\n",
    "\n",
    "    # Prepare the summary DataFrame\n",
    "    summary = spark.createDataFrame(\n",
    "        [\n",
    "            (\"Total Variants\", variant_freq.count()),\n",
    "            (\"Total Cases\", total_cases),\n",
    "            (\"Happy Path Frequency\", happy_path_frequency),\n",
    "            (\"Happy Path Percentage\", happy_path_percentage),\n",
    "        ],\n",
    "        [\"Metric\", \"Value\"],\n",
    "    )\n",
    "\n",
    "    # Prepare the top N variants DataFrame\n",
    "    top_variants = variant_freq.orderBy(desc(\"frequency\")).limit(top_n_variants)\n",
    "\n",
    "    # Combine summary and top variants into a single DataFrame\n",
    "    result = summary.select(\n",
    "        lit(\"Summary\").alias(\"category\"),\n",
    "        col(\"Metric\").alias(\"key\"),\n",
    "        col(\"Value\").alias(\"value\"),\n",
    "    ).union(\n",
    "        top_variants.select(\n",
    "            lit(\"Top Variants\").alias(\"category\"),\n",
    "            col(\"variant\").alias(\"key\"),\n",
    "            struct(col(\"frequency\"), col(\"percentage\")).alias(\"value\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\", \"Dept3\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\", \"Dept3\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", \"Low\", \"Dept1\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", \"Medium\", \"Dept1\"),\n",
    "        (\"case3\", \"End\", \"2023-01-05 08:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case4\", \"Start\", \"2023-01-01 10:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case4\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case4\", \"End\", \"2023-01-03 12:00:00\", \"High\", \"Dept1\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(\n",
    "        data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\", \"DEPARTMENT\"]\n",
    "    )\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Perform process variant analysis without attributes\n",
    "    variant_analysis = process_variant_analysis(df)\n",
    "    print(\"\\nProcess Variant Analysis (without attributes):\")\n",
    "    variant_analysis.show(truncate=False)\n",
    "\n",
    "    # Perform process variant analysis with attributes\n",
    "    variant_analysis_with_attrs = process_variant_analysis(\n",
    "        df, attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"]\n",
    "    )\n",
    "    print(\"\\nProcess Variant Analysis (with PRIORITY and DEPARTMENT attributes):\")\n",
    "    variant_analysis_with_attrs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Optional, Union, Literal\n",
    "\n",
    "\n",
    "def calculate_case_duration_stats(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    attribute_columns: Optional[Union[str, List[str]]] = None,\n",
    "    percentiles: List[float] = [0.25, 0.5, 0.75, 0.9],\n",
    "    time_unit: Literal[\"seconds\", \"minutes\", \"hours\", \"days\"] = \"days\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate case duration statistics, optionally aggregated by specified attributes, in the specified time unit.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "        attribute_columns (Optional[Union[str, List[str]]]): Column(s) to aggregate by. Can be None, a single column name, or a list of column names.\n",
    "        percentiles (List[float]): List of percentiles to calculate. Defaults to [0.25, 0.5, 0.75, 0.9].\n",
    "        time_unit (Literal[\"seconds\", \"minutes\", \"hours\", \"days\"]): The time unit for duration calculations. Defaults to \"days\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with case duration statistics, optionally aggregated by attributes, in the specified time unit.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any specified attribute column doesn't exist in the DataFrame or if an invalid time unit is provided.\n",
    "\n",
    "    Example:\n",
    "        >>> stats_df = calculate_case_duration_stats(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"],\n",
    "        ...     percentiles=[0.5, 0.75, 0.9],\n",
    "        ...     time_unit=\"hours\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a PySpark DataFrame\")\n",
    "\n",
    "    required_columns = [case_column, timestamp_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    if attribute_columns:\n",
    "        if isinstance(attribute_columns, str):\n",
    "            attribute_columns = [attribute_columns]\n",
    "        missing_attr_columns = [\n",
    "            col for col in attribute_columns if col not in df.columns\n",
    "        ]\n",
    "        if missing_attr_columns:\n",
    "            raise ValueError(\n",
    "                f\"The following attribute columns do not exist in the DataFrame: {', '.join(missing_attr_columns)}\"\n",
    "            )\n",
    "    else:\n",
    "        attribute_columns = []\n",
    "\n",
    "    # Validate and set time unit conversion factor\n",
    "    time_unit_factors = {\n",
    "        \"seconds\": 1,\n",
    "        \"minutes\": 60,\n",
    "        \"hours\": 3600,\n",
    "        \"days\": 86400,\n",
    "    }\n",
    "    if time_unit not in time_unit_factors:\n",
    "        raise ValueError(\n",
    "            f\"Invalid time unit. Must be one of: {', '.join(time_unit_factors.keys())}\"\n",
    "        )\n",
    "    time_factor = time_unit_factors[time_unit]\n",
    "\n",
    "    # Calculate case durations\n",
    "    window_spec = Window.partitionBy(case_column)\n",
    "    case_durations = (\n",
    "        df.withColumn(\n",
    "            \"start_time\",\n",
    "            F.min(F.to_timestamp(F.col(timestamp_column))).over(window_spec),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"end_time\", F.max(F.to_timestamp(F.col(timestamp_column))).over(window_spec)\n",
    "        )\n",
    "        .withColumn(\n",
    "            f\"duration_{time_unit}\",\n",
    "            F.round(\n",
    "                (F.unix_timestamp(\"end_time\") - F.unix_timestamp(\"start_time\"))\n",
    "                / time_factor,\n",
    "                2,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Select distinct rows to avoid duplication\n",
    "    select_columns = [case_column, f\"duration_{time_unit}\"] + attribute_columns\n",
    "    case_durations = case_durations.select(*select_columns).distinct()\n",
    "\n",
    "    # Prepare aggregation columns\n",
    "    agg_columns = []\n",
    "    duration_col = f\"duration_{time_unit}\"\n",
    "    agg_columns.extend(\n",
    "        [\n",
    "            F.avg(duration_col).alias(f\"avg_duration_{time_unit}\"),\n",
    "            F.min(duration_col).alias(f\"min_duration_{time_unit}\"),\n",
    "            F.max(duration_col).alias(f\"max_duration_{time_unit}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for p in percentiles:\n",
    "        agg_columns.append(\n",
    "            F.percentile_approx(duration_col, p).alias(\n",
    "                f\"p{int(p*100)}_duration_{time_unit}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Perform aggregation\n",
    "    if attribute_columns:\n",
    "        result = case_durations.groupBy(*attribute_columns).agg(*agg_columns)\n",
    "    else:\n",
    "        result = case_durations.agg(*agg_columns)\n",
    "\n",
    "    # Round duration values to 2 decimal places\n",
    "    duration_columns = [\n",
    "        f\"avg_duration_{time_unit}\",\n",
    "        f\"min_duration_{time_unit}\",\n",
    "        f\"max_duration_{time_unit}\",\n",
    "    ] + [f\"p{int(p*100)}_duration_{time_unit}\" for p in percentiles]\n",
    "\n",
    "    for col_name in duration_columns:\n",
    "        result = result.withColumn(col_name, F.round(F.col(col_name), 2))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test the function\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"TestCaseDurationStats\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "sample_data = [\n",
    "    (\"case1\", \"Start\", \"2023-01-01 10:00:00\", \"High\", \"Sales\"),\n",
    "    (\"case1\", \"Process\", \"2023-01-02 14:30:00\", \"High\", \"Sales\"),\n",
    "    (\"case1\", \"End\", \"2023-01-03 09:15:00\", \"High\", \"Sales\"),\n",
    "    (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Low\", \"Support\"),\n",
    "    (\"case2\", \"Process\", \"2023-01-01 11:30:00\", \"Low\", \"Support\"),\n",
    "    (\"case2\", \"End\", \"2023-01-01 16:45:00\", \"Low\", \"Support\"),\n",
    "    (\"case3\", \"Start\", \"2023-01-01 14:00:00\", \"Medium\", \"Sales\"),\n",
    "    (\"case3\", \"Process\", \"2023-01-03 10:30:00\", \"Medium\", \"Sales\"),\n",
    "    (\"case3\", \"End\", \"2023-01-04 15:15:00\", \"Medium\", \"Sales\"),\n",
    "]\n",
    "df = spark.createDataFrame(\n",
    "    sample_data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\", \"DEPARTMENT\"]\n",
    ")\n",
    "\n",
    "# Test cases\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\nTest 1: Calculate case duration stats without attribute columns\")\n",
    "result1 = calculate_case_duration_stats(df, time_unit=\"hours\")\n",
    "result1.show()\n",
    "\n",
    "print(\"\\nTest 2: Calculate case duration stats with one attribute column\")\n",
    "result2 = calculate_case_duration_stats(\n",
    "    df, attribute_columns=\"PRIORITY\", time_unit=\"hours\"\n",
    ")\n",
    "result2.show()\n",
    "\n",
    "print(\"\\nTest 3: Calculate case duration stats with multiple attribute columns\")\n",
    "result3 = calculate_case_duration_stats(\n",
    "    df, attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"], time_unit=\"hours\"\n",
    ")\n",
    "result3.show()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def filter_cases(\n",
    "    df: DataFrame,\n",
    "    filter_column: str,\n",
    "    filter_values: List[str],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter cases in a DataFrame based on specified criteria.\n",
    "\n",
    "    This function filters the input DataFrame to include only cases where the\n",
    "    specified filter_column has at least one value from the filter_values list.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame containing process data.\n",
    "        filter_column (str): Name of the column to apply the filter on.\n",
    "        filter_values (List[str]): List of values to filter by.\n",
    "        case_column (str, optional): Name of the case identifier column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the filter criteria.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input validation fails.\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.read.parquet(\"path/to/process_data\")\n",
    "        >>> filtered_df = filter_cases(\n",
    "        ...     df,\n",
    "        ...     filter_column=\"DIMENSION 1\",\n",
    "        ...     filter_values=[\"Value1\", \"Value2\"],\n",
    "        ...     case_column=\"_CASE_KEY\",\n",
    "        ...     activity_column=\"ACTIVITY\"\n",
    "        ... )\n",
    "        >>> filtered_df.show()\n",
    "\n",
    "    Note:\n",
    "        - This function uses Spark's DataFrame API for optimal performance.\n",
    "        - The function applies early filtering to improve efficiency.\n",
    "        - Cases are included if they have at least one event where the filter_column\n",
    "          value is in the filter_values list.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a PySpark DataFrame\")\n",
    "\n",
    "    required_columns = [case_column, activity_column, filter_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    if not isinstance(filter_values, list) or len(filter_values) == 0:\n",
    "        raise ValueError(\"filter_values must be a non-empty list\")\n",
    "\n",
    "    # Apply the filter\n",
    "    filtered_df = df.filter(F.col(filter_column).isin(filter_values))\n",
    "\n",
    "    # Get the unique case keys that meet the filter criteria\n",
    "    filtered_cases = filtered_df.select(case_column).distinct()\n",
    "\n",
    "    # Join the original DataFrame with the filtered cases\n",
    "    result_df = df.join(filtered_cases, on=case_column, how=\"inner\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"FilterCasesExample\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (\"case1\", \"Activity1\", \"DimA\", \"2023-07-01\"),\n",
    "    (\"case1\", \"Activity2\", \"DimB\", \"2023-07-02\"),\n",
    "    (\"case2\", \"Activity1\", \"DimA\", \"2023-07-01\"),\n",
    "    (\"case2\", \"Activity2\", \"DimC\", \"2023-07-02\"),\n",
    "    (\"case3\", \"Activity1\", \"DimB\", \"2023-07-01\"),\n",
    "    (\"case3\", \"Activity2\", \"DimB\", \"2023-07-02\"),\n",
    "    (\"case4\", \"Activity1\", \"DimC\", \"2023-07-01\"),\n",
    "    (\"case4\", \"Activity2\", \"DimD\", \"2023-07-02\"),\n",
    "]\n",
    "\n",
    "columns = [\"_CASE_KEY\", \"ACTIVITY\", \"DIMENSION\", \"DATE\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Apply the filter_cases function\n",
    "filtered_df = filter_cases(\n",
    "    df,\n",
    "    filter_column=\"DATE\",\n",
    "    filter_values=[\"2023-07-00\"],\n",
    "    case_column=\"_CASE_KEY\",\n",
    "    activity_column=\"ACTIVITY\",\n",
    ")\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "filtered_df.show()\n",
    "\n",
    "# Verify the results\n",
    "total_cases = df.select(\"_CASE_KEY\").distinct().count()\n",
    "filtered_cases = filtered_df.select(\"_CASE_KEY\").distinct().count()\n",
    "filtered_dimensions = filtered_df.select(\"DIMENSION\").distinct().collect()\n",
    "\n",
    "print(f\"Total cases in original DataFrame: {total_cases}\")\n",
    "print(f\"Cases in filtered DataFrame: {filtered_cases}\")\n",
    "print(\n",
    "    f\"Unique dimensions in filtered DataFrame: {[row['DIMENSION'] for row in filtered_dimensions]}\"\n",
    ")\n",
    "\n",
    "# Additional verification: Check if any non-filtered values are present\n",
    "unexpected_values = filtered_df.filter(~col(\"DIMENSION\").isin([\"DimA\", \"DimB\"])).count()\n",
    "print(f\"Number of rows with unexpected dimension values: {unexpected_values}\")\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def filter_cases(\n",
    "    df: DataFrame,\n",
    "    filter_column: str,\n",
    "    filter_values: List[str],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    start_time: Optional[str] = None,\n",
    "    end_time: Optional[str] = None,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter cases in a DataFrame based on specified criteria and an optional timeframe.\n",
    "\n",
    "    This function filters the input DataFrame to include only cases where the\n",
    "    specified filter_column has at least one value from the filter_values list,\n",
    "    and optionally within a specified timeframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame containing process data.\n",
    "        filter_column (str): Name of the column to apply the filter on.\n",
    "        filter_values (List[str]): List of values to filter by.\n",
    "        case_column (str, optional): Name of the case identifier column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "        start_time (str, optional): Start of the timeframe to filter by, in format \"DD.MM.YYYY\".\n",
    "            If None, no start time filter is applied.\n",
    "        end_time (str, optional): End of the timeframe to filter by, in format \"DD.MM.YYYY\".\n",
    "            If None, no end time filter is applied.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the filter criteria.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input validation fails or if the date format is incorrect.\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.read.parquet(\"path/to/process_data\")\n",
    "        >>> filtered_df = filter_cases(\n",
    "        ...     df,\n",
    "        ...     filter_column=\"DIMENSION 1\",\n",
    "        ...     filter_values=[\"Value1\", \"Value2\"],\n",
    "        ...     case_column=\"_CASE_KEY\",\n",
    "        ...     activity_column=\"ACTIVITY\",\n",
    "        ...     timestamp_column=\"EVENTTIME\",\n",
    "        ...     start_time=\"01.01.2023\",\n",
    "        ...     end_time=\"31.12.2023\"\n",
    "        ... )\n",
    "        >>> filtered_df.show()\n",
    "\n",
    "    Note:\n",
    "        - This function uses Spark's DataFrame API for optimal performance.\n",
    "        - The function applies early filtering to improve efficiency.\n",
    "        - Cases are included if they have at least one event where the filter_column\n",
    "          value is in the filter_values list and within the specified timeframe (if provided).\n",
    "        - The timeframe filter is inclusive of both start_time and end_time.\n",
    "        - If start_time and end_time are not provided, no time filtering is applied.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a PySpark DataFrame\")\n",
    "\n",
    "    required_columns = [case_column, activity_column, filter_column, timestamp_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    if not isinstance(filter_values, list) or len(filter_values) == 0:\n",
    "        raise ValueError(\"filter_values must be a non-empty list\")\n",
    "\n",
    "    # Function to parse date string\n",
    "    def parse_date(date_str):\n",
    "        if date_str:\n",
    "            try:\n",
    "                return datetime.strptime(date_str, \"%d.%m.%Y\")\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    f\"Incorrect date format for {date_str}. Use DD.MM.YYYY\"\n",
    "                )\n",
    "        return None\n",
    "\n",
    "    # Parse start and end times\n",
    "    start_datetime = parse_date(start_time)\n",
    "    end_datetime = parse_date(end_time)\n",
    "\n",
    "    # Build the filter condition\n",
    "    filter_condition = F.col(filter_column).isin(filter_values)\n",
    "\n",
    "    # Add timeframe condition if start_time and/or end_time are provided\n",
    "    if start_datetime:\n",
    "        filter_condition = filter_condition & (\n",
    "            F.col(timestamp_column) >= start_datetime\n",
    "        )\n",
    "    if end_datetime:\n",
    "        filter_condition = filter_condition & (F.col(timestamp_column) <= end_datetime)\n",
    "\n",
    "    # Apply the filter\n",
    "    filtered_df = df.filter(filter_condition)\n",
    "\n",
    "    # Get the unique case keys that meet the filter criteria\n",
    "    filtered_cases = filtered_df.select(case_column).distinct()\n",
    "\n",
    "    # Join the original DataFrame with the filtered cases\n",
    "    result_df = df.join(filtered_cases, on=case_column, how=\"inner\")\n",
    "\n",
    "    # Apply timeframe filter to the final result if specified\n",
    "    if start_datetime or end_datetime:\n",
    "        time_condition = F.lit(True)\n",
    "        if start_datetime:\n",
    "            time_condition = time_condition & (\n",
    "                F.col(timestamp_column) >= start_datetime\n",
    "            )\n",
    "        if end_datetime:\n",
    "            time_condition = time_condition & (F.col(timestamp_column) <= end_datetime)\n",
    "        result_df = result_df.filter(time_condition)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, lit\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"FilterCasesExample\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (\"case1\", \"Activity1\", \"DimA\", \"01.07.2023 10:00:00\"),\n",
    "    (\"case1\", \"Activity2\", \"DimB\", \"02.07.2023 11:00:00\"),\n",
    "    (\"case2\", \"Activity1\", \"DimA\", \"01.07.2023 09:00:00\"),\n",
    "    (\"case2\", \"Activity2\", \"DimC\", \"02.07.2023 14:00:00\"),\n",
    "    (\"case3\", \"Activity1\", \"DimB\", \"01.07.2023 08:00:00\"),\n",
    "    (\"case3\", \"Activity2\", \"DimB\", \"02.07.2023 16:00:00\"),\n",
    "    (\"case4\", \"Activity1\", \"DimC\", \"01.07.2023 12:00:00\"),\n",
    "    (\"case4\", \"Activity2\", \"DimD\", \"02.07.2023 13:00:00\"),\n",
    "]\n",
    "\n",
    "columns = [\"_CASE_KEY\", \"ACTIVITY\", \"DIMENSION\", \"EVENTTIME\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert string to timestamp\n",
    "df = df.withColumn(\"EVENTTIME\", to_timestamp(col(\"EVENTTIME\"), \"dd.MM.yyyy HH:mm:ss\"))\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Apply the filter_cases function with timeframe\n",
    "filtered_df = filter_cases(\n",
    "    df,\n",
    "    filter_column=\"DIMENSION\",\n",
    "    filter_values=[\"DimA\", \"DimB\"],\n",
    "    case_column=\"_CASE_KEY\",\n",
    "    activity_column=\"ACTIVITY\",\n",
    "    timestamp_column=\"EVENTTIME\",\n",
    "    start_time=\"01.07.2023\",\n",
    "    end_time=\"02.07.2023\",\n",
    ")\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "filtered_df.show(truncate=False)\n",
    "\n",
    "# Verify the results\n",
    "total_cases = df.select(\"_CASE_KEY\").distinct().count()\n",
    "filtered_cases = filtered_df.select(\"_CASE_KEY\").distinct().count()\n",
    "filtered_dimensions = filtered_df.select(\"DIMENSION\").distinct().collect()\n",
    "\n",
    "print(f\"Total cases in original DataFrame: {total_cases}\")\n",
    "print(f\"Cases in filtered DataFrame: {filtered_cases}\")\n",
    "print(\n",
    "    f\"Unique dimensions in filtered DataFrame: {[row['DIMENSION'] for row in filtered_dimensions]}\"\n",
    ")\n",
    "\n",
    "# Additional verification: Check if any events are outside the specified timeframe\n",
    "unexpected_times = filtered_df.filter(\n",
    "    (col(\"EVENTTIME\") < to_timestamp(lit(\"01.07.2023\"), \"dd.MM.yyyy\"))\n",
    "    | (col(\"EVENTTIME\") > to_timestamp(lit(\"02.07.2023\"), \"dd.MM.yyyy\"))\n",
    ").count()\n",
    "print(f\"Number of events outside the specified timeframe: {unexpected_times}\")\n",
    "\n",
    "# Test without time filter\n",
    "unfiltered_df = filter_cases(\n",
    "    df, filter_column=\"DIMENSION\", filter_values=[\"DimA\", \"DimB\"]\n",
    ")\n",
    "\n",
    "print(\"\\nFiltered DataFrame without time filter:\")\n",
    "unfiltered_df.show(truncate=False)\n",
    "print(\n",
    "    f\"Cases in unfiltered DataFrame: {unfiltered_df.select('_CASE_KEY').distinct().count()}\"\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def analyze_case_attributes(\n",
    "    df: DataFrame, attribute_columns: List[str], case_column: str = \"_CASE_KEY\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze case attributes, calculating distinct cases, total cases, and loops for each attribute combination.\n",
    "\n",
    "    This function groups the input DataFrame by the specified attribute columns, calculates the number of\n",
    "    distinct cases and total cases for each attribute combination, and identifies which combinations\n",
    "    have the most cases and loops. It uses efficient methods to avoid performance warnings.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame containing process data.\n",
    "        attribute_columns (List[str]): List of column names to use as attributes for grouping.\n",
    "        case_column (str, optional): Name of the case identifier column. Defaults to \"_CASE_KEY\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the analysis results with columns:\n",
    "            - Attribute columns (as specified in the input)\n",
    "            - distinct_cases: Number of distinct cases for each attribute combination\n",
    "            - total_cases: Total number of cases (including repeats) for each attribute combination\n",
    "            - loops: Number of loops (total_cases - distinct_cases) for each attribute combination\n",
    "            - distinct_cases_rank: Rank based on the number of distinct cases (1 being the highest)\n",
    "            - total_cases_rank: Rank based on the total number of cases (1 being the highest)\n",
    "            - loops_rank: Rank based on the number of loops (1 being the highest)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input validation fails.\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.read.parquet(\"path/to/process_data\")\n",
    "        >>> result = analyze_case_attributes(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"DIMENSION 1\", \"ACTIVITY\"],\n",
    "        ...     case_column=\"_CASE_KEY\"\n",
    "        ... )\n",
    "        >>> result.show()\n",
    "\n",
    "    Note:\n",
    "        - This function uses Spark's DataFrame API for optimal performance.\n",
    "        - The function calculates ranks for distinct cases, total cases, and loops to easily identify\n",
    "          the attribute combinations with the highest values in each category.\n",
    "        - Loops are calculated as the difference between total cases and distinct cases, representing\n",
    "          how many times cases revisit the same attribute combination on average.\n",
    "        - This version uses efficient methods to avoid performance warnings related to window functions.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(df, DataFrame):\n",
    "        raise ValueError(\"Input 'df' must be a PySpark DataFrame\")\n",
    "\n",
    "    if not attribute_columns:\n",
    "        raise ValueError(\"attribute_columns must be a non-empty list\")\n",
    "\n",
    "    if case_column not in df.columns:\n",
    "        raise ValueError(f\"Case column '{case_column}' not found in the DataFrame\")\n",
    "\n",
    "    missing_columns = [col for col in attribute_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing attribute columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Group by attribute columns and calculate metrics\n",
    "    grouped = df.groupBy(attribute_columns).agg(\n",
    "        F.countDistinct(case_column).alias(\"distinct_cases\"),\n",
    "        F.count(case_column).alias(\"total_cases\"),\n",
    "    )\n",
    "\n",
    "    # Calculate loops\n",
    "    result = grouped.withColumn(\"loops\", F.col(\"total_cases\") - F.col(\"distinct_cases\"))\n",
    "\n",
    "    # Calculate ranks using dense_rank() function\n",
    "    result = (\n",
    "        result.withColumn(\n",
    "            \"distinct_cases_rank\",\n",
    "            F.dense_rank().over(Window.orderBy(F.desc(\"distinct_cases\"))),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"total_cases_rank\",\n",
    "            F.dense_rank().over(Window.orderBy(F.desc(\"total_cases\"))),\n",
    "        )\n",
    "        .withColumn(\"loops_rank\", F.dense_rank().over(Window.orderBy(F.desc(\"loops\"))))\n",
    "    )\n",
    "\n",
    "    # Order the results\n",
    "    ordered_result = result.orderBy(\n",
    "        F.desc(\"distinct_cases\"), F.desc(\"total_cases\"), F.desc(\"loops\")\n",
    "    )\n",
    "\n",
    "    return ordered_result\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"AnalyzeCaseAttributesExample\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (\"case1\", \"DimA\", \"Activity1\", \"2023-07-01\"),\n",
    "    (\"case1\", \"DimA\", \"Activity2\", \"2023-07-02\"),\n",
    "    (\"case1\", \"DimB\", \"Activity1\", \"2023-07-03\"),\n",
    "    (\"case2\", \"DimA\", \"Activity1\", \"2023-07-01\"),\n",
    "    (\"case2\", \"DimC\", \"Activity2\", \"2023-07-02\"),\n",
    "    (\"case3\", \"DimB\", \"Activity1\", \"2023-07-01\"),\n",
    "    (\"case3\", \"DimB\", \"Activity1\", \"2023-07-02\"),\n",
    "    (\"case3\", \"DimB\", \"Activity2\", \"2023-07-03\"),\n",
    "    (\"case4\", \"DimC\", \"Activity1\", \"2023-07-01\"),\n",
    "    (\"case4\", \"DimC\", \"Activity1\", \"2023-07-02\"),\n",
    "    (\"case4\", \"DimA\", \"Activity2\", \"2023-07-03\"),\n",
    "    (\"case4\", \"DimA\", \"Activity2\", \"2023-07-04\"),\n",
    "]\n",
    "\n",
    "columns = [\"_CASE_KEY\", \"DIMENSION\", \"ACTIVITY\", \"DATE\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Analyze case attributes (using the optimized function)\n",
    "result = analyze_case_attributes(\n",
    "    df, attribute_columns=[\"DIMENSION\", \"ACTIVITY\"], case_column=\"_CASE_KEY\"\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis Results (Performance Optimized):\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Display top combinations\n",
    "print(\"\\nTop combination by distinct cases:\")\n",
    "result.filter(col(\"distinct_cases_rank\") == 1).show(truncate=False)\n",
    "\n",
    "print(\"\\nTop combination by total cases:\")\n",
    "result.filter(col(\"total_cases_rank\") == 1).show(truncate=False)\n",
    "\n",
    "print(\"\\nTop combination by loops:\")\n",
    "result.filter(col(\"loops_rank\") == 1).show(truncate=False)\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def analyze_event_frequency(\n",
    "    df: DataFrame, case_column: str = \"_CASE_KEY\", event_column: str = \"ACTIVITY\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the frequency and percentage of cases for each event, order the results in descending order,\n",
    "    and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_column (str, optional): The column name for the case key. Defaults to \"_CASE_KEY\".\n",
    "        event_column (str, optional): The column name for the event. Defaults to \"ACTIVITY\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the case count and percentage for each event, ordered in descending order.\n",
    "\n",
    "    Example:\n",
    "        example_data = [(\"CASE1\", \"Event1\"), (\"CASE1\", \"Event2\"), (\"CASE2\", \"Event1\"), (\"CASE3\", \"Event1\")]\n",
    "        example_df = spark.createDataFrame(example_data, [\"_CASE_KEY\", \"ACTIVITY\"])\n",
    "        result_df = analyze_event_frequency(example_df)\n",
    "        result_df.show()\n",
    "    \"\"\"\n",
    "    total_cases = df.select(case_column).distinct().count()\n",
    "\n",
    "    df_event_analysis = df.groupBy(event_column).agg(\n",
    "        F.countDistinct(case_column).alias(\"distinct_case_count\"),\n",
    "        F.count(case_column).alias(\"total_case_count\"),\n",
    "        (F.countDistinct(case_column) / total_cases * 100).alias(\n",
    "            \"distinct_case_percentage\"\n",
    "        ),\n",
    "        (F.count(case_column) / total_cases * 100).alias(\"total_case_percentage\"),\n",
    "    )\n",
    "\n",
    "    df_event_analysis = df_event_analysis.orderBy(F.desc(\"total_case_percentage\"))\n",
    "\n",
    "    return df_event_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def analyze_process_boundaries(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze the start and end activities of processes, showing the percentages and totals\n",
    "    for each activity that starts or ends a process.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe containing process data.\n",
    "        case_column (str, optional): The column name for the case key. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): The column name for the activity. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): The column name for the event timestamp. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A dataframe with the following columns:\n",
    "            - activity: The name of the activity\n",
    "            - total_occurrences: Total number of times this activity occurs\n",
    "            - start_count: Number of times this activity starts a process\n",
    "            - start_percentage: Percentage of processes this activity starts\n",
    "            - end_count: Number of times this activity ends a process\n",
    "            - end_percentage: Percentage of processes this activity ends\n",
    "\n",
    "    Example:\n",
    "        result_df = analyze_process_boundaries(process_df)\n",
    "        result_df.show()\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    required_columns = [case_column, activity_column, timestamp_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Window specifications\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Identify start and end activities\n",
    "    df_with_boundaries = df.withColumn(\n",
    "        \"is_start\", F.row_number().over(case_window) == 1\n",
    "    ).withColumn(\n",
    "        \"is_end\",\n",
    "        F.row_number().over(case_window.orderBy(F.desc(timestamp_column))) == 1,\n",
    "    )\n",
    "\n",
    "    # Calculate total cases\n",
    "    total_cases = df.select(case_column).distinct().count()\n",
    "\n",
    "    # Analyze activities\n",
    "    activity_analysis = df_with_boundaries.groupBy(activity_column).agg(\n",
    "        F.count(\"*\").alias(\"total_occurrences\"),\n",
    "        F.sum(F.when(F.col(\"is_start\"), 1).otherwise(0)).alias(\"start_count\"),\n",
    "        F.sum(F.when(F.col(\"is_end\"), 1).otherwise(0)).alias(\"end_count\"),\n",
    "    )\n",
    "\n",
    "    # Calculate percentages\n",
    "    result_df = activity_analysis.withColumn(\n",
    "        \"start_percentage\", (F.col(\"start_count\") / total_cases) * 100\n",
    "    ).withColumn(\"end_percentage\", (F.col(\"end_count\") / total_cases) * 100)\n",
    "\n",
    "    # Order results\n",
    "    result_df = result_df.orderBy(F.desc(\"total_occurrences\"))\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def identify_process_outliers(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    method: str = \"iqr\",\n",
    "    threshold: float = 1.5,\n",
    "    time_unit: str = \"hours\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Identify outlier cases in a process based on their total duration.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe containing process data.\n",
    "        case_column (str, optional): The column name for the case key. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): The column name for the activity. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): The column name for the event timestamp. Defaults to \"EVENTTIME\".\n",
    "        method (str, optional): The method to use for outlier detection. Options are \"iqr\" (Interquartile Range)\n",
    "                                or \"std\" (Standard Deviation). Defaults to \"iqr\".\n",
    "        threshold (float, optional): The threshold for outlier detection. For IQR method, typical values are 1.5 or 3.\n",
    "                                     For STD method, typical values are 2 or 3. Defaults to 1.5.\n",
    "        time_unit (str, optional): The unit for duration calculation. Options are \"seconds\", \"minutes\", \"hours\", \"days\".\n",
    "                                   Defaults to \"hours\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A dataframe with the following columns:\n",
    "            - case_key: The case identifier\n",
    "            - start_time: The start time of the case\n",
    "            - end_time: The end time of the case\n",
    "            - duration: The total duration of the case in the specified time unit\n",
    "            - is_outlier: Boolean indicating whether the case is an outlier\n",
    "            - outlier_score: How many times the duration exceeds the outlier threshold\n",
    "\n",
    "    Example:\n",
    "        outliers_df = identify_process_outliers(process_df, method=\"std\", threshold=3, time_unit=\"days\")\n",
    "        outliers_df.filter(F.col(\"is_outlier\") == True).show()\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    required_columns = [case_column, activity_column, timestamp_column]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    if method not in [\"iqr\", \"std\"]:\n",
    "        raise ValueError(\"Method must be either 'iqr' or 'std'\")\n",
    "\n",
    "    time_unit_seconds = {\"seconds\": 1, \"minutes\": 60, \"hours\": 3600, \"days\": 86400}\n",
    "    if time_unit not in time_unit_seconds:\n",
    "        raise ValueError(\n",
    "            f\"Invalid time unit. Choose from: {', '.join(time_unit_seconds.keys())}\"\n",
    "        )\n",
    "\n",
    "    # Calculate case durations\n",
    "    window_spec = Window.partitionBy(case_column)\n",
    "    case_durations = (\n",
    "        df.withColumn(\"start_time\", F.min(timestamp_column).over(window_spec))\n",
    "        .withColumn(\"end_time\", F.max(timestamp_column).over(window_spec))\n",
    "        .withColumn(\n",
    "            \"duration\",\n",
    "            (F.unix_timestamp(\"end_time\") - F.unix_timestamp(\"start_time\"))\n",
    "            / time_unit_seconds[time_unit],\n",
    "        )\n",
    "        .select(case_column, \"start_time\", \"end_time\", \"duration\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Calculate outlier thresholds\n",
    "    if method == \"iqr\":\n",
    "        quantiles = case_durations.approxQuantile(\"duration\", [0.25, 0.75], 0.05)\n",
    "        q1, q3 = quantiles[0], quantiles[1]\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - threshold * iqr\n",
    "        upper_bound = q3 + threshold * iqr\n",
    "    else:  # std method\n",
    "        stats = case_durations.select(\n",
    "            F.mean(\"duration\").alias(\"mean\"), F.stddev(\"duration\").alias(\"stddev\")\n",
    "        ).collect()[0]\n",
    "        mean, stddev = stats[\"mean\"], stats[\"stddev\"]\n",
    "        lower_bound = mean - threshold * stddev\n",
    "        upper_bound = mean + threshold * stddev\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = case_durations.withColumn(\n",
    "        \"is_outlier\",\n",
    "        (F.col(\"duration\") < lower_bound) | (F.col(\"duration\") > upper_bound),\n",
    "    ).withColumn(\n",
    "        \"outlier_score\",\n",
    "        F.when(\n",
    "            F.col(\"duration\") > upper_bound,\n",
    "            (F.col(\"duration\") - upper_bound) / (upper_bound - lower_bound),\n",
    "        )\n",
    "        .when(\n",
    "            F.col(\"duration\") < lower_bound,\n",
    "            (lower_bound - F.col(\"duration\")) / (upper_bound - lower_bound),\n",
    "        )\n",
    "        .otherwise(0),\n",
    "    )\n",
    "\n",
    "    return outliers.orderBy(F.desc(\"outlier_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def calculate_percentage_deviation(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    grouping_columns: List[str] = None,\n",
    "    time_unit: str = \"hours\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage deviation from the mean duration for each case,\n",
    "    based on specified grouping dimensions.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe containing process data.\n",
    "        case_column (str, optional): The column name for the case key. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str, optional): The column name for the event timestamp. Defaults to \"EVENTTIME\".\n",
    "        grouping_columns (List[str], optional): List of column names to use for grouping.\n",
    "                                                If None, calculates deviation across all cases. Defaults to None.\n",
    "        time_unit (str, optional): The unit for duration calculation.\n",
    "                                   Options are \"seconds\", \"minutes\", \"hours\", \"days\". Defaults to \"hours\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A dataframe with the following columns:\n",
    "            - case_key: The case identifier\n",
    "            - grouping columns (if specified)\n",
    "            - duration: The total duration of the case in the specified time unit\n",
    "            - mean_duration: The mean duration for the group\n",
    "            - percentage_deviation: The percentage deviation from the mean duration\n",
    "\n",
    "    Example:\n",
    "        deviation_df = calculate_percentage_deviation(\n",
    "            process_df,\n",
    "            grouping_columns=[\"DIMENSION 1\", \"ACTIVITY\"],\n",
    "            time_unit=\"days\"\n",
    "        )\n",
    "        deviation_df.orderBy(F.desc(\"percentage_deviation\")).show()\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    required_columns = [case_column, timestamp_column]\n",
    "    if grouping_columns:\n",
    "        required_columns.extend(grouping_columns)\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    time_unit_seconds = {\"seconds\": 1, \"minutes\": 60, \"hours\": 3600, \"days\": 86400}\n",
    "    if time_unit not in time_unit_seconds:\n",
    "        raise ValueError(\n",
    "            f\"Invalid time unit. Choose from: {', '.join(time_unit_seconds.keys())}\"\n",
    "        )\n",
    "\n",
    "    # Calculate case durations\n",
    "    case_window = Window.partitionBy(case_column)\n",
    "    case_durations = (\n",
    "        df.withColumn(\"start_time\", F.min(timestamp_column).over(case_window))\n",
    "        .withColumn(\"end_time\", F.max(timestamp_column).over(case_window))\n",
    "        .withColumn(\n",
    "            \"duration\",\n",
    "            (F.unix_timestamp(\"end_time\") - F.unix_timestamp(\"start_time\"))\n",
    "            / time_unit_seconds[time_unit],\n",
    "        )\n",
    "        .select(case_column, *grouping_columns, \"duration\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Calculate mean duration for each group\n",
    "    if grouping_columns:\n",
    "        group_window = Window.partitionBy(*grouping_columns)\n",
    "    else:\n",
    "        group_window = Window.partitionBy(F.lit(1))  # Global window if no grouping\n",
    "\n",
    "    deviation_df = case_durations.withColumn(\n",
    "        \"mean_duration\", F.avg(\"duration\").over(group_window)\n",
    "    ).withColumn(\n",
    "        \"percentage_deviation\",\n",
    "        (F.col(\"duration\") - F.col(\"mean_duration\")) / F.col(\"mean_duration\") * 100,\n",
    "    )\n",
    "\n",
    "    # Round the numeric columns for readability\n",
    "    numeric_columns = [\"duration\", \"mean_duration\", \"percentage_deviation\"]\n",
    "    for col in numeric_columns:\n",
    "        deviation_df = deviation_df.withColumn(col, F.round(col, 2))\n",
    "\n",
    "    return deviation_df.orderBy(F.desc(\"percentage_deviation\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
