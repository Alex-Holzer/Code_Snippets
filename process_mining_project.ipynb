{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def convert_to_timestamp(\n",
    "    df: DataFrame,\n",
    "    columns: Union[str, List[str]],\n",
    "    format: str = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert string column(s) to timestamp format in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        columns (Union[str, List[str]]): Column name(s) to convert.\n",
    "        format (str, optional): Timestamp format string. Defaults to \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with converted timestamp column(s).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input column(s) are not present in the DataFrame.\n",
    "\n",
    "    Examples:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"2023-07-27T13:18:12.039+0000\",), (\"2023-07-28T13:20:30.039+0000\",)]\n",
    "        >>> df = spark.createDataFrame(data, [\"ZEITPUNKT\"])\n",
    "        >>> result_df = convert_to_timestamp(df, \"ZEITPUNKT\")\n",
    "        >>> result_df.printSchema()\n",
    "        root\n",
    "         |-- ZEITPUNKT: timestamp (nullable = true)\n",
    "\n",
    "        >>> result_df.show(truncate=False)\n",
    "        +----------------------------+\n",
    "        |ZEITPUNKT                   |\n",
    "        +----------------------------+\n",
    "        |2023-07-27 13:18:12.039     |\n",
    "        |2023-07-28 13:20:30.039     |\n",
    "        +----------------------------+\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    # Validate input columns\n",
    "    invalid_columns = set(columns) - set(df.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns are not present in the DataFrame: {invalid_columns}\"\n",
    "        )\n",
    "\n",
    "    # Convert string columns to timestamp\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col, to_timestamp(df[col], format))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def standardize_process_mining_column_names(\n",
    "    df: DataFrame,\n",
    "    case_column: str,\n",
    "    activity_column: str,\n",
    "    timestamp_column: str,\n",
    "    standardized_case_name: str = \"_CASE_KEY\",\n",
    "    standardized_activity_name: str = \"ACTIVITY\",\n",
    "    standardized_timestamp_name: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize column names for process mining by renaming case, activity, and timestamp columns.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        case_column (str): Name of the column containing case IDs.\n",
    "        activity_column (str): Name of the column containing activities.\n",
    "        timestamp_column (str): Name of the column containing timestamps.\n",
    "        standardized_case_name (str, optional): Standardized name for the case column. Defaults to \"_CASE_KEY\".\n",
    "        standardized_activity_name (str, optional): Standardized name for the activity column. Defaults to \"ACTIVITY\".\n",
    "        standardized_timestamp_name (str, optional): Standardized name for the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with standardized column names for process mining.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns are not present in the input DataFrame.\n",
    "        ValueError: If any of the standardized names are already present in the DataFrame but don't match the columns to be renamed.\n",
    "\n",
    "    Example:\n",
    "        >>> data = [(\"A1\", \"Start\", \"2023-01-01\"), (\"A2\", \"End\", \"2023-01-02\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"ID\", \"Action\", \"Date\"])\n",
    "        >>> standardized_df = standardize_process_mining_column_names(df, \"ID\", \"Action\", \"Date\")\n",
    "        >>> standardized_df.show()\n",
    "        +-----------+--------+----------+\n",
    "        |_CASE_KEY  |ACTIVITY|EVENTTIME |\n",
    "        +-----------+--------+----------+\n",
    "        |A1         |Start   |2023-01-01|\n",
    "        |A2         |End     |2023-01-02|\n",
    "        +-----------+--------+----------+\n",
    "    \"\"\"\n",
    "    # Check if all specified columns are present in the DataFrame\n",
    "    required_columns = {case_column, activity_column, timestamp_column}\n",
    "    missing_columns = required_columns - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns are missing from the DataFrame: {missing_columns}\"\n",
    "        )\n",
    "\n",
    "    # Create a mapping of original column names to standardized names\n",
    "    column_mapping = {\n",
    "        case_column: standardized_case_name,\n",
    "        activity_column: standardized_activity_name,\n",
    "        timestamp_column: standardized_timestamp_name,\n",
    "    }\n",
    "\n",
    "    # Check if any of the standardized names already exist in the DataFrame\n",
    "    existing_standard_names = set(column_mapping.values()) & set(df.columns)\n",
    "    conflicting_names = existing_standard_names - set(column_mapping.keys())\n",
    "    if conflicting_names:\n",
    "        raise ValueError(\n",
    "            f\"The following standardized names already exist in the DataFrame and don't match the columns to be renamed: {conflicting_names}\"\n",
    "        )\n",
    "\n",
    "    # Rename the columns\n",
    "    for original, standardized in column_mapping.items():\n",
    "        df = df.withColumnRenamed(original, standardized)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import List, Callable, Dict, Any\n",
    "\n",
    "\n",
    "def process_mining_preprocessing_pipeline(\n",
    "    df: DataFrame,\n",
    "    pipeline_steps: List[Callable[[DataFrame, Dict[str, Any]], DataFrame]],\n",
    "    config: Dict[str, Any],\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a flexible series of preprocessing steps for process mining on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        pipeline_steps (List[Callable]): List of functions to apply to the DataFrame.\n",
    "        config (Dict[str, Any]): Configuration dictionary for the pipeline steps.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame ready for process mining.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the preprocessing or validation steps fail.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...         (\"A2\", \"End\", \"2023-01-02 11:00:00\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"ID\", \"Action\", \"Date\"])\n",
    "        >>> config = {\n",
    "        ...     \"case_column\": \"ID\",\n",
    "        ...     \"activity_column\": \"Action\",\n",
    "        ...     \"timestamp_column\": \"Date\",\n",
    "        ...     \"additional_string_columns\": []\n",
    "        ... }\n",
    "        >>> pipeline_steps = [\n",
    "        ...     trim_all_strings,\n",
    "        ...     convert_empty_string_to_null,\n",
    "        ...     standardize_process_mining_names,\n",
    "        ...     convert_to_timestamp,\n",
    "        ...     check_process_mining_conditions\n",
    "        ... ]\n",
    "        >>> processed_df = process_mining_preprocessing_pipeline(df, pipeline_steps, config)\n",
    "        >>> processed_df.show()\n",
    "    \"\"\"\n",
    "    for step in pipeline_steps:\n",
    "        df = df.transform(lambda df: step(df, config))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"A2\", \"End  \", \"2023-01-02 11:00:00\"),\n",
    "        (\"A3\", \"\", \"2023-01-03 12:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"ID\", \"Action\", \"Date\"])\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"case_column\": \"ID\",\n",
    "        \"activity_column\": \"Action\",\n",
    "        \"timestamp_column\": \"Date\",\n",
    "        \"additional_string_columns\": [],\n",
    "        \"show_examples\": True,  # for check_process_mining_conditions\n",
    "    }\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_steps = [\n",
    "        trim_all_strings,\n",
    "        convert_empty_string_to_null,\n",
    "        standardize_process_mining_names,\n",
    "        convert_to_timestamp,\n",
    "        check_process_mining_conditions,\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        processed_df = process_mining_preprocessing_pipeline(df, pipeline_steps, config)\n",
    "        print(\"Preprocessing successful!\")\n",
    "        processed_df.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Preprocessing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, when, year, expr, sum as spark_sum\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "def check_process_mining_conditions(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"TIMESTAMP\",\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Efficiently check if all necessary conditions for the process mining process are met.\n",
    "\n",
    "    This function performs all checks in a single pass over the data, minimizing Spark actions\n",
    "    and optimizing for large datasets.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"TIMESTAMP\".\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: A catchy success message if no errors are found, None otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the conditions are not met, with a summary of all errors found.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> from pyspark.sql.functions import to_timestamp\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [(\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...         (\"A1\", \"Process\", \"2023-01-01 11:00:00\"),\n",
    "        ...         (\"A1\", \"End\", \"2023-01-01 12:00:00\"),\n",
    "        ...         (\"A2\", \"Start\", \"2023-01-02 09:00:00\"),\n",
    "        ...         (\"A2\", \"End\", \"2023-01-02 10:00:00\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"CASE_KEY\", \"ACTIVITY\", \"TIMESTAMP\"])\n",
    "        >>> df = df.withColumn(\"TIMESTAMP\", to_timestamp(\"TIMESTAMP\"))\n",
    "        >>> result = check_process_mining_conditions(df)\n",
    "        >>> print(result)\n",
    "        ðŸŽ‰ Process mining data perfection achieved! Your data is ready to uncover insights.\n",
    "    \"\"\"\n",
    "    # Check column types\n",
    "    for column, expected_type in [\n",
    "        (case_column, StringType),\n",
    "        (activity_column, StringType),\n",
    "        (timestamp_column, TimestampType),\n",
    "    ]:\n",
    "        if not isinstance(df.schema[column].dataType, expected_type):\n",
    "            raise ValueError(\n",
    "                f\"ðŸ“Š Column '{column}' should be a {expected_type.__name__}, but it's a {df.schema[column].dataType}.\"\n",
    "            )\n",
    "\n",
    "    # Prepare all checks in a single DataFrame operation\n",
    "    checks_df = df.select(\n",
    "        when(col(case_column).isNull(), 1).otherwise(0).alias(\"null_cases\"),\n",
    "        when(col(activity_column).isNull(), 1).otherwise(0).alias(\"null_activities\"),\n",
    "        when(col(timestamp_column).isNull(), 1).otherwise(0).alias(\"null_timestamps\"),\n",
    "        when(year(col(timestamp_column)) < 1970, 1)\n",
    "        .otherwise(0)\n",
    "        .alias(\"early_timestamps\"),\n",
    "        when(year(col(timestamp_column)) > 2100, 1)\n",
    "        .otherwise(0)\n",
    "        .alias(\"future_timestamps\"),\n",
    "        expr(\n",
    "            f\"count(*) over (partition by {case_column}, {activity_column}, {timestamp_column})\"\n",
    "        ).alias(\"event_count\"),\n",
    "        expr(f\"count(*) over (partition by {case_column})\").alias(\"case_event_count\"),\n",
    "    )\n",
    "\n",
    "    # Collect all error counts in a single action\n",
    "    error_counts = checks_df.agg(\n",
    "        spark_sum(\"null_cases\").alias(\"null_cases\"),\n",
    "        spark_sum(\"null_activities\").alias(\"null_activities\"),\n",
    "        spark_sum(\"null_timestamps\").alias(\"null_timestamps\"),\n",
    "        spark_sum(\"early_timestamps\").alias(\"early_timestamps\"),\n",
    "        spark_sum(\"future_timestamps\").alias(\"future_timestamps\"),\n",
    "        spark_sum(when(col(\"event_count\") > 1, 1).otherwise(0)).alias(\n",
    "            \"duplicate_events\"\n",
    "        ),\n",
    "        spark_sum(when(col(\"case_event_count\") == 0, 1).otherwise(0)).alias(\n",
    "            \"cases_without_activities\"\n",
    "        ),\n",
    "    ).collect()[0]\n",
    "\n",
    "    # Convert to dictionary for easier handling\n",
    "    error_dict: Dict[str, int] = error_counts.asDict()\n",
    "\n",
    "    # Prepare error messages\n",
    "    error_messages = []\n",
    "    if error_dict[\"null_cases\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸš« Found {error_dict['null_cases']} null values in '{case_column}' column.\"\n",
    "        )\n",
    "    if error_dict[\"null_activities\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸš« Found {error_dict['null_activities']} null values in '{activity_column}' column.\"\n",
    "        )\n",
    "    if error_dict[\"null_timestamps\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸš« Found {error_dict['null_timestamps']} null values in '{timestamp_column}' column.\"\n",
    "        )\n",
    "    if error_dict[\"early_timestamps\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"â³ Found {error_dict['early_timestamps']} timestamps before 1970.\"\n",
    "        )\n",
    "    if error_dict[\"future_timestamps\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸ”® Found {error_dict['future_timestamps']} timestamps after 2100.\"\n",
    "        )\n",
    "    if error_dict[\"duplicate_events\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸ‘¯ Found {error_dict['duplicate_events']} duplicate events. Each combination of {case_column}, {activity_column}, and {timestamp_column} should be unique!\"\n",
    "        )\n",
    "    if error_dict[\"cases_without_activities\"] > 0:\n",
    "        error_messages.append(\n",
    "            f\"ðŸ” Found {error_dict['cases_without_activities']} cases with no activities.\"\n",
    "        )\n",
    "\n",
    "    if error_messages:\n",
    "        raise ValueError(\"\\n\".join(error_messages))\n",
    "\n",
    "    return \"ðŸŽ‰ Process mining data perfection achieved! Your data is ready to uncover insights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "\n",
    "\n",
    "def find_duplicate_case_activity_eventtime(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    eventtime_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies and returns all duplicate case-activity-eventtime combinations in a process mining DataFrame.\n",
    "\n",
    "    Duplicate combinations are defined as rows with the same values for _CASE_KEY, ACTIVITY, and EVENTTIME.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input PySpark DataFrame containing process mining data.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        eventtime_column (str, optional): Name of the eventtime column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing all rows that are part of duplicate combinations,\n",
    "                   along with a count of how many times each combination appears.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified columns are not present in the DataFrame or are of incorrect type.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> from pyspark.sql.functions import to_timestamp\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"A1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"A1\", \"Start\", \"2023-01-01 10:00:00\"),  # Duplicate\n",
    "        ...     (\"A1\", \"Process\", \"2023-01-01 11:00:00\"),\n",
    "        ...     (\"A2\", \"Start\", \"2023-01-02 09:00:00\"),\n",
    "        ...     (\"A2\", \"Start\", \"2023-01-02 09:00:00\"),  # Duplicate\n",
    "        ...     (\"A2\", \"End\", \"2023-01-02 10:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", to_timestamp(\"EVENTTIME\"))\n",
    "        >>> duplicates = find_duplicate_case_activity_eventtime(df)\n",
    "        >>> duplicates.show()\n",
    "        +---------+--------+-------------------+-----+\n",
    "        |_CASE_KEY|ACTIVITY|          EVENTTIME|count|\n",
    "        +---------+--------+-------------------+-----+\n",
    "        |       A1|   Start|2023-01-01 10:00:00|    2|\n",
    "        |       A2|   Start|2023-01-02 09:00:00|    2|\n",
    "        +---------+--------+-------------------+-----+\n",
    "    \"\"\"\n",
    "    # Validate column presence and types\n",
    "    for column, expected_type in [\n",
    "        (case_column, StringType),\n",
    "        (activity_column, StringType),\n",
    "        (eventtime_column, TimestampType),\n",
    "    ]:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
    "        if not isinstance(df.schema[column].dataType, expected_type):\n",
    "            raise ValueError(\n",
    "                f\"Column '{column}' should be of type {expected_type.__name__}, but is {df.schema[column].dataType}\"\n",
    "            )\n",
    "\n",
    "    # Identify duplicates\n",
    "    window_spec = Window.partitionBy(case_column, activity_column, eventtime_column)\n",
    "\n",
    "    duplicates_df = (\n",
    "        df.withColumn(\"count\", count(\"*\").over(window_spec))\n",
    "        .filter(col(\"count\") > 1)\n",
    "        .select(case_column, activity_column, eventtime_column, \"count\")\n",
    "        .distinct()\n",
    "        .orderBy(case_column, activity_column, eventtime_column)\n",
    "    )\n",
    "\n",
    "    return duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, lit, round\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def calculate_duration_from_start_to_target(\n",
    "    df: DataFrame,\n",
    "    target_activities: Union[str, List[str]],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the time difference between the start activity and the first occurrence of any specified target activity for each case.\n",
    "    Only returns cases where a target activity is found.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        target_activities (Union[str, List[str]]): The target activity or list of target activities to calculate duration to.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the following columns:\n",
    "            - case_column: The case identifier\n",
    "            - start_activity: The name of the start activity\n",
    "            - start_timestamp: The timestamp of the start activity\n",
    "            - target_activity: The name of the first occurring target activity\n",
    "            - target_timestamp: The timestamp of the first occurring target activity\n",
    "            - duration_minutes: Duration in minutes (rounded to 2 decimal places)\n",
    "            - duration_hours: Duration in hours (rounded to 2 decimal places)\n",
    "            - duration_days: Duration in days (rounded to 2 decimal places)\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        ...     (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        ...     (\"case2\", \"Start\", \"2023-01-01 09:00:00\"),\n",
    "        ...     (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        ...     (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        ...     (\"case3\", \"Other\", \"2023-01-02 08:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "        >>> result = calculate_duration_from_start_to_target(df, [\"Middle\", \"End\"])\n",
    "        >>> result.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Convert target_activities to a list if it's a single string\n",
    "    if isinstance(target_activities, str):\n",
    "        target_activities = [target_activities]\n",
    "\n",
    "    # Window specification for operations within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Find the start activity and its timestamp for each case\n",
    "    start_activity_df = (\n",
    "        df.withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"start_activity\"),\n",
    "            col(timestamp_column).alias(\"start_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Find the first occurrence of any target activity and its timestamp for each case\n",
    "    target_activity_df = (\n",
    "        df.filter(col(activity_column).isin(target_activities))\n",
    "        .withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"target_activity\"),\n",
    "            col(timestamp_column).alias(\"target_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join the start activity and target activity dataframes\n",
    "    result_df = start_activity_df.join(target_activity_df, case_column, \"inner\")\n",
    "\n",
    "    # Calculate duration in seconds, then convert to minutes, hours, and days\n",
    "    result_df = result_df.withColumn(\n",
    "        \"duration_seconds\",\n",
    "        F.unix_timestamp(\"target_timestamp\") - F.unix_timestamp(\"start_timestamp\"),\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        result_df.withColumn(\"duration_minutes\", round(col(\"duration_seconds\") / 60, 2))\n",
    "        .withColumn(\"duration_hours\", round(col(\"duration_seconds\") / 3600, 2))\n",
    "        .withColumn(\"duration_days\", round(col(\"duration_seconds\") / 86400, 2))\n",
    "        .drop(\"duration_seconds\")\n",
    "    )\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = [\n",
    "        case_column,\n",
    "        \"start_activity\",\n",
    "        \"start_timestamp\",\n",
    "        \"target_activity\",\n",
    "        \"target_timestamp\",\n",
    "        \"duration_minutes\",\n",
    "        \"duration_hours\",\n",
    "        \"duration_days\",\n",
    "    ]\n",
    "\n",
    "    return result_df.select(final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, round\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def calculate_duration_between_activities(\n",
    "    df: DataFrame,\n",
    "    start_activities: Union[str, List[str]],\n",
    "    target_activities: Union[str, List[str]],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the time difference between the first occurrence of any specified start activity\n",
    "    and the first occurrence of any specified target activity for each case.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        start_activities (Union[str, List[str]]): The start activity or list of start activities.\n",
    "        target_activities (Union[str, List[str]]): The target activity or list of target activities.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the following columns:\n",
    "            - case_column: The case identifier\n",
    "            - start_activity: The name of the first occurring start activity\n",
    "            - start_timestamp: The timestamp of the first occurring start activity\n",
    "            - target_activity: The name of the first occurring target activity\n",
    "            - target_timestamp: The timestamp of the first occurring target activity\n",
    "            - duration_minutes: Duration in minutes (rounded to 2 decimal places)\n",
    "            - duration_hours: Duration in hours (rounded to 2 decimal places)\n",
    "            - duration_days: Duration in days (rounded to 2 decimal places)\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        ...     (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        ...     (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        ...     (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        ...     (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        ...     (\"case3\", \"Other\", \"2023-01-02 08:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "        >>> result = calculate_duration_between_activities(df, [\"Start\", \"Begin\"], [\"Middle\", \"End\"])\n",
    "        >>> result.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Convert start_activities and target_activities to lists if they're single strings\n",
    "    if isinstance(start_activities, str):\n",
    "        start_activities = [start_activities]\n",
    "    if isinstance(target_activities, str):\n",
    "        target_activities = [target_activities]\n",
    "\n",
    "    # Window specification for operations within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Find the first occurrence of any start activity and its timestamp for each case\n",
    "    start_activity_df = (\n",
    "        df.filter(col(activity_column).isin(start_activities))\n",
    "        .withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"start_activity\"),\n",
    "            col(timestamp_column).alias(\"start_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Find the first occurrence of any target activity and its timestamp for each case\n",
    "    target_activity_df = (\n",
    "        df.filter(col(activity_column).isin(target_activities))\n",
    "        .withColumn(\"row_number\", F.row_number().over(case_window))\n",
    "        .filter(col(\"row_number\") == 1)\n",
    "        .select(\n",
    "            col(case_column),\n",
    "            col(activity_column).alias(\"target_activity\"),\n",
    "            col(timestamp_column).alias(\"target_timestamp\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join the start activity and target activity dataframes\n",
    "    result_df = start_activity_df.join(target_activity_df, case_column, \"inner\")\n",
    "\n",
    "    # Calculate duration in seconds, then convert to minutes, hours, and days\n",
    "    result_df = result_df.withColumn(\n",
    "        \"duration_seconds\",\n",
    "        F.unix_timestamp(\"target_timestamp\") - F.unix_timestamp(\"start_timestamp\"),\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        result_df.withColumn(\"duration_minutes\", round(col(\"duration_seconds\") / 60, 2))\n",
    "        .withColumn(\"duration_hours\", round(col(\"duration_seconds\") / 3600, 2))\n",
    "        .withColumn(\"duration_days\", round(col(\"duration_seconds\") / 86400, 2))\n",
    "        .drop(\"duration_seconds\")\n",
    "    )\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = [\n",
    "        case_column,\n",
    "        \"start_activity\",\n",
    "        \"start_timestamp\",\n",
    "        \"target_activity\",\n",
    "        \"target_timestamp\",\n",
    "        \"duration_minutes\",\n",
    "        \"duration_hours\",\n",
    "        \"duration_days\",\n",
    "    ]\n",
    "\n",
    "    return result_df.select(final_columns)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Other\", \"2023-01-02 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    # Calculate duration from the first occurrence of either \"Start\" or \"Begin\" to the first occurrence of either \"Middle\" or \"End\"\n",
    "    result = calculate_duration_between_activities(\n",
    "        df, [\"Start\", \"Begin\"], [\"Middle\", \"End\"]\n",
    "    )\n",
    "    result.show(truncate=False)\n",
    "\n",
    "    # Calculate duration from \"Start\" to \"End\" only\n",
    "    result_start_to_end = calculate_duration_between_activities(df, \"Start\", \"End\")\n",
    "    result_start_to_end.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, round, min as spark_min, max as spark_max\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def calculate_throughput_time(\n",
    "    df: DataFrame,\n",
    "    start_activities: Union[str, List[str]],\n",
    "    end_activities: Union[str, List[str]],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the throughput time between specified sets of start and end activities for each case.\n",
    "    The throughput time includes all intermediate activities between the first occurrence of any start activity\n",
    "    and the last occurrence of any end activity.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        start_activities (Union[str, List[str]]): The start activity or list of start activities.\n",
    "        end_activities (Union[str, List[str]]): The end activity or list of end activities.\n",
    "        case_column (str, optional): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str, optional): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str, optional): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the following columns:\n",
    "            - case_column: The case identifier\n",
    "            - first_start_activity: The name of the first occurring start activity\n",
    "            - first_start_timestamp: The timestamp of the first occurring start activity\n",
    "            - last_end_activity: The name of the last occurring end activity\n",
    "            - last_end_timestamp: The timestamp of the last occurring end activity\n",
    "            - throughput_time_minutes: Throughput time in minutes (rounded to 2 decimal places)\n",
    "            - throughput_time_hours: Throughput time in hours (rounded to 2 decimal places)\n",
    "            - throughput_time_days: Throughput time in days (rounded to 2 decimal places)\n",
    "            - activity_count: The number of activities between start and end (inclusive)\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.getOrCreate()\n",
    "        >>> data = [\n",
    "        ...     (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        ...     (\"case1\", \"Middle1\", \"2023-01-02 11:00:00\"),\n",
    "        ...     (\"case1\", \"Middle2\", \"2023-01-03 09:00:00\"),\n",
    "        ...     (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        ...     (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        ...     (\"case2\", \"Process\", \"2023-01-01 14:00:00\"),\n",
    "        ...     (\"case2\", \"Finish\", \"2023-01-01 17:00:00\"),\n",
    "        ...     (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        ...     (\"case3\", \"Other\", \"2023-01-02 08:00:00\")\n",
    "        ... ]\n",
    "        >>> df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "        >>> df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "        >>> result = calculate_throughput_time(df, [\"Start\", \"Begin\"], [\"End\", \"Finish\"])\n",
    "        >>> result.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Convert start_activities and end_activities to lists if they're single strings\n",
    "    if isinstance(start_activities, str):\n",
    "        start_activities = [start_activities]\n",
    "    if isinstance(end_activities, str):\n",
    "        end_activities = [end_activities]\n",
    "\n",
    "    # Window specification for operations within each case\n",
    "    case_window = Window.partitionBy(case_column)\n",
    "\n",
    "    # Find the first occurrence of any start activity and its timestamp for each case\n",
    "    start_df = df.filter(col(activity_column).isin(start_activities))\n",
    "    first_start = start_df.groupBy(case_column).agg(\n",
    "        spark_min(timestamp_column).alias(\"first_start_timestamp\"),\n",
    "        F.first(activity_column).alias(\"first_start_activity\"),\n",
    "    )\n",
    "\n",
    "    # Find the last occurrence of any end activity and its timestamp for each case\n",
    "    end_df = df.filter(col(activity_column).isin(end_activities))\n",
    "    last_end = end_df.groupBy(case_column).agg(\n",
    "        spark_max(timestamp_column).alias(\"last_end_timestamp\"),\n",
    "        F.last(activity_column).alias(\"last_end_activity\"),\n",
    "    )\n",
    "\n",
    "    # Join the start and end dataframes\n",
    "    result_df = first_start.join(last_end, case_column, \"inner\")\n",
    "\n",
    "    # Calculate throughput time in seconds, then convert to minutes, hours, and days\n",
    "    result_df = result_df.withColumn(\n",
    "        \"throughput_time_seconds\",\n",
    "        F.unix_timestamp(\"last_end_timestamp\")\n",
    "        - F.unix_timestamp(\"first_start_timestamp\"),\n",
    "    )\n",
    "\n",
    "    result_df = (\n",
    "        result_df.withColumn(\n",
    "            \"throughput_time_minutes\", round(col(\"throughput_time_seconds\") / 60, 2)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"throughput_time_hours\", round(col(\"throughput_time_seconds\") / 3600, 2)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"throughput_time_days\", round(col(\"throughput_time_seconds\") / 86400, 2)\n",
    "        )\n",
    "        .drop(\"throughput_time_seconds\")\n",
    "    )\n",
    "\n",
    "    # Calculate the number of activities between start and end (inclusive)\n",
    "    activity_count = (\n",
    "        df.filter(\n",
    "            (col(timestamp_column) >= col(\"first_start_timestamp\"))\n",
    "            & (col(timestamp_column) <= col(\"last_end_timestamp\"))\n",
    "        )\n",
    "        .groupBy(case_column)\n",
    "        .agg(F.count(\"*\").alias(\"activity_count\"))\n",
    "    )\n",
    "\n",
    "    # Join the activity count to the result dataframe\n",
    "    result_df = result_df.join(activity_count, case_column, \"left_outer\")\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = [\n",
    "        case_column,\n",
    "        \"first_start_activity\",\n",
    "        \"first_start_timestamp\",\n",
    "        \"last_end_activity\",\n",
    "        \"last_end_timestamp\",\n",
    "        \"throughput_time_minutes\",\n",
    "        \"throughput_time_hours\",\n",
    "        \"throughput_time_days\",\n",
    "        \"activity_count\",\n",
    "    ]\n",
    "\n",
    "    return result_df.select(final_columns)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle1\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"Middle2\", \"2023-01-03 09:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Begin\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Process\", \"2023-01-01 14:00:00\"),\n",
    "        (\"case2\", \"Finish\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Other\", \"2023-01-02 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    # Calculate throughput time from the first occurrence of either \"Start\" or \"Begin\" to the last occurrence of either \"End\" or \"Finish\"\n",
    "    result = calculate_throughput_time(df, [\"Start\", \"Begin\"], [\"End\", \"Finish\"])\n",
    "    result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lower,\n",
    "    first,\n",
    "    last,\n",
    "    collect_set,\n",
    "    array_contains,\n",
    "    lit,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "\n",
    "def filter_process_cases(\n",
    "    df: DataFrame,\n",
    "    case_flows_through: Optional[Union[str, List[str]]] = None,\n",
    "    case_does_not_flow_through: Optional[Union[str, List[str]]] = None,\n",
    "    case_starts_with: Optional[Union[str, List[str]]] = None,\n",
    "    case_ends_with: Optional[Union[str, List[str]]] = None,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Highly optimized function to filter process cases based on specified flow conditions for large datasets (case-insensitive).\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        case_flows_through (Optional[Union[str, List[str]]]): Activity or list of activities that cases must flow through.\n",
    "        case_does_not_flow_through (Optional[Union[str, List[str]]]): Activity or list of activities that cases must not flow through.\n",
    "        case_starts_with (Optional[Union[str, List[str]]]): Activity or list of activities that cases must start with.\n",
    "        case_ends_with (Optional[Union[str, List[str]]]): Activity or list of activities that cases must end with.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet all specified conditions.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_process_cases(\n",
    "        ...     df,\n",
    "        ...     case_flows_through=[\"Middle\", \"Review\"],\n",
    "        ...     case_does_not_flow_through=\"Reject\",\n",
    "        ...     case_starts_with=\"Start\",\n",
    "        ...     case_ends_with=[\"End\", \"Complete\"]\n",
    "        ... )\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to convert input to lowercase list\n",
    "    def to_lower_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        return [s.lower() for s in (x if isinstance(x, list) else [x])]\n",
    "\n",
    "    # Convert inputs to lowercase lists\n",
    "    flows_through = to_lower_list(case_flows_through)\n",
    "    not_flows_through = to_lower_list(case_does_not_flow_through)\n",
    "    starts_with = to_lower_list(case_starts_with)\n",
    "    ends_with = to_lower_list(case_ends_with)\n",
    "\n",
    "    # If no filtering conditions are provided, return the original DataFrame\n",
    "    if not any([flows_through, not_flows_through, starts_with, ends_with]):\n",
    "        return df\n",
    "\n",
    "    # Create a window spec for each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Precompute case summary\n",
    "    case_summary = df.groupBy(case_column).agg(\n",
    "        collect_set(lower(col(activity_column))).alias(\"activities\"),\n",
    "        first(lower(col(activity_column))).alias(\"first_activity\"),\n",
    "        last(lower(col(activity_column))).alias(\"last_activity\"),\n",
    "    )\n",
    "\n",
    "    # Apply filters\n",
    "    if flows_through:\n",
    "        case_summary = case_summary.filter(\n",
    "            all(\n",
    "                array_contains(col(\"activities\"), lit(activity))\n",
    "                for activity in flows_through\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if not_flows_through:\n",
    "        case_summary = case_summary.filter(\n",
    "            ~array_contains(col(\"activities\"), lit(activity))\n",
    "            for activity in not_flows_through\n",
    "        )\n",
    "\n",
    "    if starts_with:\n",
    "        case_summary = case_summary.filter(col(\"first_activity\").isin(starts_with))\n",
    "\n",
    "    if ends_with:\n",
    "        case_summary = case_summary.filter(col(\"last_activity\").isin(ends_with))\n",
    "\n",
    "    # Get the list of cases that meet all conditions\n",
    "    valid_cases = case_summary.select(case_column)\n",
    "\n",
    "    # Filter the original DataFrame\n",
    "    return df.join(valid_cases, case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data (you would replace this with your large dataset)\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"start\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Other\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case2\", \"END\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Begin\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"middle\", \"2023-01-02 08:00:00\"),\n",
    "        (\"case3\", \"Finish\", \"2023-01-03 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    filtered_df = filter_process_cases(\n",
    "        df,\n",
    "        case_flows_through=\"Middle\",\n",
    "        case_does_not_flow_through=\"Other\",\n",
    "        case_starts_with=[\"Start\", \"Begin\"],\n",
    "        case_ends_with=[\"End\", \"Finish\"],\n",
    "    )\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "    print(\"\\nFiltered DataFrame:\")\n",
    "    filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lower, lead, when, lit, collect_list\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def filter_process_flow(\n",
    "    df: DataFrame,\n",
    "    first_activity: str,\n",
    "    second_activity: str,\n",
    "    relationship: Literal[\n",
    "        \"directly_followed\",\n",
    "        \"followed_anytime_by\",\n",
    "        \"not_directly_followed\",\n",
    "        \"never_followed_by\",\n",
    "    ],\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter the process flow based on the relationship between two specified activities.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        first_activity (str): The first activity in the relationship.\n",
    "        second_activity (str): The second activity in the relationship.\n",
    "        relationship (Literal[\"directly_followed\", \"followed_anytime_by\", \"not_directly_followed\", \"never_followed_by\"]):\n",
    "            The type of relationship between the two activities.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the specified relationship.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid relationship is provided.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_process_flow(\n",
    "        ...     df,\n",
    "        ...     first_activity=\"Start\",\n",
    "        ...     second_activity=\"Review\",\n",
    "        ...     relationship=\"directly_followed\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate the relationship parameter\n",
    "    valid_relationships = [\n",
    "        \"directly_followed\",\n",
    "        \"followed_anytime_by\",\n",
    "        \"not_directly_followed\",\n",
    "        \"never_followed_by\",\n",
    "    ]\n",
    "    if relationship not in valid_relationships:\n",
    "        raise ValueError(\n",
    "            f\"Invalid relationship. Must be one of: {', '.join(valid_relationships)}\"\n",
    "        )\n",
    "\n",
    "    # Create a window spec for each case, ordered by timestamp\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Convert activities to lowercase for case-insensitive matching\n",
    "    df = df.withColumn(activity_column, lower(col(activity_column)))\n",
    "    first_activity = first_activity.lower()\n",
    "    second_activity = second_activity.lower()\n",
    "\n",
    "    if relationship == \"directly_followed\":\n",
    "        # Add a column with the next activity\n",
    "        df_with_next = df.withColumn(\n",
    "            \"next_activity\", lead(activity_column).over(case_window)\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity is directly followed by second_activity\n",
    "        filtered_cases = (\n",
    "            df_with_next.filter(\n",
    "                (col(activity_column) == first_activity)\n",
    "                & (col(\"next_activity\") == second_activity)\n",
    "            )\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "    elif relationship == \"followed_anytime_by\":\n",
    "        # Collect all activities for each case\n",
    "        case_activities = df.groupBy(case_column).agg(\n",
    "            collect_list(activity_column).alias(\"activities\")\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity appears before second_activity\n",
    "        filtered_cases = case_activities.filter(\n",
    "            (array_contains(col(\"activities\"), first_activity))\n",
    "            & (array_contains(col(\"activities\"), second_activity))\n",
    "            & (\n",
    "                col(\"activities\").getItem(\n",
    "                    array_position(col(\"activities\"), first_activity)\n",
    "                )\n",
    "                < col(\"activities\").getItem(\n",
    "                    array_position(col(\"activities\"), second_activity)\n",
    "                )\n",
    "            )\n",
    "        ).select(case_column)\n",
    "\n",
    "    elif relationship == \"not_directly_followed\":\n",
    "        # Add a column with the next activity\n",
    "        df_with_next = df.withColumn(\n",
    "            \"next_activity\", lead(activity_column).over(case_window)\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity is not directly followed by second_activity\n",
    "        filtered_cases = (\n",
    "            df_with_next.filter(\n",
    "                (col(activity_column) == first_activity)\n",
    "                & (col(\"next_activity\") != second_activity)\n",
    "            )\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "    else:  # never_followed_by\n",
    "        # Collect all activities for each case\n",
    "        case_activities = df.groupBy(case_column).agg(\n",
    "            collect_list(activity_column).alias(\"activities\")\n",
    "        )\n",
    "\n",
    "        # Filter cases where first_activity appears but is never followed by second_activity\n",
    "        filtered_cases = case_activities.filter(\n",
    "            (array_contains(col(\"activities\"), first_activity))\n",
    "            & (\n",
    "                ~array_contains(col(\"activities\"), second_activity)\n",
    "                | (\n",
    "                    col(\"activities\").getItem(\n",
    "                        array_position(col(\"activities\"), first_activity)\n",
    "                    )\n",
    "                    > col(\"activities\").getItem(\n",
    "                        array_position(col(\"activities\"), second_activity)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ).select(case_column)\n",
    "\n",
    "    # Join the filtered cases back to the original DataFrame\n",
    "    return df.join(filtered_cases, case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Review\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-03 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    relationships = [\n",
    "        \"directly_followed\",\n",
    "        \"followed_anytime_by\",\n",
    "        \"not_directly_followed\",\n",
    "        \"never_followed_by\",\n",
    "    ]\n",
    "\n",
    "    for rel in relationships:\n",
    "        filtered_df = filter_process_flow(df, \"Start\", \"Review\", rel)\n",
    "        print(f\"\\nFiltered DataFrame ({rel}):\")\n",
    "        filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lower, count, when\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def filter_rework_cases(\n",
    "    df: DataFrame,\n",
    "    activity: str,\n",
    "    occurs_between: Optional[Tuple[int, int]] = None,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter cases based on the number of occurrences of a specific activity within a given range.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        activity (str): The activity to check for rework.\n",
    "        occurs_between (Optional[Tuple[int, int]]): The range of occurrences to filter for.\n",
    "                                                    Defaults to None, which is treated as (0, float('inf')).\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the specified rework criteria.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_rework_cases(\n",
    "        ...     df,\n",
    "        ...     activity=\"Review\",\n",
    "        ...     occurs_between=(2, 4)\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Set default range if not provided\n",
    "    if occurs_between is None:\n",
    "        occurs_between = (0, float(\"inf\"))\n",
    "\n",
    "    min_occurrences, max_occurrences = occurs_between\n",
    "\n",
    "    # Validate input\n",
    "    if min_occurrences < 0 or max_occurrences < min_occurrences:\n",
    "        raise ValueError(\"Invalid occurrence range. Ensure min >= 0 and max >= min.\")\n",
    "\n",
    "    # Convert activity to lowercase for case-insensitive matching\n",
    "    activity = activity.lower()\n",
    "\n",
    "    # Count occurrences of the activity for each case\n",
    "    activity_counts = df.groupBy(case_column).agg(\n",
    "        count(when(lower(col(activity_column)) == activity, True)).alias(\n",
    "            \"activity_count\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filter based on the occurrence range\n",
    "    if max_occurrences == float(\"inf\"):\n",
    "        filtered_cases = activity_counts.filter(\n",
    "            col(\"activity_count\") >= min_occurrences\n",
    "        )\n",
    "    else:\n",
    "        filtered_cases = activity_counts.filter(\n",
    "            (col(\"activity_count\") >= min_occurrences)\n",
    "            & (col(\"activity_count\") <= max_occurrences)\n",
    "        )\n",
    "\n",
    "    # Handle the special case of (0,1)\n",
    "    if min_occurrences == 0 and max_occurrences == 1:\n",
    "        filtered_cases = activity_counts.filter(col(\"activity_count\") <= 1)\n",
    "\n",
    "    # Join the filtered cases back to the original DataFrame\n",
    "    return df.join(filtered_cases.select(case_column), case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case1\", \"Review\", \"2023-01-02 11:00:00\"),\n",
    "        (\"case1\", \"Review\", \"2023-01-03 12:00:00\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\"),\n",
    "        (\"case2\", \"Review\", \"2023-01-01 10:00:00\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-02 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-03 08:00:00\"),\n",
    "        (\"case3\", \"Review\", \"2023-01-04 08:00:00\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Test different occurrence ranges\n",
    "    ranges = [(None), (0, 1), (1, 2), (2, float(\"inf\"))]\n",
    "\n",
    "    for range_ in ranges:\n",
    "        filtered_df = filter_rework_cases(df, \"Review\", range_)\n",
    "        range_str = f\"({range_[0]}, {range_[1]})\" if range_ else \"Default (0, inf)\"\n",
    "        print(f\"\\nFiltered DataFrame (Review occurs between {range_str}):\")\n",
    "        filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, first, last, when, lit, sum, isnull\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def filter_attribute_cases(\n",
    "    df: DataFrame,\n",
    "    attribute_column: str,\n",
    "    attribute_value: Union[str, int, float],\n",
    "    once_occurred: bool = False,\n",
    "    first_occurrence: bool = False,\n",
    "    last_occurrence: bool = False,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter cases based on the occurrence of a specific attribute value in the process,\n",
    "    ignoring null values when considering first or last occurrences.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        attribute_column (str): Name of the column containing the attribute to filter on.\n",
    "        attribute_value (Union[str, int, float]): The value of the attribute to filter for.\n",
    "        once_occurred (bool): If True, select cases where the attribute occurred at least once. Defaults to False.\n",
    "        first_occurrence (bool): If True, select cases where the attribute occurred first (ignoring nulls). Defaults to False.\n",
    "        last_occurrence (bool): If True, select cases where the attribute occurred last (ignoring nulls). Defaults to False.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only the cases that meet the specified attribute criteria.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no occurrence type is specified or if the attribute column doesn't exist.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_df = filter_attribute_cases(\n",
    "        ...     df,\n",
    "        ...     attribute_column=\"PRIORITY\",\n",
    "        ...     attribute_value=\"High\",\n",
    "        ...     first_occurrence=True\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not any([once_occurred, first_occurrence, last_occurrence]):\n",
    "        raise ValueError(\n",
    "            \"At least one occurrence type (once_occurred, first_occurrence, or last_occurrence) must be True.\"\n",
    "        )\n",
    "\n",
    "    if attribute_column not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Column '{attribute_column}' does not exist in the DataFrame.\"\n",
    "        )\n",
    "\n",
    "    # Create a window spec for each case, ordered by timestamp\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Create a flag column for rows where the attribute matches the specified value\n",
    "    df_with_flag = df.withColumn(\n",
    "        \"attribute_flag\",\n",
    "        when(\n",
    "            (col(attribute_column) == attribute_value)\n",
    "            & (~isnull(col(attribute_column))),\n",
    "            1,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    # Perform filtering based on occurrence type\n",
    "    filtered_cases = None\n",
    "\n",
    "    if once_occurred:\n",
    "        # Select cases where the attribute occurred at least once\n",
    "        filtered_cases = (\n",
    "            df_with_flag.groupBy(case_column)\n",
    "            .agg((sum(\"attribute_flag\") > 0).alias(\"occurred\"))\n",
    "            .filter(col(\"occurred\"))\n",
    "            .select(case_column)\n",
    "        )\n",
    "\n",
    "    if first_occurrence:\n",
    "        # Find the first non-null occurrence of the attribute for each case\n",
    "        first_occurrences = (\n",
    "            df_with_flag.withColumn(\n",
    "                \"first_non_null\",\n",
    "                first(\"attribute_flag\", ignorenulls=True).over(case_window),\n",
    "            )\n",
    "            .filter(col(\"first_non_null\") == 1)\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        filtered_cases = (\n",
    "            first_occurrences\n",
    "            if filtered_cases is None\n",
    "            else filtered_cases.union(first_occurrences)\n",
    "        )\n",
    "\n",
    "    if last_occurrence:\n",
    "        # Find the last non-null occurrence of the attribute for each case\n",
    "        last_occurrences = (\n",
    "            df_with_flag.withColumn(\n",
    "                \"last_non_null\",\n",
    "                last(\"attribute_flag\", ignorenulls=True).over(case_window),\n",
    "            )\n",
    "            .filter(col(\"last_non_null\") == 1)\n",
    "            .select(case_column)\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        filtered_cases = (\n",
    "            last_occurrences\n",
    "            if filtered_cases is None\n",
    "            else filtered_cases.union(last_occurrences)\n",
    "        )\n",
    "\n",
    "    # Remove duplicates in case multiple conditions were applied\n",
    "    filtered_cases = filtered_cases.distinct()\n",
    "\n",
    "    # Join the filtered cases back to the original DataFrame\n",
    "    return df.join(filtered_cases, case_column, \"inner\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data with null values\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", None),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"Low\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", None),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", None),\n",
    "        (\"case3\", \"End\", \"2023-01-03 08:00:00\", \"High\"),\n",
    "        (\"case4\", \"Start\", \"2023-01-01 08:00:00\", None),\n",
    "        (\"case4\", \"Middle\", \"2023-01-02 08:00:00\", \"Medium\"),\n",
    "        (\"case4\", \"End\", \"2023-01-03 08:00:00\", None),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\"])\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Test different occurrence types\n",
    "    occurrence_types = [\n",
    "        {\"once_occurred\": True},\n",
    "        {\"first_occurrence\": True},\n",
    "        {\"last_occurrence\": True},\n",
    "        {\"once_occurred\": True, \"first_occurrence\": True, \"last_occurrence\": True},\n",
    "    ]\n",
    "\n",
    "    for occurrence in occurrence_types:\n",
    "        filtered_df = filter_attribute_cases(\n",
    "            df, attribute_column=\"PRIORITY\", attribute_value=\"High\", **occurrence\n",
    "        )\n",
    "        print(f\"\\nFiltered DataFrame (PRIORITY = 'High', {occurrence}):\")\n",
    "        filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, first, last, when\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def standardize_attribute_values(\n",
    "    df: DataFrame,\n",
    "    attribute_columns: Union[str, List[str]],\n",
    "    use_first_occurrence: bool = True,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize attribute values for each case based on the first or last non-null occurrence.\n",
    "    Can handle either a single attribute or multiple attributes.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        attribute_columns (Union[str, List[str]]): Name of the column(s) containing the attribute(s) to standardize.\n",
    "        use_first_occurrence (bool): If True, use the first non-null occurrence; if False, use the last. Defaults to True.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with standardized attribute values for each case.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the attribute columns don't exist in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> # For a single attribute\n",
    "        >>> standardized_df = standardize_attribute_values(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=\"PRIORITY\",\n",
    "        ...     use_first_occurrence=True\n",
    "        ... )\n",
    "        >>> # For multiple attributes\n",
    "        >>> standardized_df = standardize_attribute_values(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"PRIORITY\", \"CATEGORY\", \"DEPARTMENT\"],\n",
    "        ...     use_first_occurrence=False\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Convert single attribute to list for uniform processing\n",
    "    if isinstance(attribute_columns, str):\n",
    "        attribute_columns = [attribute_columns]\n",
    "\n",
    "    # Validate input\n",
    "    missing_columns = [col for col in attribute_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"The following columns do not exist in the DataFrame: {', '.join(missing_columns)}\"\n",
    "        )\n",
    "\n",
    "    # Create a window spec for each case, ordered by timestamp\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Function to get standardized value\n",
    "    def get_standardized_value(column):\n",
    "        if use_first_occurrence:\n",
    "            return first(col(column), ignorenulls=True).over(case_window)\n",
    "        else:\n",
    "            return last(col(column), ignorenulls=True).over(case_window)\n",
    "\n",
    "    # Create standardized columns\n",
    "    for attribute_column in attribute_columns:\n",
    "        standardized_value = get_standardized_value(attribute_column)\n",
    "        df = df.withColumn(f\"standardized_{attribute_column}\", standardized_value)\n",
    "\n",
    "    # Replace original attribute columns with standardized values\n",
    "    for attribute_column in attribute_columns:\n",
    "        df = df.withColumn(\n",
    "            attribute_column,\n",
    "            when(\n",
    "                col(f\"standardized_{attribute_column}\").isNotNull(),\n",
    "                col(f\"standardized_{attribute_column}\"),\n",
    "            ).otherwise(col(attribute_column)),\n",
    "        ).drop(f\"standardized_{attribute_column}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data with null values and multiple attributes\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", None, \"Dept1\", None),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\", \"Cat1\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"Low\", \"Dept1\", \"Cat2\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\", \"Dept3\", \"Cat1\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\", \"Dept3\", \"Cat2\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\", \"Dept2\", \"Cat2\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", None, None, \"Cat3\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", None, \"Dept1\", None),\n",
    "        (\"case3\", \"End\", \"2023-01-03 08:00:00\", \"High\", \"Dept2\", \"Cat1\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(\n",
    "        data,\n",
    "        [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\", \"DEPARTMENT\", \"CATEGORY\"],\n",
    "    )\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Standardize a single attribute based on first occurrence\n",
    "    df_single = standardize_attribute_values(df, \"PRIORITY\", use_first_occurrence=True)\n",
    "    print(\"\\nStandardized DataFrame (Single Attribute, First Occurrence):\")\n",
    "    df_single.show(truncate=False)\n",
    "\n",
    "    # Standardize multiple attributes based on last occurrence\n",
    "    df_multiple = standardize_attribute_values(\n",
    "        df, [\"PRIORITY\", \"DEPARTMENT\", \"CATEGORY\"], use_first_occurrence=False\n",
    "    )\n",
    "    print(\"\\nStandardized DataFrame (Multiple Attributes, Last Occurrence):\")\n",
    "    df_multiple.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    min,\n",
    "    max,\n",
    "    avg,\n",
    "    percentile_approx,\n",
    "    collect_set,\n",
    "    round as spark_round,\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from typing import Union, List, Optional, Literal\n",
    "\n",
    "\n",
    "def calculate_case_duration_stats(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    attribute_columns: Optional[Union[str, List[str]]] = None,\n",
    "    percentiles: List[float] = [0.25, 0.5, 0.75, 0.9],\n",
    "    time_unit: Literal[\"seconds\", \"minutes\", \"hours\", \"days\"] = \"days\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate case duration statistics, optionally aggregated by specified attributes, in the specified time unit.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "        attribute_columns (Optional[Union[str, List[str]]]): Column(s) to aggregate by. Can be None, a single column name, or a list of column names.\n",
    "        percentiles (List[float]): List of percentiles to calculate. Defaults to [0.25, 0.5, 0.75, 0.9].\n",
    "        time_unit (Literal[\"seconds\", \"minutes\", \"hours\", \"days\"]): The time unit for duration calculations. Defaults to \"days\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with case duration statistics, optionally aggregated by attributes, in the specified time unit.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any specified attribute column doesn't exist in the DataFrame or if an invalid time unit is provided.\n",
    "\n",
    "    Example:\n",
    "        >>> stats_df = calculate_case_duration_stats(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"],\n",
    "        ...     percentiles=[0.5, 0.75, 0.9],\n",
    "        ...     time_unit=\"hours\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if attribute_columns:\n",
    "        if isinstance(attribute_columns, str):\n",
    "            attribute_columns = [attribute_columns]\n",
    "        missing_columns = [col for col in attribute_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"The following columns do not exist in the DataFrame: {', '.join(missing_columns)}\"\n",
    "            )\n",
    "\n",
    "    # Validate and set time unit conversion factor\n",
    "    time_unit_factors = {\n",
    "        \"seconds\": 1,\n",
    "        \"minutes\": 1 / 60,\n",
    "        \"hours\": 1 / 3600,\n",
    "        \"days\": 1 / 86400,\n",
    "    }\n",
    "    if time_unit not in time_unit_factors:\n",
    "        raise ValueError(\n",
    "            f\"Invalid time unit. Must be one of: {', '.join(time_unit_factors.keys())}\"\n",
    "        )\n",
    "    time_factor = time_unit_factors[time_unit]\n",
    "\n",
    "    # Calculate case durations\n",
    "    case_durations = df.groupBy(case_column).agg(\n",
    "        (\n",
    "            (\n",
    "                max(col(timestamp_column)).cast(\"long\")\n",
    "                - min(col(timestamp_column)).cast(\"long\")\n",
    "            )\n",
    "            * time_factor\n",
    "        ).alias(f\"duration_{time_unit}\")\n",
    "    )\n",
    "\n",
    "    # Prepare aggregation columns\n",
    "    agg_columns = []\n",
    "    if attribute_columns:\n",
    "        for attr in attribute_columns:\n",
    "            agg_columns.append(collect_set(attr).alias(attr))\n",
    "\n",
    "    duration_col = f\"duration_{time_unit}\"\n",
    "    agg_columns.extend(\n",
    "        [\n",
    "            avg(duration_col).alias(f\"avg_duration_{time_unit}\"),\n",
    "            min(duration_col).alias(f\"min_duration_{time_unit}\"),\n",
    "            max(duration_col).alias(f\"max_duration_{time_unit}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for p in percentiles:\n",
    "        agg_columns.append(\n",
    "            percentile_approx(duration_col, p).alias(\n",
    "                f\"p{int(p*100)}_duration_{time_unit}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Perform aggregation\n",
    "    if attribute_columns:\n",
    "        result = case_durations.groupBy(*attribute_columns).agg(*agg_columns)\n",
    "    else:\n",
    "        result = case_durations.agg(*agg_columns)\n",
    "\n",
    "    # Round duration values to 2 decimal places\n",
    "    duration_columns = [\n",
    "        f\"avg_duration_{time_unit}\",\n",
    "        f\"min_duration_{time_unit}\",\n",
    "        f\"max_duration_{time_unit}\",\n",
    "    ] + [f\"p{int(p*100)}_duration_{time_unit}\" for p in percentiles]\n",
    "    for col_name in duration_columns:\n",
    "        result = result.withColumn(col_name, spark_round(col(col_name), 2))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\", \"Dept3\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\", \"Dept3\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", \"Low\", \"Dept1\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", \"Medium\", \"Dept1\"),\n",
    "        (\"case3\", \"End\", \"2023-01-05 08:00:00\", \"High\", \"Dept2\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(\n",
    "        data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\", \"DEPARTMENT\"]\n",
    "    )\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Calculate overall statistics in different time units\n",
    "    for unit in [\"seconds\", \"minutes\", \"hours\", \"days\"]:\n",
    "        stats = calculate_case_duration_stats(df, time_unit=unit)\n",
    "        print(f\"\\nOverall Case Duration Statistics (in {unit}):\")\n",
    "        stats.show(truncate=False)\n",
    "\n",
    "    # Calculate statistics aggregated by PRIORITY in hours\n",
    "    priority_stats = calculate_case_duration_stats(\n",
    "        df, attribute_columns=\"PRIORITY\", time_unit=\"hours\"\n",
    "    )\n",
    "    print(\"\\nCase Duration Statistics by PRIORITY (in hours):\")\n",
    "    priority_stats.show(truncate=False)\n",
    "\n",
    "    # Calculate statistics aggregated by multiple attributes in days\n",
    "    multi_attr_stats = calculate_case_duration_stats(\n",
    "        df, attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"], time_unit=\"days\"\n",
    "    )\n",
    "    print(\"\\nCase Duration Statistics by PRIORITY and DEPARTMENT (in days):\")\n",
    "    multi_attr_stats.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    concat_ws,\n",
    "    count,\n",
    "    desc,\n",
    "    percent_rank,\n",
    "    collect_list,\n",
    "    struct,\n",
    "    lit,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def process_variant_analysis(\n",
    "    df: DataFrame,\n",
    "    case_column: str = \"_CASE_KEY\",\n",
    "    activity_column: str = \"ACTIVITY\",\n",
    "    timestamp_column: str = \"EVENTTIME\",\n",
    "    attribute_columns: Optional[List[str]] = None,\n",
    "    top_n_variants: int = 10,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform process variant analysis, including unique variants count, frequency, and \"happy path\" identification.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with process mining data.\n",
    "        case_column (str): Name of the case column. Defaults to \"_CASE_KEY\".\n",
    "        activity_column (str): Name of the activity column. Defaults to \"ACTIVITY\".\n",
    "        timestamp_column (str): Name of the timestamp column. Defaults to \"EVENTTIME\".\n",
    "        attribute_columns (Optional[List[str]]): List of attribute columns to distinguish variants. Defaults to None.\n",
    "        top_n_variants (int): Number of top variants to include in the detailed output. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with process variant analysis results.\n",
    "\n",
    "    Example:\n",
    "        >>> variant_analysis = process_variant_analysis(\n",
    "        ...     df,\n",
    "        ...     attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"],\n",
    "        ...     top_n_variants=5\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if attribute_columns:\n",
    "        missing_columns = [col for col in attribute_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"The following columns do not exist in the DataFrame: {', '.join(missing_columns)}\"\n",
    "            )\n",
    "\n",
    "    # Create a window spec for ordering activities within each case\n",
    "    case_window = Window.partitionBy(case_column).orderBy(timestamp_column)\n",
    "\n",
    "    # Collect activities (and attributes if specified) for each case\n",
    "    if attribute_columns:\n",
    "        collect_columns = [activity_column] + attribute_columns\n",
    "        df_variants = df.withColumn(\n",
    "            \"variant\",\n",
    "            concat_ws(\n",
    "                \"->\",\n",
    "                collect_list(concat_ws(\":\", *[col(c) for c in collect_columns])).over(\n",
    "                    case_window\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_variants = df.withColumn(\n",
    "            \"variant\", concat_ws(\"->\", collect_list(activity_column).over(case_window))\n",
    "        )\n",
    "\n",
    "    # Get unique variants per case\n",
    "    df_unique_variants = (\n",
    "        df_variants.groupBy(case_column)\n",
    "        .agg(collect_list(\"variant\").alias(\"variant_list\"))\n",
    "        .select(case_column, col(\"variant_list\").getItem(0).alias(\"variant\"))\n",
    "    )\n",
    "\n",
    "    # Calculate variant frequencies\n",
    "    variant_freq = (\n",
    "        df_unique_variants.groupBy(\"variant\")\n",
    "        .agg(count(\"*\").alias(\"frequency\"))\n",
    "        .orderBy(desc(\"frequency\"))\n",
    "    )\n",
    "\n",
    "    # Calculate total number of cases and identify the \"happy path\"\n",
    "    total_cases = df_unique_variants.count()\n",
    "    happy_path = variant_freq.first()[\"variant\"]\n",
    "    happy_path_frequency = variant_freq.first()[\"frequency\"]\n",
    "    happy_path_percentage = (happy_path_frequency / total_cases) * 100\n",
    "\n",
    "    # Calculate percentage for each variant\n",
    "    variant_freq = variant_freq.withColumn(\n",
    "        \"percentage\", (col(\"frequency\") / total_cases) * 100\n",
    "    )\n",
    "\n",
    "    # Add rank to variants\n",
    "    variant_freq = variant_freq.withColumn(\n",
    "        \"rank\", percent_rank().over(Window.orderBy(desc(\"frequency\")))\n",
    "    )\n",
    "\n",
    "    # Prepare the summary DataFrame\n",
    "    summary = spark.createDataFrame(\n",
    "        [\n",
    "            (\"Total Variants\", variant_freq.count()),\n",
    "            (\"Total Cases\", total_cases),\n",
    "            (\"Happy Path Frequency\", happy_path_frequency),\n",
    "            (\"Happy Path Percentage\", happy_path_percentage),\n",
    "        ],\n",
    "        [\"Metric\", \"Value\"],\n",
    "    )\n",
    "\n",
    "    # Prepare the top N variants DataFrame\n",
    "    top_variants = variant_freq.orderBy(desc(\"frequency\")).limit(top_n_variants)\n",
    "\n",
    "    # Combine summary and top variants into a single DataFrame\n",
    "    result = summary.select(\n",
    "        lit(\"Summary\").alias(\"category\"),\n",
    "        col(\"Metric\").alias(\"key\"),\n",
    "        col(\"Value\").alias(\"value\"),\n",
    "    ).union(\n",
    "        top_variants.select(\n",
    "            lit(\"Top Variants\").alias(\"category\"),\n",
    "            col(\"variant\").alias(\"key\"),\n",
    "            struct(col(\"frequency\"), col(\"percentage\")).alias(\"value\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (\"case1\", \"Start\", \"2023-01-01 10:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case1\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case1\", \"End\", \"2023-01-03 12:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case2\", \"Start\", \"2023-01-01 09:00:00\", \"Medium\", \"Dept3\"),\n",
    "        (\"case2\", \"Middle\", \"2023-01-01 10:00:00\", \"High\", \"Dept3\"),\n",
    "        (\"case2\", \"End\", \"2023-01-01 17:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case3\", \"Start\", \"2023-01-01 08:00:00\", \"Low\", \"Dept1\"),\n",
    "        (\"case3\", \"Middle\", \"2023-01-02 08:00:00\", \"Medium\", \"Dept1\"),\n",
    "        (\"case3\", \"End\", \"2023-01-05 08:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case4\", \"Start\", \"2023-01-01 10:00:00\", \"High\", \"Dept1\"),\n",
    "        (\"case4\", \"Middle\", \"2023-01-02 11:00:00\", \"High\", \"Dept2\"),\n",
    "        (\"case4\", \"End\", \"2023-01-03 12:00:00\", \"High\", \"Dept1\"),\n",
    "    ]\n",
    "    df = spark.createDataFrame(\n",
    "        data, [\"_CASE_KEY\", \"ACTIVITY\", \"EVENTTIME\", \"PRIORITY\", \"DEPARTMENT\"]\n",
    "    )\n",
    "    df = df.withColumn(\"EVENTTIME\", F.to_timestamp(\"EVENTTIME\"))\n",
    "\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # Perform process variant analysis without attributes\n",
    "    variant_analysis = process_variant_analysis(df)\n",
    "    print(\"\\nProcess Variant Analysis (without attributes):\")\n",
    "    variant_analysis.show(truncate=False)\n",
    "\n",
    "    # Perform process variant analysis with attributes\n",
    "    variant_analysis_with_attrs = process_variant_analysis(\n",
    "        df, attribute_columns=[\"PRIORITY\", \"DEPARTMENT\"]\n",
    "    )\n",
    "    print(\"\\nProcess Variant Analysis (with PRIORITY and DEPARTMENT attributes):\")\n",
    "    variant_analysis_with_attrs.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
