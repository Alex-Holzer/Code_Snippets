{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/22 19:16:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Change Data Type\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(1, \"100\", \"Tom\", \"Brady\"), (2, \"200\", \"Tomson\", \"Lary\")],\n",
    "    [\"id\", \"value\", \"Name\", \"SURNAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtypes(df, dict_mapping):\n",
    "    \"\"\"\n",
    "    This function changes the data types of specified columns in a DataFrame.\n",
    "\n",
    "    It iterates over a dictionary where the keys are column names and the values are the desired data types.\n",
    "    For each key-value pair, it changes the data type of the corresponding column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pyspark.sql.DataFrame): The DataFrame whose column data types are to be changed.\n",
    "    dict_mapping (dict): A dictionary mapping column names (keys) to their desired data types (values).\n",
    "\n",
    "    Returns:\n",
    "    df (pyspark.sql.DataFrame): The DataFrame with the changed data types.\n",
    "\n",
    "    Example:\n",
    "    Suppose we have a DataFrame 'df' with columns 'A', 'B', and 'C' of type string, integer, and string respectively.\n",
    "    If we want to change 'A' to integer and 'C' to float, we would use the function as follows:\n",
    "\n",
    "    dict_mapping_dtypes = {'A': 'int', 'C': 'float'}\n",
    "    df = change_dtypes(df, dict_mapping_dtypes)\n",
    "    \"\"\"\n",
    "    for col_name, col_dtype in dict_mapping.items():\n",
    "        df = df.withColumn(col_name, df[col_name].cast(col_dtype))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, columns_map):\n",
    "    \"\"\"\n",
    "    This function renames the columns of a DataFrame based on a provided mapping.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame whose columns are to be renamed.\n",
    "    columns_map (dict): A dictionary mapping old column names (keys) to new column names (values).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns.\n",
    "\n",
    "    Example:\n",
    "    Suppose we have a DataFrame 'df' with columns 'first_name' and 'last_name'.\n",
    "    If we want to rename 'first_name' to 'name' and 'last_name' to 'surname', we would use the function as follows:\n",
    "\n",
    "    columns_map = {\"first_name\": \"name\", \"last_name\": \"surname\"}\n",
    "    df = rename_columns(df, columns_map)\n",
    "    \"\"\"\n",
    "    for old_name, new_name in columns_map.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    # Return the DataFrame with renamed columns\n",
    "    return df\n",
    "\n",
    "\n",
    "columns_map = {\"first_name\": \"name\", \"last_name\": \"surname\"}\n",
    "# Use the rename_columns function to rename the columns in the DataFrame\n",
    "df = rename_columns(df, columns_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+\n",
      "| id|value|  Name|SURNAME|\n",
      "+---+-----+------+-------+\n",
      "|  1|  100|  Benn|  Brady|\n",
      "|  2|  200|Tomson|   Lary|\n",
      "+---+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary for replacement\n",
    "replacement_dict = {\n",
    "    \"Tom\": \"Benn\",\n",
    "}\n",
    "\n",
    "df_replaced = df.replace(replacement_dict, subset=[\"Name\"])\n",
    "df_replaced.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+----------+\n",
      "| ID|Age|  Salary|Experience|\n",
      "+---+---+--------+----------+\n",
      "|  1| 25| 30000.0|         2|\n",
      "|  2| 35| 50000.0|         7|\n",
      "|  3| 45| 75000.0|        15|\n",
      "|  4| 55|100000.0|        25|\n",
      "|  5| 65| 80000.0|        35|\n",
      "|  6| 18| 20000.0|         0|\n",
      "|  7| 75| 60000.0|        40|\n",
      "|  8| 30| 45000.0|         5|\n",
      "|  9| 40| 70000.0|        12|\n",
      "| 10| 50| 90000.0|        20|\n",
      "+---+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+\n",
      "|   name|age|stte|\n",
      "+-------+---+----+\n",
      "|  Alice| 34|  NY|\n",
      "|    Bob| 45|  CA|\n",
      "|Charlie| 29|  TX|\n",
      "+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the column names of a PySpark DataFrame to ensure consistency:\n",
    "    - Trims leading and trailing spaces.\n",
    "    - Replaces spaces with underscores.\n",
    "    - Removes any non-alphanumeric characters except underscores.\n",
    "    - Ensures the column name does not start with a digit by prefixing with 'col_' if necessary.\n",
    "    - Ensures the column name does not end with an underscore.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with cleaned column names.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.appName(\"CleanColumnNames\").getOrCreate()\n",
    "        >>> data = [(\"Alice\", 34, \"NY\"), (\"Bob\", 45, \"CA\"), (\"Charlie\", 29, \"TX\")]\n",
    "        >>> df = spark.createDataFrame(data, [\" name \", \" age \", \" st@te \"])\n",
    "        >>> df_clean = clean_column_names(df)\n",
    "        >>> df_clean.show()\n",
    "        +-------+---+-----+\n",
    "        |   name|age|state|\n",
    "        +-------+---+-----+\n",
    "        |  Alice| 34|   NY|\n",
    "        |    Bob| 45|   CA|\n",
    "        |Charlie| 29|   TX|\n",
    "        +-------+---+-----+\n",
    "        >>> spark.stop()\n",
    "    \"\"\"\n",
    "    new_column_names = []\n",
    "    for column in df.columns:\n",
    "        # Trim leading and trailing spaces\n",
    "        new_col = column.strip()\n",
    "        # Replace spaces with underscores\n",
    "        new_col = new_col.replace(\" \", \"_\")\n",
    "        # Remove any non-alphanumeric characters except underscores\n",
    "        new_col = re.sub(r\"[^\\w]\", \"\", new_col)\n",
    "        # Ensure the column does not start with a digit\n",
    "        if new_col[0].isdigit():\n",
    "            new_col = f\"col_{new_col}\"\n",
    "        # Ensure the column does not end with an underscore\n",
    "        if new_col.endswith(\"_\"):\n",
    "            new_col = new_col.rstrip(\"_\")\n",
    "        new_column_names.append(new_col)\n",
    "\n",
    "    for old_col, new_col in zip(df.columns, new_column_names):\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CleanColumnNames\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 34, \"NY\"), (\"Bob\", 45, \"CA\"), (\"Charlie\", 29, \"TX\")]\n",
    "df = spark.createDataFrame(data, [\" name \", \" age \", \" st@te \"])\n",
    "\n",
    "df_clean = clean_column_names(df)\n",
    "df_clean.show()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|   NAME|AGE|STATE|\n",
      "+-------+---+-----+\n",
      "|  Alice| 34|   NY|\n",
      "|    Bob| 45|   CA|\n",
      "|Charlie| 29|   TX|\n",
      "+-------+---+-----+\n",
      "\n",
      "+-------+---+-----+\n",
      "|   name|age|state|\n",
      "+-------+---+-----+\n",
      "|  Alice| 34|   NY|\n",
      "|    Bob| 45|   CA|\n",
      "|Charlie| 29|   TX|\n",
      "+-------+---+-----+\n",
      "\n",
      "+-------+---+-----+\n",
      "|   Name|Age|State|\n",
      "+-------+---+-----+\n",
      "|  Alice| 34|   NY|\n",
      "|    Bob| 45|   CA|\n",
      "|Charlie| 29|   TX|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def convert_column_names(df: DataFrame, case: str = \"lower\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the column names of a PySpark DataFrame to the specified case:\n",
    "    - 'upper': Converts all column names to uppercase.\n",
    "    - 'lower': Converts all column names to lowercase.\n",
    "    - 'title': Converts the first letter of each word in column names to uppercase.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input PySpark DataFrame.\n",
    "        case (str): The case to convert the column names to ('upper', 'lower', 'title').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with converted column names.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.appName(\"ConvertColumnNames\").getOrCreate()\n",
    "        >>> data = [(\"Alice\", 34, \"NY\"), (\"Bob\", 45, \"CA\"), (\"Charlie\", 29, \"TX\")]\n",
    "        >>> df = spark.createDataFrame(data, [\"name\", \"age\", \"state\"])\n",
    "        >>> df_upper = convert_column_names(df, 'upper')\n",
    "        >>> df_upper.show()\n",
    "        +-------+---+-----+\n",
    "        |   NAME|AGE|STATE|\n",
    "        +-------+---+-----+\n",
    "        |  Alice| 34|   NY|\n",
    "        |    Bob| 45|   CA|\n",
    "        |Charlie| 29|   TX|\n",
    "        +-------+---+-----+\n",
    "        >>> df_title = convert_column_names(df, 'title')\n",
    "        >>> df_title.show()\n",
    "        +-------+---+-----+\n",
    "        |   Name|Age|State|\n",
    "        +-------+---+-----+\n",
    "        |  Alice| 34|   NY|\n",
    "        |    Bob| 45|   CA|\n",
    "        |Charlie| 29|   TX|\n",
    "        +-------+---+-----+\n",
    "        >>> spark.stop()\n",
    "    \"\"\"\n",
    "    if case not in [\"upper\", \"lower\", \"title\"]:\n",
    "        raise ValueError(\n",
    "            \"The 'case' parameter must be one of 'upper', 'lower', or 'title'.\"\n",
    "        )\n",
    "\n",
    "    new_column_names = []\n",
    "    for column in df.columns:\n",
    "        if case == \"upper\":\n",
    "            new_col = column.upper()\n",
    "        elif case == \"lower\":\n",
    "            new_col = column.lower()\n",
    "        elif case == \"title\":\n",
    "            new_col = column.title()\n",
    "        new_column_names.append(new_col)\n",
    "\n",
    "    for old_col, new_col in zip(df.columns, new_column_names):\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ConvertColumnNames\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 34, \"NY\"), (\"Bob\", 45, \"CA\"), (\"Charlie\", 29, \"TX\")]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"state\"])\n",
    "\n",
    "df_upper = convert_column_names(df, \"upper\")\n",
    "df_upper.show()\n",
    "\n",
    "df_lower = convert_column_names(df, \"lower\")\n",
    "df_lower.show()\n",
    "\n",
    "df_title = convert_column_names(df, \"title\")\n",
    "df_title.show()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "| Age| Name|               Group|\n",
      "+----+-----+--------------------+\n",
      "|  15| John|   Name contains 'o'|\n",
      "|  25| Jane|Age is 25, 35, or 55|\n",
      "|  35|  Doe|            35 - Doe|\n",
      "|  55|Alice|          55 - Alice|\n",
      "|  75|  Bob|            75 - Bob|\n",
      "|null|  Eve|            Has Name|\n",
      "|  30| null|               Adult|\n",
      "+----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession, functions as F\n",
    "from pyspark.sql.column import Column\n",
    "from typing import List, Tuple, Union, Callable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "\n",
    "def add_mapped_column(\n",
    "    df: DataFrame,\n",
    "    target_column: str,\n",
    "    mapping_list: List[Tuple[Column, Union[str, Callable[[DataFrame], Column]]]],\n",
    "    default_value: str = \"Unknown\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column to the DataFrame based on mappings defined in a list of tuples.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        target_column (str): Name of the new column to create.\n",
    "        mapping_list (List[Tuple[Column, Union[str, Callable[[DataFrame], Column]]]]): List with conditions as elements and mapped values or functions as values.\n",
    "        default_value (str): Default value for rows not matching any condition.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the new mapped column.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession, functions as F\n",
    "        >>> spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "        >>> data = [\n",
    "        >>>     (15, \"John\"),\n",
    "        >>>     (25, \"Jane\"),\n",
    "        >>>     (35, \"Doe\"),\n",
    "        >>>     (55, \"Alice\"),\n",
    "        >>>     (75, \"Bob\"),\n",
    "        >>>     (None, \"Eve\"),\n",
    "        >>>     (30, None),\n",
    "        >>> ]\n",
    "        >>> columns = [\"Age\", \"Name\"]\n",
    "        >>> df = spark.createDataFrame(data, columns)\n",
    "        >>> age_group_mapping = [\n",
    "        >>>     (F.col(\"Age\") < 18, \"Minor\"),\n",
    "        >>>     (F.col(\"Age\").isNull(), \"No Age\"),\n",
    "        >>>     ((F.col(\"Age\") >= 30) & (F.col(\"Age\") < 50), \"Adult\"),\n",
    "        >>>     (F.col(\"Name\").startswith(\"A\"), \"Name with A\"),\n",
    "        >>>     (F.col(\"Name\").rlike(\"^[Jj]\"), \"Starts with J or j\"),\n",
    "        >>>     (F.col(\"Name\").contains(\"a\"), \"Contains 'a'\"),\n",
    "        >>>     ((F.col(\"Age\") == 25) | (F.col(\"Age\") == 35), \"Specific Age 25 or 35\"),\n",
    "        >>>     (~F.col(\"Age\").between(18, 65), \"Not between 18 and 65\"),\n",
    "        >>>     (F.col(\"Name\").isNotNull(), \"Has Name\"),\n",
    "        >>>     (F.col(\"Age\").isin(25, 35, 55), \"Age is 25, 35, or 55\"),\n",
    "        >>>     (F.col(\"Name\").like(\"%o%\"), \"Name contains 'o'\"),\n",
    "        >>>     (F.col(\"Age\") > 10, lambda df: F.concat_ws(\" - \", df[\"Age\"], df[\"Name\"])),\n",
    "        >>> ]\n",
    "        >>> df_with_age_group = add_mapped_column(df, \"Group\", age_group_mapping)\n",
    "        >>> df_with_age_group.show()\n",
    "    \"\"\"\n",
    "    # Start with the default value\n",
    "    when_expr: Column = F.lit(default_value)\n",
    "\n",
    "    # Iterate over the mapping list and build the condition expression\n",
    "    for condition, value in mapping_list:\n",
    "        if callable(value):\n",
    "            when_expr = F.when(condition, value(df)).otherwise(when_expr)\n",
    "        else:\n",
    "            when_expr = F.when(condition, value).otherwise(when_expr)\n",
    "\n",
    "    # Add the new column to the DataFrame\n",
    "    df_with_new_column = df.withColumn(target_column, when_expr)\n",
    "\n",
    "    return df_with_new_column\n",
    "\n",
    "\n",
    "data = [\n",
    "    (15, \"John\"),\n",
    "    (25, \"Jane\"),\n",
    "    (35, \"Doe\"),\n",
    "    (55, \"Alice\"),\n",
    "    (75, \"Bob\"),\n",
    "    (None, \"Eve\"),\n",
    "    (30, None),\n",
    "]\n",
    "columns = [\"Age\", \"Name\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "age_group_mapping = [\n",
    "    (F.col(\"Age\") < 18, \"Minor\"),\n",
    "    (F.col(\"Age\").isNull(), \"No Age\"),\n",
    "    ((F.col(\"Age\") >= 30) & (F.col(\"Age\") < 50), \"Adult\"),\n",
    "    (F.col(\"Name\").startswith(\"A\"), \"Name with A\"),\n",
    "    (F.col(\"Name\").rlike(\"^[Jj]\"), \"Starts with J or j\"),\n",
    "    (F.col(\"Name\").contains(\"a\"), \"Contains 'a'\"),\n",
    "    ((F.col(\"Age\") == 25) | (F.col(\"Age\") == 35), \"Specific Age 25 or 35\"),\n",
    "    (~F.col(\"Age\").between(18, 65), \"Not between 18 and 65\"),\n",
    "    (F.col(\"Name\").isNotNull(), \"Has Name\"),\n",
    "    (F.col(\"Age\").isin(25, 35, 55), \"Age is 25, 35, or 55\"),\n",
    "    (F.col(\"Name\").like(\"%o%\"), \"Name contains 'o'\"),\n",
    "    (F.col(\"Age\") > 30, F.concat_ws(\" - \", df[\"Age\"], df[\"Name\"])),\n",
    "]\n",
    "\n",
    "df_with_age_group = add_mapped_column(df, \"Group\", age_group_mapping)\n",
    "df_with_age_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ Column id has duplicate values.\n",
      "ðŸ’¡ Examples: 1\n",
      "âœ… Uniqueness check passed for: id, value\n",
      "âœ… Uniqueness check passed for: id, value, category\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def assert_unique_combination(df: DataFrame, columns: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Assert that specified columns or their combination have unique values in the DataFrame.\n",
    "\n",
    "    This function checks for uniqueness in the specified columns. If a single column\n",
    "    is provided, it checks that column for uniqueness. If multiple columns are provided,\n",
    "    it checks the combination of these columns for uniqueness.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame to check.\n",
    "        columns (List[str]): List of column names to check for uniqueness.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If duplicate values are found in the specified column(s).\n",
    "\n",
    "    Example:\n",
    "        >>> df = spark.createDataFrame([(1, 'A'), (2, 'B'), (1, 'C')], ['id', 'value'])\n",
    "        >>> assert_unique_combination(df, ['id'])  # Raises ValueError\n",
    "        >>> assert_unique_combination(df, ['id', 'value'])  # No error\n",
    "    \"\"\"\n",
    "    if not columns:\n",
    "        raise ValueError(\"At least one column must be specified.\")\n",
    "\n",
    "    if len(columns) == 1:\n",
    "        check_column = columns[0]\n",
    "    else:\n",
    "        check_column = \"unique_combo_\" + \"_\".join(columns)\n",
    "        concat_expr = F.concat_ws(\"||\", *columns)\n",
    "        df = df.withColumn(check_column, concat_expr)\n",
    "\n",
    "    duplicates = df.groupBy(check_column).count().filter(F.col(\"count\") > 1)\n",
    "\n",
    "    if duplicates.count() > 0:\n",
    "        dup_values = duplicates.select(check_column).limit(3).collect()\n",
    "        dup_list = [row[check_column] for row in dup_values]\n",
    "\n",
    "        if len(columns) == 1:\n",
    "            error_message = f\"ðŸš¨ Column {columns[0]} has duplicate values.\"\n",
    "        else:\n",
    "            error_message = f\"ðŸš¨ Combination of {', '.join(columns)} is not unique.\"\n",
    "\n",
    "        error_message += f\"\\nðŸ’¡ Examples: {', '.join(map(str, dup_list))}\"\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    if len(columns) > 1:\n",
    "        df = df.drop(check_column)\n",
    "\n",
    "    print(f\"âœ… Uniqueness check passed for: {', '.join(columns)}\")\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"UniqueComboCheck\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"A\", \"X\"),\n",
    "    (2, \"B\", \"Y\"),\n",
    "    (3, \"C\", \"Z\"),\n",
    "    (1, \"D\", \"W\"),  # Duplicate in first column\n",
    "    (4, \"B\", \"V\"),  # Duplicate in second column\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\", \"category\"])\n",
    "\n",
    "# Check single column\n",
    "try:\n",
    "    assert_unique_combination(df, [\"id\"])\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Check multiple columns\n",
    "try:\n",
    "    assert_unique_combination(df, [\"id\", \"value\"])\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Check columns with no duplicates\n",
    "assert_unique_combination(df, [\"id\", \"value\", \"category\"])\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points:\n",
    "\n",
    "It returns a DataFrame representing the table.\n",
    "The table must exist in the Hive metastore or Spark catalog.\n",
    "It's database-agnostic, meaning it can read from any supported database type that's configured in your Spark environment.\n",
    "\n",
    "df = spark.table(\"database_name.table_name\")\n",
    "\n",
    "try:\n",
    "    df = spark.table(\"database_name.table_name\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading table: {e}\")\n",
    "\n",
    "\n",
    "df = spark.table(\"database_name.table_name\").select(\"column1\", \"column2\")\n",
    "\n",
    "sales_db = spark.conf.get(\"my_app.sales_database\")\n",
    "customer_data = spark.table(f\"{sales_db}.customer_info\")\n",
    "\n",
    "df = spark.table(\"database_name.table_name\").filter(F.col(\"date\") > \"2023-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+-----+----+-------------+\n",
      "|category|total_value|avg_value|count|rank|running_total|\n",
      "+--------+-----------+---------+-----+----+-------------+\n",
      "|       A|        250|    125.0|    2|   1|          250|\n",
      "|       B|        450|    225.0|    2|   1|          450|\n",
      "|       C|        650|    325.0|    2|   1|          650|\n",
      "+--------+-----------+---------+-----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ModularGroupBy\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"A\", 1, 100),\n",
    "    (\"A\", 2, 150),\n",
    "    (\"B\", 1, 200),\n",
    "    (\"B\", 2, 250),\n",
    "    (\"C\", 1, 300),\n",
    "    (\"C\", 2, 350),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"category\", \"subcategory\", \"value\"])\n",
    "\n",
    "\n",
    "def apply_groupby_aggregations(\n",
    "    df: DataFrame, group_cols: List[str], agg_expressions: Dict[str, Any]\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply groupBy and aggregations on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    group_cols (List[str]): Columns to group by\n",
    "    agg_expressions (Dict[str, Any]): Aggregation expressions\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Grouped and aggregated DataFrame\n",
    "    \"\"\"\n",
    "    return df.groupBy(*group_cols).agg(\n",
    "        *[F.expr(expr).alias(column) for column, expr in agg_expressions.items()]\n",
    "    )\n",
    "\n",
    "\n",
    "def add_window_calculations(\n",
    "    df: DataFrame,\n",
    "    partition_cols: List[str],\n",
    "    order_cols: List[str],\n",
    "    window_expressions: Dict[str, Any],\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add window function calculations to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    partition_cols (List[str]): Columns to partition by\n",
    "    order_cols (List[str]): Columns to order by\n",
    "    window_expressions (Dict[str, Any]): Window function expressions\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with additional window calculations\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(*partition_cols).orderBy(*order_cols)\n",
    "    for column, expr in window_expressions.items():\n",
    "        df = df.withColumn(column, expr.over(window_spec))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage\n",
    "def main():\n",
    "    # GroupBy aggregations\n",
    "    agg_result = apply_groupby_aggregations(\n",
    "        df,\n",
    "        group_cols=[\"category\"],\n",
    "        agg_expressions={\n",
    "            \"total_value\": \"sum(value)\",\n",
    "            \"avg_value\": \"avg(value)\",\n",
    "            \"count\": \"count(*)\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Window calculations\n",
    "    window_result = add_window_calculations(\n",
    "        agg_result,\n",
    "        partition_cols=[\"category\"],\n",
    "        order_cols=[\"total_value\"],\n",
    "        window_expressions={\"rank\": F.rank(), \"running_total\": F.sum(\"total_value\")},\n",
    "    )\n",
    "\n",
    "    window_result.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|  John    |  Doe    |30 |\n",
      "|Jane      | Smith   |25 |\n",
      "| Bob      |Johnson  |35 |\n",
      "+----------+---------+---+\n",
      "\n",
      "\n",
      "Trimmed DataFrame:\n",
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|John      |Doe      |30 |\n",
      "|Jane      |Smith    |25 |\n",
      "|Bob       |Johnson  |35 |\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "def trim_all_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Trim leading and trailing whitespace from all string columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with all string columns trimmed\n",
    "    \"\"\"\n",
    "    # Get the schema of the DataFrame\n",
    "    schema = df.schema\n",
    "\n",
    "    # Create a list of column expressions\n",
    "    trim_exprs = [\n",
    "        (\n",
    "            F.trim(F.col(field.name)).alias(field.name)\n",
    "            if isinstance(field.dataType, StringType)\n",
    "            else F.col(field.name)\n",
    "        )\n",
    "        for field in schema.fields\n",
    "    ]\n",
    "\n",
    "    # Apply the trim expressions to the DataFrame\n",
    "    trimmed_df = df.select(*trim_exprs)\n",
    "\n",
    "    return trimmed_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"TrimColumns\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"  John  \", \"  Doe  \", 30), (\"Jane \", \" Smith\", 25), (\" Bob\", \"Johnson \", 35)]\n",
    "df = spark.createDataFrame(data, [\"first_name\", \"last_name\", \"age\"])\n",
    "\n",
    "# Apply the trim function\n",
    "trimmed_df = trim_all_columns(df)\n",
    "\n",
    "# Show the results\n",
    "print(\"Original DataFrame:\")\n",
    "df.show(truncate=False)\n",
    "print(\"\\nTrimmed DataFrame:\")\n",
    "trimmed_df.show(truncate=False)\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in a database\n",
    "spark.catalog.listTables(\"default\").show()\n",
    "\n",
    "# Get details of a specific table\n",
    "spark.catalog.getTable(\"default\", \"my_table\").show()\n",
    "\n",
    "catalog = spark.catalog\n",
    "\n",
    "databases = catalog.listDatabases()\n",
    "databases.show()\n",
    "\n",
    "tables = catalog.listTables(\"default\")  # \"default\" is the database name\n",
    "tables.show()\n",
    "\n",
    "columns = catalog.listColumns(\"my_table\", \"default\")\n",
    "columns.show()\n",
    "\n",
    "functions = catalog.listFunctions()\n",
    "functions.show()\n",
    "\n",
    "table_details = catalog.getTable(\"default\", \"my_table\")\n",
    "print(table_details)\n",
    "\n",
    "current_db = catalog.currentDatabase()\n",
    "print(current_db)\n",
    "\n",
    "table = catalog.getTable(\"default\", \"my_table\")\n",
    "properties = table.properties\n",
    "print(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "def one_hot_encode(df: DataFrame, categorical_columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on specified categorical columns.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    categorical_columns (list): List of categorical column names to encode\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with one-hot encoded columns\n",
    "    \"\"\"\n",
    "    stages = []\n",
    "    for cat_col in categorical_columns:\n",
    "        string_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_index\")\n",
    "        encoder = OneHotEncoder(\n",
    "            inputCols=[f\"{cat_col}_index\"], outputCols=[f\"{cat_col}_encoded\"]\n",
    "        )\n",
    "        stages += [string_indexer, encoder]\n",
    "\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    model = pipeline.fit(df)\n",
    "    encoded_df = model.transform(df)\n",
    "\n",
    "    return encoded_df\n",
    "\n",
    "\n",
    "# Usage\n",
    "encoded_df = one_hot_encode(df, [\"category\", \"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def remove_duplicates(df: DataFrame, subset: list = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    subset (list): List of columns to consider for duplicates (default: None, uses all columns)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    if subset:\n",
    "        return df.dropDuplicates(subset=subset)\n",
    "    else:\n",
    "        return df.dropDuplicates()\n",
    "\n",
    "\n",
    "# Usage\n",
    "deduped_df = remove_duplicates(df, [\"id\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def apply_sql_query(\n",
    "    df: DataFrame, query: str, view_name: str = \"temp_view\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a custom SQL query to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    query (str): SQL query to apply\n",
    "    view_name (str): Name for the temporary view (default: \"temp_view\")\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Result of the SQL query\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(view_name)\n",
    "    result = df.sparkSession.sql(query)\n",
    "    df.sparkSession.catalog.dropTempView(view_name)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Usage\n",
    "query = \"\"\"\n",
    "    SELECT category, AVG(price) as avg_price\n",
    "    FROM temp_view\n",
    "    GROUP BY category\n",
    "    HAVING AVG(price) > 100\n",
    "\"\"\"\n",
    "result_df = apply_sql_query(df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Dict, Any, Union\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def summarize_numeric_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for all numeric columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Summary statistics including count, mean, stddev, min, and max for each numeric column\n",
    "    \"\"\"\n",
    "    numeric_columns = [f.name for f in df.schema.fields if f.dataType.simpleString() in ('double', 'float', 'int', 'long')]\n",
    "    \n",
    "    summary = df.select([\n",
    "        F.count(F.col(c)).alias(f\"{c}_count\"),\n",
    "        F.mean(F.col(c)).alias(f\"{c}_mean\"),\n",
    "        F.stddev(F.col(c)).alias(f\"{c}_stddev\"),\n",
    "        F.min(F.col(c)).alias(f\"{c}_min\"),\n",
    "        F.max(F.col(c)).alias(f\"{c}_max\")\n",
    "    for c in numeric_columns])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def identify_outliers(df: DataFrame, column: str, lower_quantile: float = 0.25, upper_quantile: float = 0.75, iqr_multiplier: float = 1.5) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Identify outliers in a specified column using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    column (str): Name of the column to check for outliers\n",
    "    lower_quantile (float): Lower quantile for IQR calculation (default: 0.25)\n",
    "    upper_quantile (float): Upper quantile for IQR calculation (default: 0.75)\n",
    "    iqr_multiplier (float): Multiplier for IQR to determine outlier boundaries (default: 1.5)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Original DataFrame with an additional boolean column indicating outliers\n",
    "    \"\"\"\n",
    "    quantiles = df.approxQuantile(column, [lower_quantile, upper_quantile], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "    \n",
    "    return df.withColumn(f\"{column}_is_outlier\", \n",
    "                         ~F.col(column).between(lower_bound, upper_bound))\n",
    "\n",
    "def pivot_and_unpivot(df: DataFrame, id_cols: List[str], pivot_col: str, value_col: str) -> Dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform both pivot and unpivot operations on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    id_cols (List[str]): List of columns to use as identifiers\n",
    "    pivot_col (str): Column to use for pivoting\n",
    "    value_col (str): Column containing the values to be pivoted\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, DataFrame]: Dictionary containing both pivoted and unpivoted DataFrames\n",
    "    \"\"\"\n",
    "    # Pivot operation\n",
    "    pivot_df = df.groupBy(id_cols).pivot(pivot_col).agg(F.first(value_col))\n",
    "    \n",
    "    # Unpivot operation\n",
    "    value_columns = [c for c in pivot_df.columns if c not in id_cols]\n",
    "    unpivot_expr = [F.expr(f\"stack({len(value_columns)}, {', '.join([f''''{c}', {c}''' for c in value_columns])}) as ({pivot_col}, {value_col})\")]\n",
    "    unpivot_df = pivot_df.select(*id_cols, *unpivot_expr)\n",
    "    \n",
    "    return {\"pivoted\": pivot_df, \"unpivoted\": unpivot_df}\n",
    "\n",
    "def add_date_features(df: DataFrame, date_column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add various date-related features to a DataFrame based on a specified date column.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    date_column (str): Name of the column containing date information\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with additional date-related columns\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"year\", F.year(F.col(date_column))) \\\n",
    "             .withColumn(\"month\", F.month(F.col(date_column))) \\\n",
    "             .withColumn(\"day\", F.dayofmonth(F.col(date_column))) \\\n",
    "             .withColumn(\"day_of_week\", F.dayofweek(F.col(date_column))) \\\n",
    "             .withColumn(\"day_of_year\", F.dayofyear(F.col(date_column))) \\\n",
    "             .withColumn(\"week_of_year\", F.weekofyear(F.col(date_column))) \\\n",
    "             .withColumn(\"quarter\", F.quarter(F.col(date_column)))\n",
    "\n",
    "def compare_dataframes(df1: DataFrame, df2: DataFrame, join_columns: List[str]) -> Dict[str, Union[DataFrame, int]]:\n",
    "    \"\"\"\n",
    "    Compare two DataFrames and identify differences.\n",
    "\n",
    "    Args:\n",
    "    df1 (DataFrame): First DataFrame\n",
    "    df2 (DataFrame): Second DataFrame\n",
    "    join_columns (List[str]): Columns to use for joining the DataFrames\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Union[DataFrame, int]]: Dictionary containing DataFrames with differences and counts\n",
    "    \"\"\"\n",
    "    # Ensure both DataFrames have the same columns\n",
    "    columns = sorted(set(df1.columns + df2.columns))\n",
    "    df1 = df1.select(columns)\n",
    "    df2 = df2.select(columns)\n",
    "    \n",
    "    # Perform full outer join\n",
    "    joined = df1.join(df2, join_columns, \"full_outer\")\n",
    "    \n",
    "    # Identify rows present in df1 but not in df2\n",
    "    in_df1_not_df2 = joined.filter(' AND '.join([f\"(df2.`{col}` IS NULL OR df1.`{col}` != df2.`{col}`)\" for col in columns if col not in join_columns]))\n",
    "    \n",
    "    # Identify rows present in df2 but not in df1\n",
    "    in_df2_not_df1 = joined.filter(' AND '.join([f\"(df1.`{col}` IS NULL OR df1.`{col}` != df2.`{col}`)\" for col in columns if col not in join_columns]))\n",
    "    \n",
    "    return {\n",
    "        \"in_df1_not_df2\": in_df1_not_df1,\n",
    "        \"in_df2_not_df1\": in_df2_not_df1,\n",
    "        \"total_differences\": in_df1_not_df2.count() + in_df2_not_df1.count()\n",
    "    }\n",
    "\n",
    "def validate_schema(df: DataFrame, expected_schema: StructType) -> List[str]:\n",
    "    \"\"\"\n",
    "    Validate the schema of a DataFrame against an expected schema.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame to validate\n",
    "    expected_schema (StructType): Expected schema\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of discrepancies between actual and expected schema\n",
    "    \"\"\"\n",
    "    actual_fields = df.schema.fields\n",
    "    expected_fields = expected_schema.fields\n",
    "    discrepancies = []\n",
    "\n",
    "    for actual, expected in zip(actual_fields, expected_fields):\n",
    "        if actual.name != expected.name:\n",
    "            discrepancies.append(f\"Column name mismatch: {actual.name} != {expected.name}\")\n",
    "        elif actual.dataType != expected.dataType:\n",
    "            discrepancies.append(f\"Data type mismatch for column {actual.name}: {actual.dataType} != {expected.dataType}\")\n",
    "        elif actual.nullable != expected.nullable:\n",
    "            discrepancies.append(f\"Nullability mismatch for column {actual.name}: {actual.nullable} != {expected.nullable}\")\n",
    "\n",
    "    if len(actual_fields) != len(expected_fields):\n",
    "        discrepancies.append(f\"Number of columns mismatch: {len(actual_fields)} != {len(expected_fields)}\")\n",
    "\n",
    "    return discrepancies\n",
    "\n",
    "# Usage examples:\n",
    "spark = SparkSession.builder.appName(\"DataAnalysisFunctions\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"2023-06-01\", 100, \"A\"),\n",
    "    (2, \"2023-06-02\", 200, \"B\"),\n",
    "    (3, \"2023-06-03\", 300, \"A\"),\n",
    "    (4, \"2023-06-04\", 400, \"B\"),\n",
    "    (5, \"2023-06-05\", 500, \"C\")\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"date\", \"value\", \"category\"])\n",
    "\n",
    "# Example usage of summarize_numeric_columns\n",
    "summary = summarize_numeric_columns(df)\n",
    "summary.show()\n",
    "\n",
    "# Example usage of identify_outliers\n",
    "df_with_outliers = identify_outliers(df, \"value\")\n",
    "df_with_outliers.show()\n",
    "\n",
    "# Example usage of pivot_and_unpivot\n",
    "pivot_results = pivot_and_unpivot(df, [\"id\"], \"category\", \"value\")\n",
    "pivot_results[\"pivoted\"].show()\n",
    "pivot_results[\"unpivoted\"].show()\n",
    "\n",
    "# Example usage of add_date_features\n",
    "df_with_date_features = add_date_features(df, \"date\")\n",
    "df_with_date_features.show()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Dict, Any, Union\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def profile_data_quality(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Profile the data quality of a DataFrame, including null counts, distinct counts, and data types.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Data quality profile including column name, data type, null count, distinct count, and sample values\n",
    "    \"\"\"\n",
    "    def sample_values(col_name):\n",
    "        return F.concat_ws(\", \", F.collect_set(F.col(col_name)).over(Window.partitionBy(F.lit(1)).rowsBetween(-1000, 1000)))\n",
    "\n",
    "    profile = df.select([\n",
    "        F.lit(c).alias(\"column_name\"),\n",
    "        F.lit(str(df.schema[c].dataType)).alias(\"data_type\"),\n",
    "        F.count(F.when(F.col(c).isNull(), c)).alias(\"null_count\"),\n",
    "        F.count(F.when(F.col(c).isNotNull(), c)).alias(\"non_null_count\"),\n",
    "        F.countDistinct(c).alias(\"distinct_count\"),\n",
    "        sample_values(c).alias(\"sample_values\")\n",
    "    for c in df.columns])\n",
    "\n",
    "    return profile.select(\"column_name\", \"data_type\", \"null_count\", \"non_null_count\", \"distinct_count\", \n",
    "                          F.expr(\"substring(sample_values, 1, 100)\").alias(\"sample_values\"))\n",
    "\n",
    "def handle_missing_values(df: DataFrame, strategy: str = \"mean\", columns: List[str] = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing values in specified columns using various imputation strategies.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    strategy (str): Imputation strategy ('mean', 'median', 'mode', or 'constant')\n",
    "    columns (List[str]): List of columns to impute (default: None, which means all numeric columns)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with imputed values\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = [c for c, dtype in df.dtypes if dtype in ('int', 'double', 'float')]\n",
    "\n",
    "    imputer = Imputer(inputCols=columns, outputCols=columns)\n",
    "    imputer.setStrategy(strategy)\n",
    "\n",
    "    if strategy == \"constant\":\n",
    "        imputer.setMissingValue(0)  # You can change this to any constant value\n",
    "\n",
    "    imputed_df = imputer.fit(df).transform(df)\n",
    "    return imputed_df\n",
    "\n",
    "def calculate_correlation_matrix(df: DataFrame, method: str = \"pearson\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix for numeric columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    method (str): Correlation method ('pearson' or 'spearman')\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Correlation matrix\n",
    "    \"\"\"\n",
    "    numeric_columns = [c for c, dtype in df.dtypes if dtype in ('int', 'double', 'float')]\n",
    "    \n",
    "    if method == \"pearson\":\n",
    "        correlation = df.select(numeric_columns).corr()\n",
    "    elif method == \"spearman\":\n",
    "        correlation = df.select(numeric_columns).corr(method=\"spearman\")\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'pearson' or 'spearman'\")\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "def binning(df: DataFrame, column: str, num_bins: int, strategy: str = \"equal_range\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform binning on a numeric column.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    column (str): Column to bin\n",
    "    num_bins (int): Number of bins\n",
    "    strategy (str): Binning strategy ('equal_range' or 'equal_frequency')\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with binned column added\n",
    "    \"\"\"\n",
    "    if strategy == \"equal_range\":\n",
    "        return df.withColumn(f\"{column}_binned\", F.ntile(num_bins).over(Window.orderBy(column)))\n",
    "    elif strategy == \"equal_frequency\":\n",
    "        quantiles = df.approxQuantile(column, [i/num_bins for i in range(1, num_bins)], 0.01)\n",
    "        return df.withColumn(f\"{column}_binned\", F.bucketizer(F.col(column), [-float(\"inf\")] + quantiles + [float(\"inf\")]))\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be either 'equal_range' or 'equal_frequency'\")\n",
    "\n",
    "def detect_data_drift(df1: DataFrame, df2: DataFrame, columns: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detect data drift between two DataFrames for specified columns.\n",
    "\n",
    "    Args:\n",
    "    df1 (DataFrame): First DataFrame (e.g., training data)\n",
    "    df2 (DataFrame): Second DataFrame (e.g., new data)\n",
    "    columns (List[str]): List of columns to check for drift\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Data drift statistics including column name, drift metric, and p-value\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import kurtosis, skewness, mean, stddev\n",
    "    from scipy import stats\n",
    "\n",
    "    drift_results = []\n",
    "\n",
    "    for column in columns:\n",
    "        stats1 = df1.select(mean(column).alias(\"mean\"), \n",
    "                            stddev(column).alias(\"std\"),\n",
    "                            kurtosis(column).alias(\"kurtosis\"),\n",
    "                            skewness(column).alias(\"skewness\")).collect()[0]\n",
    "        \n",
    "        stats2 = df2.select(mean(column).alias(\"mean\"), \n",
    "                            stddev(column).alias(\"std\"),\n",
    "                            kurtosis(column).alias(\"kurtosis\"),\n",
    "                            skewness(column).alias(\"skewness\")).collect()[0]\n",
    "\n",
    "        # Perform Kolmogorov-Smirnov test\n",
    "        ks_statistic, p_value = stats.ks_2samp(df1.select(column).rdd.flatMap(lambda x: x).collect(),\n",
    "                                               df2.select(column).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "        drift_results.append((column, ks_statistic, p_value, \n",
    "                              stats1.mean, stats2.mean, \n",
    "                              stats1.std, stats2.std,\n",
    "                              stats1.kurtosis, stats2.kurtosis,\n",
    "                              stats1.skewness, stats2.skewness))\n",
    "\n",
    "    drift_df = spark.createDataFrame(drift_results, \n",
    "                                     [\"column\", \"ks_statistic\", \"p_value\", \n",
    "                                      \"mean1\", \"mean2\", \"std1\", \"std2\", \n",
    "                                      \"kurtosis1\", \"kurtosis2\", \"skewness1\", \"skewness2\"])\n",
    "    \n",
    "    return drift_df\n",
    "\n",
    "def generate_summary_report(df: DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary report of the DataFrame in Markdown format.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    str: Markdown-formatted summary report\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    num_columns = len(df.columns)\n",
    "    \n",
    "    numeric_columns = [c for c, dtype in df.dtypes if dtype in ('int', 'double', 'float')]\n",
    "    categorical_columns = [c for c, dtype in df.dtypes if dtype not in ('int', 'double', 'float')]\n",
    "    \n",
    "    summary_stats = df.summary()\n",
    "    \n",
    "    report = f\"# DataFrame Summary Report\\n\\n\"\n",
    "    report += f\"## Basic Information\\n\"\n",
    "    report += f\"- Number of rows: {num_rows}\\n\"\n",
    "    report += f\"- Number of columns: {num_columns}\\n\"\n",
    "    report += f\"- Numeric columns: {', '.join(numeric_columns)}\\n\"\n",
    "    report += f\"- Categorical columns: {', '.join(categorical_columns)}\\n\\n\"\n",
    "    \n",
    "    report += f\"## Numeric Column Statistics\\n\"\n",
    "    for col in numeric_columns:\n",
    "        stats = summary_stats.filter(F.col(\"summary\").isin(\"min\", \"max\", \"mean\", \"stddev\")).select(col).collect()\n",
    "        report += f\"### {col}\\n\"\n",
    "        report += f\"- Min: {stats[0][0]}\\n\"\n",
    "        report += f\"- Max: {stats[1][0]}\\n\"\n",
    "        report += f\"- Mean: {stats[2][0]}\\n\"\n",
    "        report += f\"- StdDev: {stats[3][0]}\\n\\n\"\n",
    "    \n",
    "    report += f\"## Categorical Column Statistics\\n\"\n",
    "    for col in categorical_columns:\n",
    "        top_values = df.groupBy(col).count().orderBy(F.desc(\"count\")).limit(5)\n",
    "        report += f\"### {col}\\n\"\n",
    "        report += f\"Top 5 values:\\n\"\n",
    "        for row in top_values.collect():\n",
    "            report += f\"- {row[0]}: {row[1]}\\n\"\n",
    "        report += \"\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "spark = SparkSession.builder.appName(\"AdvancedDataAnalysisFunctions\").getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"2023-06-01\", 100, \"A\", 0.5),\n",
    "    (2, \"2023-06-02\", 200, \"B\", 1.5),\n",
    "    (3, \"2023-06-03\", None, \"A\", 2.5),\n",
    "    (4, \"2023-06-04\", 400, \"B\", 3.5),\n",
    "    (5, \"2023-06-05\", 500, \"C\", None)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"date\", \"value\", \"category\", \"score\"])\n",
    "\n",
    "# Profile data quality\n",
    "quality_profile = profile_data_quality(df)\n",
    "quality_profile.show(truncate=False)\n",
    "\n",
    "# Handle missing values\n",
    "df_imputed = handle_missing_values(df, strategy=\"mean\")\n",
    "df_imputed.show()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = calculate_correlation_matrix(df_imputed)\n",
    "corr_matrix.show()\n",
    "\n",
    "# Perform binning\n",
    "df_binned = binning(df_imputed, \"value\", 3, strategy=\"equal_range\")\n",
    "df_binned.show()\n",
    "\n",
    "# Detect data drift (using the same DataFrame for demonstration)\n",
    "drift_stats = detect_data_drift(df_imputed, df_imputed, [\"value\", \"score\"])\n",
    "drift_stats.show()\n",
    "\n",
    "# Generate summary report\n",
    "report = generate_summary_report(df_imputed)\n",
    "print(report)\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "\n",
    "def create_event_log_schema() -> StructType:\n",
    "    \"\"\"\n",
    "    Create a standard schema for event logs in process mining.\n",
    "\n",
    "    Returns:\n",
    "    StructType: Schema for event logs\n",
    "    \"\"\"\n",
    "    return StructType(\n",
    "        [\n",
    "            StructField(\"case_id\", StringType(), False),\n",
    "            StructField(\"activity\", StringType(), False),\n",
    "            StructField(\"timestamp\", TimestampType(), False),\n",
    "            StructField(\"resource\", StringType(), True),\n",
    "            StructField(\"cost\", IntegerType(), True),\n",
    "            # Add more fields as needed for your specific process mining project\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_schema_to_df(df: DataFrame, schema: StructType) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a given schema to a DataFrame, casting columns to the specified types.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    schema (StructType): Schema to apply\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the applied schema\n",
    "    \"\"\"\n",
    "    for field in schema.fields:\n",
    "        df = df.withColumn(field.name, F.col(field.name).cast(field.dataType))\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_case_duration(\n",
    "    df: DataFrame, case_id_col: str, timestamp_col: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the duration of each case in the event log.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with case duration added\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(case_id_col).orderBy(timestamp_col)\n",
    "\n",
    "    df_with_duration = (\n",
    "        df.withColumn(\"case_start\", F.first(timestamp_col).over(window_spec))\n",
    "        .withColumn(\"case_end\", F.last(timestamp_col).over(window_spec))\n",
    "        .withColumn(\n",
    "            \"case_duration_seconds\",\n",
    "            F.unix_timestamp(\"case_end\") - F.unix_timestamp(\"case_start\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_with_duration.select(case_id_col, \"case_duration_seconds\").distinct()\n",
    "\n",
    "\n",
    "def identify_happy_path(\n",
    "    df: DataFrame, case_id_col: str, activity_col: str\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify the most common sequence of activities (happy path) in the event log.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    activity_col (str): Name of the activity column\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of activities in the happy path\n",
    "    \"\"\"\n",
    "    activity_sequences = df.groupBy(case_id_col).agg(\n",
    "        F.collect_list(activity_col).alias(\"activities\")\n",
    "    )\n",
    "\n",
    "    most_common_sequence = (\n",
    "        activity_sequences.groupBy(\"activities\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .first()[\"activities\"]\n",
    "    )\n",
    "\n",
    "    return most_common_sequence\n",
    "\n",
    "\n",
    "def calculate_activity_frequency(df: DataFrame, activity_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the frequency of each activity in the event log.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    activity_col (str): Name of the activity column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with activity frequencies\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.groupBy(activity_col)\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"frequency\")\n",
    "        .orderBy(F.desc(\"frequency\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def detect_bottlenecks(\n",
    "    df: DataFrame, case_id_col: str, activity_col: str, timestamp_col: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detect potential bottlenecks by calculating the average duration between activities.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    activity_col (str): Name of the activity column\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with average durations between activities\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(case_id_col).orderBy(timestamp_col)\n",
    "\n",
    "    df_with_next = df.withColumn(\n",
    "        \"next_activity\", F.lead(activity_col).over(window_spec)\n",
    "    ).withColumn(\"next_timestamp\", F.lead(timestamp_col).over(window_spec))\n",
    "\n",
    "    df_durations = df_with_next.withColumn(\n",
    "        \"duration_seconds\",\n",
    "        F.unix_timestamp(\"next_timestamp\") - F.unix_timestamp(timestamp_col),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df_durations.groupBy(activity_col, \"next_activity\")\n",
    "        .agg(F.avg(\"duration_seconds\").alias(\"avg_duration_seconds\"))\n",
    "        .orderBy(F.desc(\"avg_duration_seconds\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def create_process_variants(\n",
    "    df: DataFrame, case_id_col: str, activity_col: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create process variants by grouping similar sequences of activities.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    activity_col (str): Name of the activity column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with process variants\n",
    "    \"\"\"\n",
    "    variants = df.groupBy(case_id_col).agg(\n",
    "        F.concat_ws(\"->\", F.collect_list(activity_col)).alias(\"variant\")\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        variants.groupBy(\"variant\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"variant_frequency\")\n",
    "        .orderBy(F.desc(\"variant_frequency\"))\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "spark = SparkSession.builder.appName(\"ProcessMiningFunctions\").getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (\"case1\", \"Start\", \"2023-06-01 09:00:00\", \"John\", 10),\n",
    "    (\"case1\", \"Activity A\", \"2023-06-01 10:00:00\", \"Alice\", 20),\n",
    "    (\"case1\", \"Activity B\", \"2023-06-01 11:00:00\", \"Bob\", 30),\n",
    "    (\"case1\", \"End\", \"2023-06-01 12:00:00\", \"John\", 10),\n",
    "    (\"case2\", \"Start\", \"2023-06-01 09:30:00\", \"Alice\", 10),\n",
    "    (\"case2\", \"Activity A\", \"2023-06-01 10:30:00\", \"Bob\", 20),\n",
    "    (\"case2\", \"Activity C\", \"2023-06-01 11:30:00\", \"John\", 40),\n",
    "    (\"case2\", \"End\", \"2023-06-01 12:30:00\", \"Alice\", 10),\n",
    "]\n",
    "\n",
    "# Create DataFrame and apply schema\n",
    "schema = create_event_log_schema()\n",
    "df = spark.createDataFrame(\n",
    "    data, [\"case_id\", \"activity\", \"timestamp\", \"resource\", \"cost\"]\n",
    ")\n",
    "df = apply_schema_to_df(df, schema)\n",
    "\n",
    "# Calculate case duration\n",
    "case_durations = calculate_case_duration(df, \"case_id\", \"timestamp\")\n",
    "case_durations.show()\n",
    "\n",
    "# Identify happy path\n",
    "happy_path = identify_happy_path(df, \"case_id\", \"activity\")\n",
    "print(\"Happy Path:\", happy_path)\n",
    "\n",
    "# Calculate activity frequency\n",
    "activity_freq = calculate_activity_frequency(df, \"activity\")\n",
    "activity_freq.show()\n",
    "\n",
    "# Detect bottlenecks\n",
    "bottlenecks = detect_bottlenecks(df, \"case_id\", \"activity\", \"timestamp\")\n",
    "bottlenecks.show()\n",
    "\n",
    "# Create process variants\n",
    "variants = create_process_variants(df, \"case_id\", \"activity\")\n",
    "variants.show(truncate=False)\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "\n",
    "def remove_incomplete_cases(\n",
    "    df: DataFrame,\n",
    "    case_id_col: str,\n",
    "    activity_col: str,\n",
    "    start_activity: str,\n",
    "    end_activity: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove cases that don't start with the specified start activity or don't end with the specified end activity.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    activity_col (str): Name of the activity column\n",
    "    start_activity (str): Expected start activity\n",
    "    end_activity (str): Expected end activity\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with incomplete cases removed\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(case_id_col).orderBy(F.col(\"timestamp\"))\n",
    "\n",
    "    df_with_start_end = df.withColumn(\n",
    "        \"first_activity\", F.first(activity_col).over(window_spec)\n",
    "    ).withColumn(\"last_activity\", F.last(activity_col).over(window_spec))\n",
    "\n",
    "    return df_with_start_end.filter(\n",
    "        (F.col(\"first_activity\") == start_activity)\n",
    "        & (F.col(\"last_activity\") == end_activity)\n",
    "    ).drop(\"first_activity\", \"last_activity\")\n",
    "\n",
    "\n",
    "def remove_outlier_cases(\n",
    "    df: DataFrame,\n",
    "    case_id_col: str,\n",
    "    timestamp_col: str,\n",
    "    lower_quantile: float = 0.05,\n",
    "    upper_quantile: float = 0.95,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove outlier cases based on case duration.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "    lower_quantile (float): Lower quantile for outlier removal (default: 0.05)\n",
    "    upper_quantile (float): Upper quantile for outlier removal (default: 0.95)\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with outlier cases removed\n",
    "    \"\"\"\n",
    "    case_durations = calculate_case_duration(df, case_id_col, timestamp_col)\n",
    "\n",
    "    quantiles = case_durations.approxQuantile(\n",
    "        \"case_duration_seconds\", [lower_quantile, upper_quantile], 0.01\n",
    "    )\n",
    "    lower_bound, upper_bound = quantiles[0], quantiles[1]\n",
    "\n",
    "    valid_cases = case_durations.filter(\n",
    "        (F.col(\"case_duration_seconds\") >= lower_bound)\n",
    "        & (F.col(\"case_duration_seconds\") <= upper_bound)\n",
    "    ).select(case_id_col)\n",
    "\n",
    "    return df.join(valid_cases, on=case_id_col, how=\"inner\")\n",
    "\n",
    "\n",
    "def standardize_activity_names(\n",
    "    df: DataFrame, activity_col: str, mapping: Dict[str, str]\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize activity names based on a provided mapping.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    activity_col (str): Name of the activity column\n",
    "    mapping (Dict[str, str]): Dictionary mapping original activity names to standardized names\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with standardized activity names\n",
    "    \"\"\"\n",
    "    return df.replace(mapping, subset=[activity_col])\n",
    "\n",
    "\n",
    "def add_time_attributes(df: DataFrame, timestamp_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add time-related attributes to the event log.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with additional time attributes\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.withColumn(\"hour\", F.hour(F.col(timestamp_col)))\n",
    "        .withColumn(\"day_of_week\", F.dayofweek(F.col(timestamp_col)))\n",
    "        .withColumn(\n",
    "            \"is_weekend\",\n",
    "            F.when(F.dayofweek(F.col(timestamp_col)).isin([1, 7]), 1).otherwise(0),\n",
    "        )\n",
    "        .withColumn(\"month\", F.month(F.col(timestamp_col)))\n",
    "        .withColumn(\"quarter\", F.quarter(F.col(timestamp_col)))\n",
    "    )\n",
    "\n",
    "\n",
    "def add_event_index(df: DataFrame, case_id_col: str, timestamp_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add an event index to each event within a case.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with event index added\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(case_id_col).orderBy(timestamp_col)\n",
    "    return df.withColumn(\"event_index\", F.row_number().over(window_spec))\n",
    "\n",
    "\n",
    "def add_next_activity(\n",
    "    df: DataFrame, case_id_col: str, activity_col: str, timestamp_col: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add the next activity for each event within a case.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    case_id_col (str): Name of the case ID column\n",
    "    activity_col (str): Name of the activity column\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with next activity added\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(case_id_col).orderBy(timestamp_col)\n",
    "    return df.withColumn(\"next_activity\", F.lead(activity_col).over(window_spec))\n",
    "\n",
    "\n",
    "def filter_time_range(\n",
    "    df: DataFrame, timestamp_col: str, start_date: str, end_date: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter events within a specific time range.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Event log DataFrame\n",
    "    timestamp_col (str): Name of the timestamp column\n",
    "    start_date (str): Start date in format 'YYYY-MM-DD'\n",
    "    end_date (str): End date in format 'YYYY-MM-DD'\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with events filtered to the specified time range\n",
    "    \"\"\"\n",
    "    return df.filter(\n",
    "        (F.col(timestamp_col) >= start_date) & (F.col(timestamp_col) <= end_date)\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "spark = SparkSession.builder.appName(\"ProcessMiningPreprocessing\").getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (\"case1\", \"Start Process\", \"2023-06-01 09:00:00\", \"John\"),\n",
    "    (\"case1\", \"Activity A\", \"2023-06-01 10:00:00\", \"Alice\"),\n",
    "    (\"case1\", \"Activity B\", \"2023-06-01 11:00:00\", \"Bob\"),\n",
    "    (\"case1\", \"End Process\", \"2023-06-01 12:00:00\", \"John\"),\n",
    "    (\"case2\", \"Start Process\", \"2023-06-01 09:30:00\", \"Alice\"),\n",
    "    (\"case2\", \"Activity A\", \"2023-06-01 10:30:00\", \"Bob\"),\n",
    "    (\"case2\", \"Activity C\", \"2023-06-01 11:30:00\", \"John\"),\n",
    "    (\"case3\", \"Start Process\", \"2023-06-02 09:00:00\", \"Bob\"),\n",
    "    (\"case3\", \"Activity B\", \"2023-06-02 10:00:00\", \"Alice\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"case_id\", \"activity\", \"timestamp\", \"resource\"])\n",
    "\n",
    "# Remove incomplete cases\n",
    "df_complete = remove_incomplete_cases(\n",
    "    df, \"case_id\", \"activity\", \"Start Process\", \"End Process\"\n",
    ")\n",
    "print(\"Complete cases:\")\n",
    "df_complete.show()\n",
    "\n",
    "# Standardize activity names\n",
    "activity_mapping = {\"Start Process\": \"Start\", \"End Process\": \"End\"}\n",
    "df_standardized = standardize_activity_names(df, \"activity\", activity_mapping)\n",
    "print(\"Standardized activity names:\")\n",
    "df_standardized.show()\n",
    "\n",
    "# Add time attributes\n",
    "df_with_time = add_time_attributes(df, \"timestamp\")\n",
    "print(\"With time attributes:\")\n",
    "df_with_time.show()\n",
    "\n",
    "# Add event index\n",
    "df_with_index = add_event_index(df, \"case_id\", \"timestamp\")\n",
    "print(\"With event index:\")\n",
    "df_with_index.show()\n",
    "\n",
    "# Add next activity\n",
    "df_with_next = add_next_activity(df, \"case_id\", \"activity\", \"timestamp\")\n",
    "print(\"With next activity:\")\n",
    "df_with_next.show()\n",
    "\n",
    "# Filter time range\n",
    "df_filtered = filter_time_range(df, \"timestamp\", \"2023-06-01\", \"2023-06-01\")\n",
    "print(\"Filtered by time range:\")\n",
    "df_filtered.show()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
